# LLM BUNDLE (chunked)
# ROOT: D:\thordata_work\thordata-python-sdk
# GENERATED_AT: 2025-12-31T10:14:13
# FINGERPRINT: 399ea474b5a06e50e636ff79dde97dcf654a654b
# NOTE: Fetch /all for index, then /all?part=N for chunks.



========================================================================
FILE: .env.example
SIZE: 2152
TRUNCATED: no
========================================================================
# Thordata SDK unified configuration (.env.example)
# Keep this file identical across all SDK repositories.

# Required: Scraper token (Dashboard -> Account Settings)
THORDATA_SCRAPER_TOKEN=your_scraper_token_here

# Optional (required for Web Scraper task status/download and Locations API)
# Dashboard -> My Account
THORDATA_PUBLIC_TOKEN=your_public_token_here
THORDATA_PUBLIC_KEY=your_public_key_here

# Optional: Base URLs
THORDATA_SCRAPERAPI_BASE_URL=https://scraperapi.thordata.com
THORDATA_UNIVERSALAPI_BASE_URL=https://universalapi.thordata.com
THORDATA_WEB_SCRAPER_API_BASE_URL=https://openapi.thordata.com/api/web-scraper-api
THORDATA_LOCATIONS_BASE_URL=https://openapi.thordata.com/api/locations

# Optional: Web Scraper task example (copy from Dashboard -> Web Scraper Store -> API Builder)
THORDATA_TASK_SPIDER_NAME=youtube.com
THORDATA_TASK_SPIDER_ID=youtube_video-post_by-url
THORDATA_TASK_FILE_NAME={{VideoID}}
THORDATA_TASK_PARAMETERS_JSON={"url":"https://www.youtube.com/@stephcurry/videos","order_by":"","num_of_posts":""}

# Proxy endpoint (Dashboard -> Residential Proxies -> Endpoint Generator)
# Many accounts require HTTPS proxy endpoints (proxy URL starts with https://).
THORDATA_PROXY_HOST=vpnXXXX.pr.thordata.net
THORDATA_PROXY_PORT=9999
THORDATA_PROXY_PROTOCOL=https

# Optional: Residential Proxy (port 9999)
THORDATA_RESIDENTIAL_USERNAME=
THORDATA_RESIDENTIAL_PASSWORD=
THORDATA_RESIDENTIAL_PROXY_PROTOCOL=

# Optional: Datacenter Proxy (port 7777)
THORDATA_DATACENTER_USERNAME=
THORDATA_DATACENTER_PASSWORD=
THORDATA_DATACENTER_PROXY_PROTOCOL=

# Optional: Mobile Proxy (port 5555)
THORDATA_MOBILE_USERNAME=
THORDATA_MOBILE_PASSWORD=
THORDATA_MOBILE_PROXY_PROTOCOL=

# Optional: Static ISP Proxy (port 6666)
THORDATA_ISP_HOST=
THORDATA_ISP_USERNAME=
THORDATA_ISP_PASSWORD=

# Optional: HTTP proxy for restricted networks (e.g. Clash Verge)
HTTP_PROXY=http://127.0.0.1:7897
HTTPS_PROXY=http://127.0.0.1:7897

# Optional: Proxy whitelist mode (no username/password)
# If enabled, SDK will use THORDATA_PROXY_HOST/PORT without proxy auth
THORDATA_PROXY_WHITELIST=false

========================================================================
FILE: .gitignore
SIZE: 969
TRUNCATED: no
========================================================================
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# PyInstaller
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot

# Environments
.env
.env.local
.venv
env/
venv/
ENV/
env.bak/
venv.bak/
.venv/
.venv-*/

# IDE
.idea/
.vscode/
*.swp
*.swo
*~

# Jupyter Notebook
.ipynb_checkpoints

# pytype static type analyzer
.pytype/

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Ruff
.ruff_cache/

# OS
.DS_Store
Thumbs.db

# Project specific
*.log
secrets.json
.secrets/


========================================================================
FILE: CHANGELOG.md
SIZE: 244
TRUNCATED: no
========================================================================
# Changelog

See [GitHub Releases](https://github.com/Thordata/thordata-python-sdk/releases) for version history.

## Development Notes

- Version follows [Semantic Versioning](https://semver.org/)
- Current version: see `pyproject.toml`

========================================================================
FILE: CONTRIBUTING.md
SIZE: 3803
TRUNCATED: no
========================================================================
# Contributing to Thordata Python SDK

Thank you for your interest in contributing! This document provides guidelines and instructions for contributing.

---

## ðŸ“‹ Code of Conduct

Please be respectful and constructive in all interactions.

---

## ðŸš€ Getting Started

### 1. Fork and Clone

```bash
git clone https://github.com/YOUR_USERNAME/thordata-python-sdk.git
cd thordata-python-sdk
```

### 2. Set Up Development Environment

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install with dev dependencies
pip install -e ".[dev]"
```

### 3. Create a Branch

```bash
git checkout -b feature/your-feature-name
# or
git checkout -b fix/your-bug-fix
```

---

## ðŸ’» Development Workflow

### Code Style

We use the following tools to maintain code quality:

```bash
# Format code with Black
black src tests

# Lint with Ruff
ruff check src tests --fix

# Type check with MyPy
mypy src
```

### Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=thordata --cov-report=html

# Run specific test
pytest tests/test_client.py::test_serp_search -v
```

### Pre-commit Checks

Before committing, ensure:

- âœ… All tests pass
- âœ… Code is formatted (black)
- âœ… No linting errors (ruff)
- âœ… Type checks pass (mypy)

---

## ðŸ“ Pull Request Process

1. Update documentation if you've changed APIs
2. Add tests for new functionality
3. Update CHANGELOG.md with your changes
4. Ensure CI passes on your PR
5. Request review from maintainers

### PR Title Convention

- `feat:` Add new feature
- `fix:` Fix bug description
- `docs:` Update documentation
- `refactor:` Refactor code
- `test:` Add tests
- `chore:` Update dependencies

---

## ðŸ“ Project Structure

```
src/thordata/
â”œâ”€â”€ __init__.py         # Public exports
â”œâ”€â”€ client.py           # Sync client (main)
â”œâ”€â”€ async_client.py     # Async client
â”œâ”€â”€ models.py           # Dataclass models
â”œâ”€â”€ enums.py            # Enumerations
â”œâ”€â”€ exceptions.py       # Exception classes
â”œâ”€â”€ retry.py            # Retry logic
â””â”€â”€ _utils.py           # Internal utilities
```

---

## ðŸ§ª Testing Guidelines

- Write tests for all new features
- Use pytest fixtures for common setups
- Mock external API calls
- Aim for >80% coverage on new code

### Example Test

```python
import pytest
from thordata import ThordataClient, ProxyConfig

def test_proxy_config_validation():
    """Test that ProxyConfig validates parameters correctly."""
    with pytest.raises(ValueError, match="session_duration"):
        ProxyConfig(
            username="user",
            password="pass",
            session_duration=100  # Invalid: max is 90
        )
```

---

## ðŸ“– Documentation

- Update docstrings for API changes
- Follow Google-style docstrings
- Include examples in docstrings

### Example Docstring

```python
def serp_search(
    self,
    query: str,
    *,
    engine: str = "google",
) -> Dict[str, Any]:
    """
    Execute a SERP search.
    
    Args:
        query: The search keywords.
        engine: Search engine to use.
    
    Returns:
        Parsed JSON results from the search.
    
    Raises:
        ThordataAuthError: If authentication fails.
        ThordataRateLimitError: If rate limited.
    
    Example:
        >>> results = client.serp_search("python tutorial")
        >>> print(len(results.get("organic", [])))
    """
```

---

## â“ Questions?

- Open an issue for bugs or feature requests
- Email support@thordata.com for other questions

---

Thank you for contributing! ðŸŽ‰

========================================================================
FILE: LICENSE
SIZE: 1109
TRUNCATED: no
========================================================================
MIT License

Copyright (c) 2025 Thordata Â· AI Proxy & Web Data

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


========================================================================
FILE: py.typed
SIZE: 84
TRUNCATED: no
========================================================================
# This file intentionally left empty.
# It marks this package as typed for PEP 561.

========================================================================
FILE: pyproject.toml
SIZE: 3457
TRUNCATED: no
========================================================================
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "thordata-sdk"
version = "0.8.0"
description = "The Official Python SDK for Thordata - AI Data Infrastructure & Proxy Network."
readme = "README.md"
requires-python = ">=3.9"
license = {text = "MIT"}
authors = [
  {name = "Thordata Developer Team", email = "support@thordata.com"}
]
keywords = [
    "web scraping", 
    "proxy", 
    "residential proxy",
    "datacenter proxy",
    "ai", 
    "llm", 
    "data-mining", 
    "serp", 
    "thordata",
    "web scraper",
    "anti-bot bypass"
]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Topic :: Internet :: WWW/HTTP",
    "Topic :: Internet :: Proxy Servers",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
    "Typing :: Typed",
]

dependencies = [
    "requests>=2.25.0",
    "aiohttp>=3.9.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
    "pytest-cov>=4.0.0",
    "pytest-httpserver>=1.0.0",
    "python-dotenv>=1.0.0",
    "black>=23.0.0",
    "ruff>=0.1.0",
    "mypy>=1.0.0",
    "types-requests>=2.28.0",
    "aioresponses>=0.7.6",
]

[project.urls]
"Homepage" = "https://www.thordata.com"
"Documentation" = "https://github.com/Thordata/thordata-python-sdk#readme"
"Source" = "https://github.com/Thordata/thordata-python-sdk"
"Tracker" = "https://github.com/Thordata/thordata-python-sdk/issues"
"Changelog" = "https://github.com/Thordata/thordata-python-sdk/blob/main/CHANGELOG.md"

# Setuptools setup
[tool.setuptools.packages.find]
where = ["src"]

[tool.setuptools.package-data]
thordata = ["py.typed"]

[tool.black]
line-length = 88
target-version = ['py39', 'py310', 'py311', 'py312']
include = '\.pyi?$'

[tool.ruff]
line-length = 88
target-version = "py39"

[tool.ruff.lint]
select = [
    "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "I",   # isort (import sorting)
    "B",   # flake8-bugbear
]
ignore = [
    "E501",  # line too long (handled by black)
    "E731",  # do not assign a lambda expression
    "F401",  # imported but unused (we have some intentional re-exports)
]

[tool.ruff.lint.isort]
known-first-party = ["thordata"]

# Mypy
[tool.mypy]
python_version = "3.9"
warn_return_any = false
warn_unused_ignores = false
disallow_untyped_defs = false
disallow_incomplete_defs = false
check_untyped_defs = false
strict_optional = false
show_error_codes = true
ignore_missing_imports = true

[[tool.mypy.overrides]]
module = ["aiohttp.*", "requests.*"]
ignore_missing_imports = true

# Pytest setup
[tool.pytest.ini_options]
testpaths = ["tests"]
asyncio_mode = "auto"
addopts = "-v --cov=thordata --cov-report=term-missing"

# Coverage setup
[tool.coverage.run]
source = ["src/thordata"]
branch = true

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise NotImplementedError",
    "if TYPE_CHECKING:",
]

========================================================================
FILE: README.md
SIZE: 3807
TRUNCATED: no
========================================================================
# Thordata Python SDK

<div align="center">

**Official Python client for Thordata's Proxy Network, SERP API, Web Unlocker, and Web Scraper API.**

*Async-ready, type-safe, built for AI agents and large-scale data collection.*

[![PyPI](https://img.shields.io/pypi/v/thordata-sdk?color=blue)](https://pypi.org/project/thordata-sdk/)
[![Python](https://img.shields.io/badge/python-3.9+-blue)](https://python.org)
[![License](https://img.shields.io/badge/license-MIT-green)](LICENSE)

[Documentation](https://doc.thordata.com) â€¢ [Dashboard](https://www.thordata.com) â€¢ [Examples](examples/)

</div>

---

## âœ¨ Features

- ðŸŒ **Proxy Network**: Residential, Mobile, Datacenter, ISP proxies with geo-targeting
- ðŸ” **SERP API**: Google, Bing, Yandex, DuckDuckGo search results
- ðŸ”“ **Web Unlocker**: Bypass Cloudflare, CAPTCHAs, anti-bot systems
- ðŸ•·ï¸ **Web Scraper API**: Async task-based scraping (Text & Video/Audio)
- ðŸ“Š **Account Management**: Usage stats, sub-users, IP whitelist
- âš¡ **Async Support**: Full async/await support with aiohttp
- ðŸ”„ **Auto Retry**: Configurable retry with exponential backoff

---

## ðŸ“¦ Installation

```bash
pip install thordata-sdk
```

---

## ðŸ” Configuration

Set environment variables:

```bash
# Required for Scraper APIs (SERP, Universal, Tasks)
export THORDATA_SCRAPER_TOKEN=your_token

# Public/Location APIs (Dashboard -> My account -> API)
export THORDATA_PUBLIC_TOKEN=your_public_token
export THORDATA_PUBLIC_KEY=your_public_key

```

---

## ðŸš€ Quick Start

```python
from thordata import ThordataClient, Engine

# Initialize (reads from env vars)
client = ThordataClient(
    scraper_token="your_token", 
    public_token="pub_token", 
    public_key="pub_key"
)

# SERP Search
results = client.serp_search("python tutorial", engine=Engine.GOOGLE)
print(f"Found {len(results.get('organic', []))} results")

# Universal Scrape
html = client.universal_scrape("https://httpbin.org/html")
print(html[:100])
```

---

## ðŸ“– Feature Guide

### SERP API

```python
from thordata import SerpRequest

# Advanced search
results = client.serp_search_advanced(SerpRequest(
    query="pizza",
    engine="google_local",
    country="us",
    location="New York",
    num=10
))
```

### Web Scraper API (Async Tasks)

**Create Task:**
```python
task_id = client.create_scraper_task(
    file_name="my_task",
    spider_id="universal",
    spider_name="universal",
    parameters={"url": "https://example.com"}
)
```

**Video Download (New):**
```python
from thordata import CommonSettings

task_id = client.create_video_task(
    file_name="{{VideoID}}",
    spider_id="youtube_video_by-url",
    spider_name="youtube.com",
    parameters={"url": "https://youtube.com/watch?v=..."},
    common_settings=CommonSettings(resolution="1080p")
)
```

**Wait & Download:**
```python
status = client.wait_for_task(task_id)
if status == "ready":
    url = client.get_task_result(task_id)
    print(url)
```

### Account Management

```python
# Usage Statistics
stats = client.get_usage_statistics("2024-01-01", "2024-01-31")
print(f"Balance: {stats.balance_gb():.2f} GB")

# Proxy Users
users = client.list_proxy_users()
print(f"Sub-users: {users.user_count}")

# Whitelist IP
client.add_whitelist_ip("1.2.3.4")
```

### Proxy Network

```python
from thordata import ProxyConfig

# Generate Proxy URL
proxy_url = client.build_proxy_url(
    username="proxy_user",
    password="proxy_pass",
    country="us",
    city="ny"
)

# Use with requests
import requests
requests.get("https://httpbin.org/ip", proxies={"http": proxy_url, "https": proxy_url})
```

---

## ðŸ“„ License

MIT License

========================================================================
FILE: requirements.txt
SIZE: 350
TRUNCATED: no
========================================================================
# Runtime Dependencies (Libraries required for the SDK to run)
requests>=2.28.1
aiohttp>=3.8.1

# Development and Testing Dependencies (Not essential for runtime, but used for development and quality assurance)
pytest>=7.0,<9.0
requests-mock>=1.12.1
aioresponses>=0.7.6
pytest-asyncio>=0.23.0
twine>=4.0.2
build>=0.8.0
python-dotenv>=1.0.0

========================================================================
FILE: .github\dependabot.yml
SIZE: 473
TRUNCATED: no
========================================================================
version: 2
updates:
  - package-ecosystem: "pip"
    directory: "/"
    schedule:
      interval: "weekly"
    open-pull-requests-limit: 10
    labels:
      - "dependencies"
    # Optional: group dev dependency updates to reduce PR noise
    groups:
      dev-dependencies:
        patterns:
          - "pytest*"
          - "black"
          - "ruff"
          - "mypy"
          - "types-*"
          - "python-dotenv"
          - "pytest-httpserver"

========================================================================
FILE: .github\pull_request_template.md
SIZE: 391
TRUNCATED: no
========================================================================
## Summary

Describe the changes in this PR.

## Type of change

- [ ] Bug fix
- [ ] New feature
- [ ] Documentation update
- [ ] Refactor / Chore

## Related issue (if any)

Fixes #...

## Checklist

- [ ] Code compiles / runs locally
- [ ] Tests added or updated (if applicable)
- [ ] `python -m pytest` passes locally
- [ ] Docs / README / examples updated (if needed)

========================================================================
FILE: .github\ISSUE_TEMPLATE\bug_report.md
SIZE: 1812
TRUNCATED: no
========================================================================
---
name: Bug report
about: Help us improve thordata-python-sdk by reporting a bug
title: "[BUG] "
labels: bug
assignees: ''
---

## Describe the bug

A clear and concise description of what the bug is.

> Example: `client.serp_search(...)` raises `KeyError` when using `engine="google_news"`.

---

## To Reproduce

Please provide a **minimal, selfâ€‘contained** code snippet that reproduces the issue:

```python
from thordata import ThordataClient, Engine

client = ThordataClient(
    scraper_token="...",
    public_token="...",
    public_key="...",
)

# minimal reproducer
...
```

### Code snippet / steps:
1. 
2. 
3. 

### What command you ran
(e.g. `python script.py`, `pytest`):

```
```

### What happened
Paste error message / stack trace below.

```text
<full error / stack trace here>
```

---

## Expected behavior

A clear description of what you expected to happen instead.

---

## Which API(s) are you using?

Please check all that apply:

- [ ] Proxy `client.get(...)`
- [ ] SERP API `client.serp_search(...)`
- [ ] Universal Scraping `client.universal_scrape(...)`
- [ ] Web Scraper API `create_scraper_task` / `get_task_status` / `get_task_result`
- [ ] Locations API `list_countries` / `list_states` / `list_cities` / `list_asn`
- [ ] Other: 

---

## Environment

- **OS:** (e.g. Windows 11 / macOS 14 / Ubuntu 22.04)
- **Python version:** (e.g. 3.10.12)
- **SDK version:** (e.g. 0.3.0)

### How did you install the SDK?

- [ ] `pip install thordata-sdk`
- [ ] `pip install -e .` (from source)
- [ ] Other: 

---

## Additional context

Any other context, screenshots or logs that might help us understand the problem.

> Please mask any real tokens / credentials before pasting logs or responses.

========================================================================
FILE: .github\ISSUE_TEMPLATE\feature_request.md
SIZE: 1112
TRUNCATED: no
========================================================================
---
name: Feature request
about: Suggest an idea or improvement for thordata-python-sdk
title: "[FEAT] "
labels: enhancement
assignees: ''
---

## Is your feature request related to a problem? Please describe

A clear and concise description of what the problem is.

> Example: "I want to easily list supported locations (countries/states),
> but currently I have to manually call the public API endpoints."

---

## Describe the solution you'd like

A clear and concise description of what you want to happen.

- What new method / option / behavior do you expect?
- Which API(s) should it work with? (Proxy / SERP / Universal / Web Scraper / Locations / ...)

> Example:
> - Add `client.list_locations(proxy_type=1)` that wraps
>   `list_countries`, `list_states`, etc.

---

## Describe alternatives you've considered

If you have any workarounds (custom code, shell commands, etc.), please mention them here.

---

## Additional context

Any other relevant information, links, or examples.

> If this feature is inspired by another SDK or tool, feel free to link to it.

========================================================================
FILE: .github\workflows\ci.yml
SIZE: 2986
TRUNCATED: no
========================================================================
name: CI

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  security-audit:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies (for audit)
        run: |
          python -m pip install --upgrade pip
          python -m pip install ".[dev]"

      - name: Install pip-audit
        run: |
          python -m pip install pip-audit

      - name: Audit installed environment
        run: |
          pip-audit
          
  test:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.9", "3.10", "3.11", "3.12"]

    steps:
      - uses: actions/checkout@v4
        with:
          submodules: recursive
          
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ matrix.python-version }}-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run linter (Ruff)
        run: |
          ruff check src tests examples

      - name: Run formatter check (Black)
        run: |
          black --check src tests examples

      - name: Run type checker (MyPy)
        run: |
          mypy src --ignore-missing-imports

      - name: Compile examples
        run: |
          python -m compileall examples

      - name: Run tests with coverage
        run: |
          pytest --cov=thordata --cov-report=xml --cov-report=term-missing

      - name: Upload coverage to Codecov
        if: matrix.python-version == '3.11'
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage.xml
          fail_ci_if_error: false

  build:
    runs-on: ubuntu-latest
    needs: test

    steps:
      - uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install build tools
        run: |
          python -m pip install --upgrade pip
          pip install build twine

      - name: Build package
        run: python -m build

      - name: Check package
        run: twine check dist/*

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist
          path: dist/

========================================================================
FILE: .github\workflows\pypi-publish.yml
SIZE: 762
TRUNCATED: no
========================================================================
name: Publish to PyPI

on:
  push:
    tags:
      - "v*"

permissions:
  contents: read
  id-token: write  # required for OIDC Trusted Publishing

jobs:
  build-and-publish:
    runs-on: ubuntu-latest
    environment: pypi

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install build tools
        run: |
          python -m pip install --upgrade pip
          python -m pip install build twine

      - name: Build
        run: python -m build

      - name: Twine check
        run: python -m twine check dist/*

      - name: Publish to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1

========================================================================
FILE: docs\serp_reference.md
SIZE: 3799
TRUNCATED: no
========================================================================
# Thordata SERP API Reference (SDK-aligned)

> For a comprehensive parameter reference, see [serp_reference_legacy.md](./serp_reference_legacy.md).
This document is an **SDK-aligned reference** for Thordata SERP API usage.
For the authoritative and up-to-date parameter list, always refer to the official docs:
- Google Search parameters: doc.thordata.com (Google Search)
- Google News parameters: doc.thordata.com (Google News)
- Google Shopping parameters: doc.thordata.com (Google Shopping)
- Yandex parameters: doc.thordata.com (Yandex)

## Endpoint & Authentication

- Endpoint: `POST https://scraperapi.thordata.com/request`
- Headers:
  - `Authorization: Bearer <THORDATA_SCRAPER_TOKEN>`
  - `Content-Type: application/x-www-form-urlencoded`

## Billing & Response Codes

Billing applies **only** when API response `code == 200`.
Other codes are **not billed**.

Common codes include:
- `200`: Success (billed)
- `300`: Not collected (not billed)
- `400`: Bad Request (not billed)
- `401`: Unauthorized (not billed)
- `403`: Forbidden (not billed)
- `404`: Not Found (not billed)
- `429`: Too Many Requests (not billed)
- `500`: Internal Server Error (not billed)
- `504`: Timeout Error (not billed)

## SDK Quick Start

```python
from thordata import ThordataClient

client = ThordataClient(scraper_token="YOUR_TOKEN")

data = client.serp_search(
    query="pizza",
    engine="google",
    country="us",
    language="en",
    num=10,
)

print(len(data.get("organic", [])))
```

## Engine Selection (Recommended)

Use dedicated engines for verticals when available:

- Web search: google, bing, duckduckgo, yandex
- Google News: google_news
- Google Shopping: google_shopping
- Google Product: google_product

Why: dedicated engines typically have clearer parameter contracts and reduce ambiguity.

## Parameter Mapping (SDK -> API)

The SDK sends URL-encoded form fields.

Common fields:

- `engine`: search engine identifier (e.g., google, google_news)
- `q`: query (Google/Bing/DuckDuckGo)
- `text`: query (Yandex)
- `gl`: country (Google)
- `hl`: language (Google)
- `num`: results per page
- `start`: result offset (Google)
- `json`: output format (commonly json=1)

Notes:

- Yandex uses `text` (not `q`) for the query field.
- For Google web search types, `tbm` may be used (images/news/videos/shopping) but using dedicated engines like `google_news` / `google_shopping` is recommended.

## Examples

### Google Search (web)

```bash
curl -X POST https://scraperapi.thordata.com/request \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -H "Authorization: Bearer token" \
  -d "engine=google" \
  -d "q=pizza" \
  -d "json=1"
```

### Google News

```bash
curl -X POST https://scraperapi.thordata.com/request \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -H "Authorization: Bearer token" \
  -d "engine=google_news" \
  -d "q=pizza" \
  -d "json=1"
```

### Google Shopping

```bash
curl -X POST https://scraperapi.thordata.com/request \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -H "Authorization: Bearer token" \
  -d "engine=google_shopping" \
  -d "q=pizza" \
  -d "json=1"
```

### Yandex

```bash
curl -X POST https://scraperapi.thordata.com/request \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -H "Authorization: Bearer token" \
  -d "engine=yandex" \
  -d "text=pizza" \
  -d "json=1"
```

---

> The engines and request examples involved in this document can all be found on the corresponding pages of the official website: Google Search / Google News / Google Shopping / Yandex. <!--citation:3-->  
> Billing and code tables come from SERP API Billing Instructions. <!--citation:4-->

========================================================================
FILE: docs\serp_reference_legacy.md
SIZE: 48763
TRUNCATED: no
========================================================================
# Thordata SERP API Reference (Python SDK)

> **SDK Version**: 0.4.0  
> **Last Updated**: 2025-12-16

This document explains how to call Thordata's SERP API via `thordata-python-sdk`, covering:

- **Google** sub-features  
  Search / Shopping / Local / Videos / News / Product / Flights / Images / Lens / Trends / Hotels / Play / Jobs / Scholar / Maps / Finance / Patents
- **Bing** sub-features  
  Search / News / Shopping / Maps / Images / Videos
- **Yandex** Search
- **DuckDuckGo** Search

All examples use the unified entry point:

```python
from thordata import ThordataClient, Engine

client = ThordataClient(scraper_token="YOUR_SCRAPER_TOKEN")
```

## Important

API parameter names in this document (e.g. gl, hl, shoprs, efirst) come directly from the SERP documentation and are the canonical reference.
The SDK exposes some convenience fields (e.g. query, country, language, num, start) and forwards all other parameters via **kwargs unchanged.
When in doubt, prefer the parameter names listed under "API param" in each section.

## 0. Common Calling Pattern

All SERP calls follow the same pattern:

```python
results = client.serp_search(
    query="your query",           # API param: q (if applicable)
    engine=Engine.GOOGLE,         # or Engine.BING / Engine.YANDEX / Engine.DUCKDUCKGO
    search_type="news",           # Google: maps to tbm or mode; Bing: maps to mode; others: ignored
    country="us",                 # API param: gl (where applicable)
    language="en",                # API param: hl (where applicable)
    num=10,                       # items per page (if supported by engine/mode)
    start=0,                      # result offset / page index (if supported)
    # Mode-specific parameters passed directly via kwargs:
    topic_token="...",
    shoprs="...",
    departure_id="CDG,ORY",
    # ...
)
```

### 0.1 SDK vs. API parameter mapping

Where applicable, the SDK provides the following convenience mappings:

| SDK argument | API param (Google example) | Description |
|-------------|---------------------------|-------------|
| query | q | Text search query. |
| country | gl | Country/region code. |
| language | hl | UI/result language code. |
| num | num | Number of results per page. |
| start | start | Result offset. |

All other parameters should be passed to serp_search using their documented API names as keyword arguments (e.g. shoprs, topic_token, departure_id, filters, within, etc.). The SDK forwards them unchanged to the SERP API.

## 1. Google Mode Overview

Supported Google sub-features:

1.1 Google Search  
1.2 Google Shopping  
1.3 Google Local  
1.4 Google Videos  
1.5 Google News  
1.6 Google Product  
1.7 Google Flights  
1.8 Google Images  
1.9 Google Lens  
1.10 Google Trends  
1.11 Google Hotels  
1.12 Google Play  
1.13 Google Jobs  
1.14 Google Scholar  
1.15 Google Maps  
1.16 Google Finance  
1.17 Google Patents

All are accessed through:

```python
results = client.serp_search(..., engine=Engine.GOOGLE, search_type="...") 
```

Unless explicitly stated, Google sub-features share the same basic localization and pagination behavior as 1.1 Google Search.

### 1.1 Google Search

#### Calling method

```python
results = client.serp_search(
    query="pizza",               # q
    engine=Engine.GOOGLE,
    google_domain="google.com",  # google_domain
    country="us",                # gl
    language="en",               # hl
    num=10,                      # num
    start=0,                     # start
    # Optional filters:
    countries_filter="countryFR|countryDE",  # cr
    languages_filter="lang_fr|lang_de",      # lr
    location="San Francisco",    # location
    uule="BASE64_ENCODED",       # uule
    search_type=None,            # tbm; e.g. 'isch','nws','shop','vid'
    safe_search=True,            # safe
    no_autocorrect=True,         # nfpr
    filter_duplicates=True,      # filter
    # Low-level parameters (kwargs):
    ibp="...", lsig="...", si="...", uds="...", tbs="qdr:m",
)

organic_results = results.get("organic_results", [])
```

#### Parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| q | query | Yes | Search query used for crawling. Any language is supported. |
| google_domain | google_domain | No | Google domain used for scraping. Default: google.com. |
| gl | country or gl (kwargs) | No | Country/region code for localized results (e.g. us, ru, uk). |
| hl | language or hl (kwargs) | No | UI language code for search results (e.g. en, es, zh-CN). |
| cr | countries_filter or cr | No | Restrict results to one or more specific countries. Use countryXX codes separated by \|, e.g. countryFR\|countryDE. Overrides gl. |
| lr | languages_filter or lr | No | Restrict results to one or more languages using lang_XX codes, e.g. lang_fr\|lang_de. Overrides hl. |

##### Geographical location

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| location | location | No | Human-readable location string (e.g., San Francisco, California, United States). Often paired with uule. |
| uule | uule | No | Base64-encoded location used internally by Google. |

##### Search type

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| tbm | search_type or tbm (kwargs) | No | Search vertical. Common values: isch (Images), shop (Shopping), nws (News), vid (Videos). SDK maps search_type="images", "shopping", "news", "videos" accordingly. |

##### Pagination

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| start | start | No | Result offset. 0 (default) = first page, 10 = second, 20 = third, etc. |
| num | num | No | Number of results per page. |

##### Advanced parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| ludocid | ludocid | No | Google Place ID (CID) of a Google My Business listing. |
| kgmid | kgmid | No | Google Knowledge Graph ID. Directly targets a specific entity. |
| ibp | ibp | No | Internal rendering / element parameter (e.g., to expand Knowledge Graph cards). |
| lsig | lsig | No | Internal signature parameter, often used to force map/Knowledge Graph views. |
| si | si | No | Cached-search parameter. Overrides all other parameters except start and num. Used for Knowledge Graph tabs. |
| uds | uds | No | Filter/search parameter. Similar to si, used to fetch certain cached or filtered views. |

##### Advanced filters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| tbs | tbs or time_filter (shortcut) | No | Advanced search parameters. Time examples: qdr:h (past hour), qdr:d (past day), qdr:w (past week), qdr:m (past month), qdr:y (past year). Also used for other filters (e.g., image rights). |
| safe | safe_search or safe | No | Adult content filter: active (strict; default) or off. |
| nfpr | no_autocorrect or nfpr | No | Disable automatic spelling correction when set to 1. |
| filter | filter_duplicates or filter | No | Duplicate/omitted result handling: 1 (default) to enable "Similar/Omitted Results" filters; 0 to disable. |

### 1.2 Google Shopping

#### Calling method

```python
results = client.serp_search(
    query="iPhone 15",           # q
    engine=Engine.GOOGLE,
    search_type="shopping",      # tbm=shop
    google_domain="google.com",
    country="us",                # gl
    language="en",               # hl
    start=0,
    num=20,
    # Shopping-specific filters:
    shoprs="FILTER_ID",
    min_price=500,
    max_price=1500,
    sort_by=1,                   # 1 = price lowâ†’high; 2 = price highâ†’low
    free_shipping=True,
    on_sale=True,
    small_business=False,
    direct_link=True,
)

shopping_results = results.get("shopping_results", results.get("organic", []))
```

All Search localization and pagination parameters (Section 1.1) are supported.

#### Shopping-specific parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| shoprs | shoprs | No | Shopping filter ID. Typically copied from Google Shopping URL when specific filters are applied. |
| min_price | min_price | No | Minimum product price. Filters results to price â‰¥ this value. |
| max_price | max_price | No | Maximum product price. Filters results to price â‰¤ this value. |
| sort_by | sort_by | No | Sorting criterion: 1 â€“ price lowâ†’high; 2 â€“ price highâ†’low. |
| free_shipping | free_shipping | No | Boolean (true / false). Show only products with free shipping. |
| on_sale | on_sale | No | Boolean. Show only products currently on sale. |
| small_business | small_business | No | Boolean. Filter for small-business sellers. |
| direct_link | direct_link | No | Boolean. Include direct merchant links (behavior depends on Google Shopping). |

### 1.3 Google Local

#### Calling method

```python
results = client.serp_search(
    query="pizza near me",       # q
    engine=Engine.GOOGLE,
    search_type="local",         # tbm="local"
    google_domain="google.com",
    country="us",                # gl
    language="en",               # hl
    location="San Francisco",
    uule="ENCODED_LOCATION",
    start=0,                     # 0, 20, 40, ...
    ludocid="OPTIONAL_CID",
)

local_results = results.get("local_results", results.get("organic", []))
```

Google Local reuses most Search parameters. The main differences are:

#### Local-specific behavior / parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| start | start | No | Local results only accept multiples of 20 (0, 20, 40, ...). |
| ludocid | ludocid | No | Google Place ID (CID) for a specific business. Can be used to focus on one POI. |

All other parameters (google_domain, gl, hl, location, uule, tbs, etc.) behave as in Search.

### 1.4 Google Videos

#### Calling method

```python
results = client.serp_search(
    query="python async tutorial",  # q
    engine=Engine.GOOGLE,
    search_type="videos",           # tbm=vid
    google_domain="google.com",
    country="us",                   # gl
    language="en",                  # hl
    lr="lang_en|lang_fr",
    location="United States",
    uule="ENCODED_LOCATION",
    start=0,
    num=10,
    tbs="qdr:m",                    # or time_filter="month"
    safe=True,
    nfpr=1,
    filter=1,
)

video_results = results.get("video_results", results.get("organic", []))
```

#### Parameters

Google Videos uses the same set of parameters as Search (Section 1.1):

- **Localization**: google_domain, gl, hl, lr
- **Geotargeting**: location, uule
- **Pagination**: start, num
- **Advanced filters**: tbs, safe, nfpr, filter

No additional video-exclusive parameters are required.

### 1.5 Google News

#### Calling method

```python
results = client.serp_search(
    query="AI regulation",       # q
    engine=Engine.GOOGLE,
    search_type="news",          # tbm=nws
    google_domain="google.com",
    country="us",                # gl
    language="en",               # hl
    topic_token="...",           # optional
    publication_token="...",     # optional
    section_token="...",         # optional
    story_token="...",           # optional
    so=1,                        # 0 = Relevance, 1 = Date
)

news_results = results.get("news_results", results.get("organic", []))
```

Basic localization (gl, hl) behaves as in Search.

#### News-specific parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| topic_token | topic_token | No | Token for a Google News topic (e.g., World, Business, Technology). |
| publication_token | publication_token | No | Token for a specific publisher (e.g., CNN, BBC). |
| section_token | section_token | No | Token for a subsection of a topic (e.g., Business â†’ Economy). |
| story_token | story_token | No | Token identifying a particular story cluster (full coverage). |
| so | so | No | Sort order: 0 â€“ relevance (default); 1 â€“ date (latest first). |

### 1.6 Google Product

#### Calling method

```python
results = client.serp_search(
    query="",                      # not required for Product mode
    engine=Engine.GOOGLE,
    search_type="product",
    product_id="PRODUCT_ID_HERE",
    google_domain="google.com",
    country="us",                  # gl
    language="en",                 # hl
    # One of the following (mutually exclusive with offer_id):
    offers=True,                   # API: offers=1
    # specs=True,                  # API: specs=1
    # reviews=True,                # API: reviews=1
    # Optional:
    # offer_id="OFFER_ID",         # mutually exclusive with offers/specs/reviews
    # page=1,                      # when offers enabled
    # filter="...",                # advanced filters
)

product_results = results.get("product_results", results.get("shopping_results", []))
```

#### Core product parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| product_id | product_id | Yes | Product ID from URLs like https://www.google.com/shopping/product/{product_id}. |

##### Views and mode selection

The following flags are mutually exclusive with each other and with offer_id.

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| offers | offers | No | Enable offers results (online sellers). Accepts 1 or true. |
| specs | specs | No | Enable product specification results. Accepts 1 or true. |
| reviews | reviews | No | Enable review results. Accepts 1 or true. |

##### Pagination for offers

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| start | start | No | Standard offset: start=30 skips first 30 results. |
| page | page | No | Page number for online sellers (10 results per page). Equivalent to start = page * 10. Only available when offers is enabled. |

##### Advanced filters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| filter | filter | No | Complex filter string for reviews and offers (sorting, pagination, etc.). |
| offer_id | offer_id | No | ID used to get multiple offers from online sellers (from sellers_results.online_sellers). Cannot be used with offers, specs, or reviews. |

Standard localization parameters (google_domain, gl, hl, location, uule) work as in Search.

### 1.7 Google Flights

#### Calling method

```python
results = client.serp_search(
    query="",                        # not used
    engine=Engine.GOOGLE,
    search_type="flights",
    departure_id="CDG,ORY",          # airport codes or kgmids
    arrival_id="AUS,/m/0vzm",
    outbound_date="2025-08-31",
    return_date="2025-09-06",
    travel_class=2,                  # 1=Economy, 2=Premium economy, 3=Business, 4=First
    adults=2,
    children=1,
    infants_in_seat=0,
    infants_on_lap=0,
)

flights_results = results.get("flights_results", results)
```

#### Parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| departure_id | departure_id | Yes | One or more departure airports or location kgmids, comma-separated. Airport codes are 3-letter uppercase (e.g., CDG). Location kgmids start with /m/ (e.g., /m/0vzm). |
| arrival_id | arrival_id | Yes | One or more arrival airports or location kgmids, comma-separated. |

##### Dates

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| outbound_date | outbound_date | No | Outbound date in YYYY-MM-DD format (e.g., 2025-08-31). |
| return_date | return_date | No | Return date in YYYY-MM-DD format (e.g., 2025-09-06). Optional for one-way trips. |

##### Travel class

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| travel_class | travel_class | No | Travel class: 1 â€“ Economy (default), 2 â€“ Premium economy, 3 â€“ Business, 4 â€“ First. |

##### Number of passengers

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| adults | adults | No | Number of adults. Default: 1. |
| children | children | No | Number of children. Default: 0. |
| infants_in_seat | infants_in_seat | No | Number of infants in their own seats. Default: 0. |
| infants_on_lap | infants_on_lap | No | Number of lap infants. Default: 0. |

### 1.8 Google Images

#### Calling method

```python
results = client.serp_search(
    query="cute cats",             # q
    engine=Engine.GOOGLE,
    search_type="images",          # tbm=isch
    google_domain="google.com",
    g1="us",                       # country (API: g1)
    h1="en",                       # language (API: h1)
    cr="countryUS|countryCA",
    location="United States",
    uule="ENCODED_LOCATION",
    # Time period:
    period_unit="d",               # s/n/h/d/w/m/y
    period_value=7,                # default 1
    start_date="20240101",         # YYYYMMDD
    end_date="20241231",           # YYYYMMDD
    # Pagination:
    ijn=0,                         # image page index
    # Filters:
    chips="red apple",
    imgar="w",
    imgsz="large",
    image_color="red",
    image_type="photo",
    licenses="fc",
    safe="active",
    nfpr=1,
    filter=1,
)

images_results = results.get("images_results", results.get("organic", []))
```

#### Localization & geotargeting

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| google_domain | google_domain | No | Google domain for image search. |
| g1 | g1 | No | Country for image search results. |
| h1 | h1 | No | Language for image results. |
| cr | cr | No | Multiple-country filter, same semantics as in Search. |
| location | location | No | Human-readable location string. |
| uule | uule | No | Encoded location. |

#### Time period

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| period_unit | period_unit | No | Unit for recent-time image search: s (second), n (minute), h (hour), d (day), w (week), m (month), y (year). |
| period value | period_value | No | Time period multiplier, combined with period_unit (e.g., 42 hours, 178 days). Default: 1. Range: 1..2147483647. |
| start date | start_date | No | Start date of a custom time period, in YYYYMMDD format. |
| end date | end_date | No | End date of a custom time period, in YYYYMMDD format (e.g., 20241231). |

#### Pagination

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| ijn | ijn | No | Page number for Google Images. Each page contains 100 images. Equivalent to start = ijn * 100. |

#### Advanced filters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| chips | chips | No | Alternative image search filter. String from suggested searches (e.g., red apple), available under suggested_searches when ijn=0. |
| tbs | tbs | No | Raw advanced search parameter string for options not exposed elsewhere. |
| imgar | imgar | No | Aspect ratio: s (square), t (tall), w (wide), xw (panoramic). |
| imgsz | imgsz | No | Image size: l (large), m (medium), i (icon), or thresholds like qsvga, vga, svga, xga, 2mp, 4mp, 6mp, 8mp, 10mp, 12mp, 15mp, 20mp, 40mp. |
| image_color | image_color | No | Color filter: bw, trans, red, orange, yellow, green, teal, blue, purple, pink, white, gray, black. |
| image_type | image_type | No | Type filter: face, photo, clipart, lineart, animated. |
| licenses | licenses | No | Usage rights: f (free to use/share), fc (free to use/share, even commercially), fm (free to use/share/modify), fmc (free to use/share/modify, even commercially), cl (Creative Commons), ol (commercial & other licenses). |
| safe | safe | No | Adult filter: active (strict; default), off. |
| nfpr | nfpr | No | Exclude results from auto-corrected queries when set to 1. |
| filter | filter | No | Turn Similar/Omitted results filters on (1, default) or off (0). |

### 1.9 Google Lens

#### Calling method

```python
results = client.serp_search(
    query="",                       # optional text query for some types
    engine=Engine.GOOGLE,
    search_type="lens",
    url="https://i.imgur.com/HBrB8p0.png",  # image URL
    country="us",                   # gl
    language="en",                  # hl
    type="products",                # all | products | exact_matches | visual_matches
    q="acting",                     # extra text query (optional)
    safe="active",
)

lens_results = results.get("lens_results", results)
```

#### Parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| url | url | Yes | Public URL of the image to use in the Lens search. |
| gl | country or gl | No | Country/region code. |
| hl | language or hl | No | UI language code. |

##### Search type

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| type | type | No | Type of Lens search: all (default), products, exact_matches, visual_matches. |

##### Advanced parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| q | q or query | No | Additional search query text. Only applicable for all, products, and visual_matches. |
| safe | safe | No | Adult content filtering: active (strict; default) or off. |

### 1.10 Google Trends

#### Calling method

```python
results = client.serp_search(
    query="Python",              # q
    engine=Engine.GOOGLE,
    search_type="trends",
    language="en",               # hl
    geo="US",
    region="COUNTRY",            # COUNTRY / REGION / DMA / CITY
    data_type="TIMESERIES",
    tz=420,                      # minutes, e.g., 420 = PDT (UTC-7)
    cat=0,                       # category, 0 = all
    gprop="news",                # images | news | froogle | youtube
    date="today 12-m",           # time range
)

trends_results = results.get("trends_results", results)
```

#### Parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| q | query | Yes | Search query for Google Trends. |
| hl | language or hl | No | Language code. |

##### Geographical location

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| geo | geo | No | Location code. Defaults to Worldwide when empty. See Google Trends Locations for full list. |
| region | region | No | Used only with GEO_MAP and related charts to refine geography. Options: COUNTRY, REGION, DMA, CITY. |

##### Search type

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| data_type | data_type | No | Trend chart type: TIMESERIES (interest over time), GEO_MAP (compared breakdown by region; multiple queries), GEO_MAP_0 (interest by region; single query), RELATED_TOPICS, RELATED_QUERIES. |

##### Advanced parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| tz | tz | No | Time zone offset in minutes (default 420 = PDT). Range: -1439..1439. |
| cat | cat | No | Category ID. Default 0 for all categories. |
| gprop | gprop | No | Property: images, news, froogle (Shopping), youtube, or empty for Web Search. |
| date | date | No | Date/time range. Options include: now 1-H, now 4-H, now 1-d, now 7-d, today 1-m, today 3-m, today 12-m, today 5-y, all, or custom ranges like 2021-10-15 2022-05-25. |

### 1.11 Google Hotels

#### Calling method

```python
results = client.serp_search(
    query="hotels in New York",   # q
    engine=Engine.GOOGLE,
    search_type="hotels",
    google_domain="google.com",
    country="us",                 # gl
    language="en",                # hl
    check_in_date="2025-08-22",
    check_out_date="2025-08-23",
    adults=2,
)

hotels_results = results.get("hotels_results", results)
```

#### Parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| q | query | Yes | Hotel search query. |
| google_domain | google_domain | No | Google domain. |
| gl | country or gl | No | Country/region code. |
| hl | language or hl | No | Language code. |

##### Advanced parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| check_in_date | check_in_date | No | Check-in date in YYYY-MM-DD format (e.g., 2025-08-22). |
| check_out_date | check_out_date | No | Check-out date in YYYY-MM-DD format (e.g., 2025-08-23). |
| adults | adults | No | Number of adults. Default: 2. |

### 1.12 Google Play

#### Calling method

```python
results = client.serp_search(
    query="todo list",            # q
    engine=Engine.GOOGLE,
    search_type="play",
    g1="us",                      # country
    h1="en",                      # language
    apps_category="PRODUCTIVITY",
    next_page_token="...",
    section_page_token="...",
    chart="top_free",
    see_more_token="...",
    store_device="phone",
    age="9plus",
)

play_results = results.get("play_results", results)
```

#### Parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| q | query | Yes | Search query for Google Play. |

##### Localization

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| g1 | g1 | No | Country code for Play content. |
| h1 | h1 | No | Language code for Play content. |

##### Search type / filters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| apps_category | apps_category | No | App category filter (e.g., Productivity, Games, etc.). |

##### Pagination / navigation

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| next_page_token | next_page_token | No | Token for retrieving the next page of results. |
| section_page_token | section_page_token | No | Token for navigating sections within Play results. |
| chart | chart | No | Chart selection (e.g., top free, top grossing). |
| see_more_token | see_more_token | No | Token for "see more" sections. |

##### Advanced parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| store_device | store_device | No | Device type (e.g., phone, tablet). |
| age | age | No | Age filter for content (e.g., age ratings). |

### 1.13 Google Jobs

#### Calling method

```python
results = client.serp_search(
    query="data scientist",       # q
    engine=Engine.GOOGLE,
    search_type="jobs",
    google_domain="google.com",
    country="us",                 # gl
    language="en",                # hl
    location="San Francisco",
    uule="ENCODED_LOCATION",
    start=0,
    page=0,
    next_page_token="1",
    lrad=50,                      # search radius (km)
    ltype=1,                      # work from home filter
    uds="FILTER_STRING",
)

jobs_results = results.get("jobs_results", results)
```

#### Parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| q | query | Yes | Job search query. |
| google_domain | google_domain | No | Google domain. |
| gl | country or gl | No | Country/region code. |
| hl | language or hl | No | Language code. |

##### Geographical location

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| location | location | No | Location string (e.g., city name). |
| uule | uule | No | Encoded location. |

##### Pagination

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| start | start | No | Result offset. |
| page | page | No | Page index (number of results per page depends on Google Jobs behavior). |

##### Advanced parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| next_page_token | next_page_token | No | Token for retrieving the next page of results (e.g., 1). |
| lrad | lrad | No | Search radius in kilometers. Radius is not strictly limited. |
| ltype | ltype | No | Work-from-home filter (e.g., 1 to filter for WFH jobs). |
| uds | uds | No | Filter search string provided by Google. Values appear under filters with uds, q, and serpapi_link. |

### 1.14 Google Scholar

#### Calling method

```python
results = client.serp_search(
    query="graph neural networks", # q
    engine=Engine.GOOGLE,
    search_type="scholar",
    language="en",                 # hl
    lr="lang_en",
    location="United States",
    uule="ENCODED_LOCATION",
    start=0,
    num=10,
    as_sdt=0,
    cites="1275980731835430123",
    as_ylo=2018,
    as_yhi=2024,
    scisbd=1,
    cluster=None,
    safe="active",
    nfpr=1,
    filter=1,
    as_vis=0,
    as_rr=0,
)

scholar_results = results.get("scholar_results", results)
```

#### Basic parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| q | query | Yes | Scholar search query. |
| hl | language or hl | No | UI language. |
| lr | lr | No | Language filter (lang_XX codes). |

##### Geographical location

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| location | location | No | Location string. |
| uule | uule | No | Encoded location. |

##### Search type

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| as_sdt | as_sdt | No | Search type / filter. As filter (articles only): 0 â€“ exclude patents (default); 7 â€“ include patents. As search type: 4 â€“ case law (US courts only; selects all state and federal courts). |

##### Pagination

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| start | start | No | Result offset. |
| num | num | No | Number of results per page. |

##### Advanced parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| cites | cites | No | ID for "Cited by" searches. Example: cites=1275980731835430123. Using cites with q triggers search within citing articles. |
| as_ylo | as_ylo | No | From year. Can be combined with as_yhi. |
| as_yhi | as_yhi | No | To year. Can be combined with as_ylo. |
| scisbd | scisbd | No | Sort by date: 1 â€“ last year, abstracts only; 2 â€“ last year, all content; 0 (default) â€“ sort by relevance. |
| cluster | cluster | No | ID for "All versions" searches. Example: cluster=1275980731835430123. Must not be used together with q and cites. |

##### Advanced filters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| safe | safe | No | Adult content filter. |
| nfpr | nfpr | No | Exclude auto-corrected results when set to 1. |
| filter | filter | No | Similar/Omitted results filter: 1 (default) enabled, 0 disabled. |
| as_vis | as_vis | No | Show citations: 1 â€“ exclude citation-only items; 0 (default) â€“ include them. |
| as_rr | as_rr | No | Show only review/comment articles: 1 â€“ review-only; 0 (default) â€“ all results. |

### 1.15 Google Maps

#### Calling method

```python
results = client.serp_search(
    query="coffee near me",       # q
    engine=Engine.GOOGLE,
    search_type="maps",
    google_domain="google.com",
    country="us",                 # gl
    language="en",                # hl
    ll="@40.7455096,-74.0083012,14z",
    type="search",                # search | place
    start=0,
    data="DATA_STRING",
    place_id=None,
    data_cid=None,
)

maps_results = results.get("maps_results", results.get("local_results", []))
```

#### Parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| q | query | Yes | Query text for Maps search. |
| google_domain | google_domain | No | Google domain. |
| gl | country or gl | No | Country code. |
| hl | language or hl | No | Language code. |

##### Geographical location

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| ll | ll | No | GPS coordinates of origin in the format: @<latitude>,<longitude>,<scale>, e.g. @40.7455096,-74.0083012,14z. |

##### Search type

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| type | type | No | Type of search: search â€“ list of results when q is set; place â€“ details for a specific location when data is set. No type is required when using place_id or data_cid directly. |

##### Pagination

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| start | start | No | Result offset for Maps results. (For Local results, only multiples of 20 may be valid.) |

##### Advanced parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| data | data | No | Complex filter string copied from the Google Maps URL after setting filters on the website. |
| place_id | place_id | No | Unique reference to a place in Google Maps. Works for businesses, landmarks, parks, intersections. Can be used without any other parameter. |
| data_cid | data_cid | No | Google CID (customer identifier) for a place. Often labeled as place_id in other APIs. |

### 1.16 Google Finance

#### Calling method

```python
results = client.serp_search(
    query="GOOGL",                # q
    engine=Engine.GOOGLE,
    search_type="finance",
    language="en",                # hl
    window="1M",
)

finance_results = results.get("finance_results", results)
```

#### Parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| q | query | Yes | Finance search query (ticker, company name, etc.). |
| hl | language or hl | No | UI language. |

##### Advanced parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| window | window | No | Time range for graph: 1D (default, 1 day), 5D, 1M, 6M, YTD, 1Y, 5Y, MAX. |

### 1.17 Google Patents

#### Calling method

```python
results = client.serp_search(
    query="machine learning optimization",  # q
    engine=Engine.GOOGLE,
    search_type="patents",
    before="priority:20241231",
    after="filing:20200101",
    inventor="John Doe,Jane Smith",
    assignee="Google LLC",
    page=0,
    num=10,
    sort="new",
    clustered="true",
    dups="language",
    patents=True,
    scholar=False,
)

patent_results = results.get("patent_results", results)
```

#### Query

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| q | query | Yes | Patent search query. |

#### Date range

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| before | before | No | Limit results to dates on or before a specified date. Format: "type:YYYYMMDD", where type is priority, filing, or publication. Example: priority:20241231. |
| after | after | No | Limit results to dates on or after a specified date, same "type:YYYYMMDD" format. |

#### Participants

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| inventor | inventor | No | Inventor name(s), separated by commas. |
| assignee | assignee | No | Assignee name(s), separated by commas. |

#### Pagination

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| page | page | No | Page number (used to compute result offset). |
| num | num | No | Maximum number of results per page (e.g., 10). |

#### Advanced parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| sort | sort | No | Sort order. Default: relevance. Examples: new (newest), old (oldest). |
| clustered | clustered | No | Grouping mode. Example value: true â€“ group by classification. |
| dups | dups | No | Deduplication method. Supported: language â€“ deduplicate by publication (language). Default behavior may be family-level deduplication. |
| patents | patents | No | Include Google Patents results. Default true. |
| scholar | scholar | No | Include Google Scholar results. Default false. |

## 2. Bing Mode

Thordata's SERP API supports:

- Bing Search
- Bing News
- Bing Shopping
- Bing Maps
- Bing Images
- Bing Videos

Use:

```python
engine=Engine.BING
search_type="search" | "news" | "shopping" | "maps" | "images" | "videos"
```

All Bing-specific parameters are passed via kwargs using the exact names from this section.

### 2.1 Bing Search

#### Calling method

```python
results = client.serp_search(
    query="Thordata proxy network",  # q
    engine=Engine.BING,
    cc="us",
    mkt="en-US",
    location="New York",
    lat=40.7128,
    lon=-74.0060,
    first=1,
    count=10,
    adlt="strict",
)
```

#### Parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| q | query | Yes | Search query. |

##### Localization & geolocation

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| cc | cc | No | Country/region code (e.g., us, fr). Influences result localization. |
| mkt | mkt | No | Market/language in the format <language>-<country>, e.g. en-US, zh-CN. |
| location | location | No | Starting geographical location name. Should be used with lat and lon. |
| lat | lat | No | Latitude (-90.0 to 90.0). Use with lon. |
| lon | lon | No | Longitude (-180.0 to 180.0). Use with lat. |

##### Pagination

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| first | first | No | Result offset (1-based). Default 1. first=10 promotes the 10th result to the top. |
| count | count | No | Number of results per page. Range: 1â€“50. |

##### Advanced filters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| adlt | adlt | No | Adult content filter: strict (default, blocks adult content) or off (allows adult content). |

### 2.2 Bing News

#### Calling method

```python
results = client.serp_search(
    query="AI regulation",         # q
    engine=Engine.BING,
    search_type="news",
    cc="us",
    setlang="en",
    mkt="en-US",
    first=1,
    count=10,
    qft="sortbydate=1",            # example filter
)
```

#### Parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| q | query | Yes | News search query. |

##### Localization

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| cc | cc | No | Country/region code. |
| setlang | setlang | No | UI language. |
| mkt | mkt | No | Market/locale (e.g., en-US). |

##### Pagination

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| first | first | No | Result offset (1-based). |
| count | count | No | Results per page. |

##### Advanced filters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| qft | qft | No | Query filter text for sorting, date range, etc. Example: qft="sortbydate=1" to sort by date. |

### 2.3 Bing Shopping

#### Calling method

```python
results = client.serp_search(
    query="laptop",                # q
    engine=Engine.BING,
    search_type="shopping",
    cc="us",
    mkt="en-US",
    efirst=1,
    filters='ex1:"ez5_18169_18230"',
)

shopping_results = results.get("shopping_results", results.get("organic", []))
```

#### Parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| q | query | Yes | Shopping search query. |

##### Localization

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| cc | cc | No | Country/region code. |
| mkt | mkt | No | Market/locale. |

##### Pagination

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| efirst | efirst | No | Shopping offset (default: 1). Works similarly to first, e.g. efirst=10 promotes the 10th result. |

##### Advanced filters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| filters | filters | No | Complex filtering string, e.g. ex1:"ez5_18169_18230" or longer strings like ufn:"Wunderman+Thompson"+sid:"..."+catguid:"..."+segment:"generic.carousel"+entitysegment:"Organization". Copy from Bing Shopping URLs. |

### 2.4 Bing Maps

#### Calling method

```python
results = client.serp_search(
    query="coffee near me",       # q
    engine=Engine.BING,
    search_type="maps",
    setlang="en",
    cp="40.7455096~-74.0083012",
    first=1,
    count=20,
    place_id="PLACE_ID_HERE",
)

maps_results = results.get("maps_results", results.get("local_results", []))
```

#### Parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| q | query | Yes | Maps search query. |

##### Localization & coordinates

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| setlang | setlang | No | UI language. |
| cp | cp | No | GPS coordinates as latitude~longitude, e.g. 40.7455096~-74.0083012. |

##### Pagination

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| first | first | No | Result offset. |
| count | count | No | Results per page. |

##### Advanced filters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| place_id | place_id | No | Unique reference to a place on Bing Maps. Can be used without q to retrieve a specific POI. |

### 2.5 Bing Images

#### Calling method

```python
results = client.serp_search(
    query="sunset",               # q
    engine=Engine.BING,
    search_type="images",
    cc="us",
    mkt="en-US",
    first=1,
    count=50,
    adlt="strict",
)

image_results = results.get("image_results", results)
```

#### Parameters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| q | query | Yes | Image search query. |
| cc | cc | No | Country/region. |
| mkt | mkt | No | Market/locale. |
| first | first | No | Offset of first result. |
| count | count | No | Number of images per page. |
| adlt | adlt | No | Adult content filter: strict or off. |

### 2.6 Bing Videos

#### Calling method

```python
results = client.serp_search(
    query="python tutorial",      # q
    engine=Engine.BING,
    search_type="videos",
    cc="us",
    mkt="en-US",
    first=1,
    count=20,
    adlt="off",
)

video_results = results.get("video_results", results)
```

#### Parameters

Same as Bing Images:

q, cc, mkt, first, count, adlt.

## 3. Yandex

Yandex mode focuses on web search, accessed with:

```python
results = client.serp_search(
    query="python tutorial",      # q
    engine=Engine.YANDEX,
    num=10,
    yandex_domain="yandex.com",
    lang="en",
    lr="Moscow,Russia",
    rstr="family",
    p=2,
    within="7d",
)
```

Internally, query is converted to the Yandex search parameter (text), but you only need to set query.

### 3.1 Parameters

#### Query

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| q | query | Yes | Yandex search query. |

##### Localization

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| yandex_domain | yandex_domain | No | Yandex domain used for crawling. Default: yandex.com. Common: yandex.com, yandex.ru, yandex.com.tr. |
| lang | lang | No | Language used for search results. Default: en. Two-letter codes (e.g., en, ru, es). |

##### Geographical location

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| lr | lr | No | Region for search results, e.g. Moscow,Russia. Has priority over IP-based geolocation. |
| rstr | rstr | No | Location strictness and safe search. Example: family enforces family-safe results and region lock. |

##### Pagination & advanced

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| p | p | No | Page number (0-based). Use together with SDK num to define page size. Example: p=2 â†’ third page. |
| within | within | No | Time/domain restriction. E.g., within="7d" for past 7 days; within="example.com" to restrict to a specific domain. |

## 4. DuckDuckGo

DuckDuckGo mode supports basic web search with localization, pagination, and filters:

```python
results = client.serp_search(
    query="python web scraping",  # q
    engine=Engine.DUCKDUCKGO,
    num=10,
    start=0,
    kl="zh-cn",
    df="d",
    kp=-1,
)
```

### 4.1 Parameters

#### Query

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| q | query | Yes | DuckDuckGo search query. |

##### Localization

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| kl | kl | No | Interface language and region. Example: kl="zh-cn" for Simplified Chinese UI and China-region results. |

##### Pagination

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| start | start | No | Offset of first result. Example: start=30 skips first 30 results (page 2). Max recommended: 500 (quality drops after ~16 pages). |
| num | num | No | Number of results per page (SDK convenience field). |

##### Advanced filters

| API param | SDK argument | Required | Description |
|-----------|-------------|----------|-------------|
| df | df | No | Date filter. Specific values depend on DuckDuckGo's config (e.g., d for day). |
| kp | kp | No | Safe search level (e.g., -1, 1, 2). Semantics follow DuckDuckGo's safe-search configuration. |

## Summary

All SERP engines and modes are accessed via a single SDK method:

```
ThordataClient.serp_search(query=..., engine=..., search_type=..., **kwargs).
```

Common fields (query, engine, country, language, num, start) are explicit SDK arguments; all documented API parameters in this file can be passed directly as keyword arguments with their exact API names.

This reference follows the canonical parameter names from the SERP documentation (e.g. g1, h1, shoprs, efirst, within); use these names in **kwargs to match the underlying API.

========================================================================
FILE: docs\universal_reference.md
SIZE: 10533
TRUNCATED: no
========================================================================
# Thordata Web Unlocker / Universal API Reference (Python SDK)

> **SDK Version**: 0.4.0  
> **Last Updated**: 2025-12-16

Thordata Web Unlocker (name on Dashboard) corresponds to the **Universal Scraping API** in the SDK, with the underlying request endpoint:

- `https://universalapi.thordata.com/request`

In the Python SDK, accessed through the following interfaces:

- `ThordataClient.universal_scrape(...)`
- `ThordataClient.universal_scrape_advanced(UniversalScrapeRequest)`
- `AsyncThordataClient.universal_scrape(...)`
- `AsyncThordataClient.universal_scrape_advanced(UniversalScrapeRequest)`

---

## 0. Common Calling Pattern

### Synchronous

```python
from thordata import ThordataClient

client = ThordataClient(scraper_token="YOUR_SCRAPER_TOKEN")

html = client.universal_scrape(
    url="https://example.com",
    js_render=True,
    output_format="html",
    country="us",
    block_resources="image",
    clean_content="js,css",
    wait=5000,
    wait_for=".content",
)
```

### Asynchronous

```python
import asyncio
from thordata import AsyncThordataClient

async def main():
    async with AsyncThordataClient(scraper_token="YOUR_SCRAPER_TOKEN") as client:
        html = await client.universal_scrape(
            url="https://example.com",
            js_render=True,
            output_format="html",
            country="us",
        )
        print(html[:200])

asyncio.run(main())
```

### Note

The SDK automatically handles the Authorization: Bearer <token> header, you don't need to manually pass the token parameter.

The SDK automatically encodes Python list structures in headers/cookies as JSON strings to comply with API requirements.

## 1. SDK API vs Web Unlocker Parameter Mapping

In the Dashboard documentation, Web Unlocker parameters include:

- token (authentication)
- url
- js_render
- type
- block_resources
- country
- clean_content
- wait
- wait_for
- headers
- cookies

In the SDK's UniversalScrapeRequest / universal_scrape, the corresponding relationships are as follows:

| API param | SDK field/parameter | Type | Required | Default | Description |
|-----------|-------------------|------|----------|---------|-------------|
| token | scraper_token (Client) | str | Yes | â€” | Passed when initializing ThordataClient / AsyncThordataClient, SDK automatically sets Authorization header. |
| url | url | str | Yes | â€” | Target webpage URL to scrape. |
| js_render | js_render | bool | No | False | Whether to enable JS rendering (Headless browser) for SPA/dynamic websites. |
| type | output_format | "html" / "png" | No | "html" | Return HTML text or PNG screenshot. |
| block_resources | block_resources | str | No | None | Resource types to block loading, examples: "script", "image", "css", "media", can be combined. |
| country | country | str | No | None | Proxy country/region code (e.g.: "us", "de", "al"). |
| clean_content | clean_content | str | No | None | Content types to remove from returned results, examples: "js", "css", "js,css". |
| wait | wait | int | No | None | Page loading wait time (milliseconds), maximum 100000. |
| wait_for | wait_for | str | No | None | CSS selector, wait for this element to appear in DOM before returning (higher priority than wait). |
| headers | headers | List[Dict[str, str]] | No | None | Custom request headers list, automatically encoded as JSON string. |
| cookies | cookies | List[Dict[str, str]] | No | None | Custom Cookie list, automatically encoded as JSON string. |
| (Others) | extra_params / **kwargs | Dict[str, Any] | No | {} | Future new parameters can be passed directly via kwargs. |

## 2. UniversalScrapeRequest Model (Internal Structure)

UniversalScrapeRequest dataclass definition in SDK (simplified version):

```python
from dataclasses import dataclass, field
from typing import Optional, Dict, Any, List

@dataclass
class UniversalScrapeRequest:
    url: str
    js_render: bool = False
    output_format: str = "html"  # 'html' or 'png'
    country: Optional[str] = None
    block_resources: Optional[str] = None
    clean_content: Optional[str] = None
    wait: Optional[int] = None
    wait_for: Optional[str] = None
    headers: Optional[List[Dict[str, str]]] = None
    cookies: Optional[List[Dict[str, str]]] = None
    extra_params: Dict[str, Any] = field(default_factory=dict)

    def to_payload(self) -> Dict[str, Any]:
        # Convert to API form parameters
        ...
```

A typical advanced calling approach:

```python
from thordata import ThordataClient, UniversalScrapeRequest

client = ThordataClient(scraper_token="YOUR_SCRAPER_TOKEN")

request = UniversalScrapeRequest(
    url="https://example.com/dashboard",
    js_render=True,
    output_format="html",
    country="us",
    block_resources="image",
    clean_content="js,css",
    wait=8000,
    wait_for=".dashboard-loaded",
    headers=[{"name": "User-Agent", "value": "Mozilla/5.0 (ThordataBot)"}],
    cookies=[{"name": "session", "value": "abc123"}],
)

html = client.universal_scrape_advanced(request)
```

## 3. Common Usage Scenario Examples

### 3.1 Basic HTML Scraping (No JS Rendering)

```python
html = client.universal_scrape(
    url="https://httpbin.org/html",
    js_render=False,
    output_format="html",
)

print(html[:200])
```

**Features:**

- Fast speed
- Suitable for static pages or simple sites that don't depend on JS rendering

### 3.2 Enable JS Rendering (SPA / Dynamic Pages)

```python
html = client.universal_scrape(
    url="https://example.com/spa",
    js_render=True,
    output_format="html",
    wait=5000,                 # Wait 5 seconds
)
```

Or more recommended "wait for specified element" approach:

```python
html = client.universal_scrape(
    url="https://example.com/spa",
    js_render=True,
    output_format="html",
    wait_for=".main-content",  # Wait for .main-content to appear on page
)
```

**Description:**

- When wait_for exists, it takes priority over wait, maximum wait time is generally 30 seconds (controlled by server)
- Suitable for React/Vue/Angular single-page applications

### 3.3 Use Proxy Country (country)

```python
html = client.universal_scrape(
    url="https://www.google.com/search?q=weather",
    js_render=True,
    output_format="html",
    country="de",              # Use German exit IP
)
```

For strongly regional websites/search results (such as Google/Bing/ecommerce sites), setting country allows you to see the real experience of users in the corresponding region.

### 3.4 Block Resources to Improve Performance (block_resources)

```python
html = client.universal_scrape(
    url="https://example.com/heavy-page",
    js_render=True,
    output_format="html",
    block_resources="image,media",  # Block image and media resources
)
```

**Common values:**

- script
- image
- css
- media

Can be combined using comma separation.

### 3.5 Clean Returned Content (clean_content)

```python
html = client.universal_scrape(
    url="https://example.com",
    js_render=True,
    output_format="html",
    clean_content="js,css",      # Remove JS and CSS content from HTML
)
```

**Suitable for:**

- Reducing noise in LLM input (remove script/style content)
- Reducing page size, controlling token usage cost

### 3.6 Pass Custom Headers and Cookies (Advanced Usage)

Recommend using UniversalScrapeRequest + universal_scrape_advanced:

```python
from thordata import UniversalScrapeRequest

request = UniversalScrapeRequest(
    url="https://example.com/account",
    js_render=True,
    output_format="html",
    headers=[
        {"name": "User-Agent", "value": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"},
        {"name": "Accept-Language", "value": "en-US,en;q=0.9"},
    ],
    cookies=[
        {"name": "sessionid", "value": "session_abc123"},
        {"name": "prefs", "value": "dark_mode=1"},
    ],
)

html = client.universal_scrape_advanced(request)
```

**Note:**

The SDK automatically converts headers/cookies to JSON strings, complying with API documentation requirements:

```json
headers=[{"name":"name1","value":"value1"}, ...]
cookies=[{"name":"name2","value":"value2"}, ...]
```

### 3.7 Screenshots (PNG Output)

```python
png_bytes = client.universal_scrape(
    url="https://example.com",
    js_render=True,
    output_format="png",
)

with open("screenshot.png", "wb") as f:
    f.write(png_bytes)
```

**Asynchronous version:**

```python
async with AsyncThordataClient(scraper_token="YOUR_SCRAPER_TOKEN") as client:
    png_bytes = await client.universal_scrape(
        url="https://example.com",
        js_render=True,
        output_format="png",
    )
    ...
```

## 4. Error Handling Recommendations

When using Universal API, it's recommended to catch the following types of exceptions:

```python
from thordata import (
    ThordataClient,
    ThordataError,
    ThordataAuthError,
    ThordataRateLimitError,
    ThordataNetworkError,
    ThordataTimeoutError,
)

client = ThordataClient(scraper_token="YOUR_SCRAPER_TOKEN")

try:
    html = client.universal_scrape(
        url="https://example.com",
        js_render=True,
        output_format="html",
    )
except ThordataAuthError as e:
    print(f"Auth error: {e}. Check your token or account permissions.")
except ThordataRateLimitError as e:
    print(f"Rate limited: {e}. Consider backing off or upgrading your plan.")
except ThordataTimeoutError as e:
    print(f"Timeout: {e}. Try increasing wait time or output timeout.")
except ThordataNetworkError as e:
    print(f"Network error: {e}. Check connectivity or proxy settings.")
except ThordataError as e:
    print(f"Thordata SDK error: {e}")
except Exception as e:
    print(f"Unexpected error: {e}")
```

## Summary

Web Unlocker / Universal API provides a unified way to scrape complex webpages, automatically handling JS rendering, anti-crawling, CAPTCHA and other issues.

In the SDK, most parameters can be set through explicit fields of universal_scrape, and other parameters can be extended via kwargs or extra_params.

For complex scenarios (such as custom headers/cookies, multi-parameter combinations), it's recommended to use the UniversalScrapeRequest + universal_scrape_advanced pattern to keep configuration clear and reusable.

========================================================================
FILE: examples\async_high_concurrency.py
SIZE: 4313
TRUNCATED: no
========================================================================
"""
Async Concurrency Demo (Offline-test friendly)

Demonstrates:
- AsyncThordataClient usage
- Parallel requests with asyncio.gather
- Basic performance measurement
- Error handling in concurrent context

Notes:
- This demo uses the SERP API to make concurrent calls.
- For CI/offline testing, set THORDATA_SCRAPERAPI_BASE_URL to a local mock server.

Environment variables:
- THORDATA_SCRAPER_TOKEN (required)
- THORDATA_CONCURRENCY (optional, default: 10)
- THORDATA_DEMO_QUERY (optional, default: "python")

Usage:
    python examples/async_high_concurrency.py
"""

from __future__ import annotations

import asyncio
import logging
import os
import sys
import time
from typing import Any, Dict

try:
    from dotenv import load_dotenv
except ImportError:
    load_dotenv = None

from thordata import AsyncThordataClient, ThordataError

logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")


def _configure_stdio() -> None:
    # Avoid UnicodeEncodeError on Windows consoles with legacy encodings.
    if hasattr(sys.stdout, "reconfigure"):
        sys.stdout.reconfigure(encoding="utf-8", errors="replace")
    if hasattr(sys.stderr, "reconfigure"):
        sys.stderr.reconfigure(encoding="utf-8", errors="replace")


def _load_env() -> None:
    if load_dotenv is None:
        return
    from pathlib import Path

    repo_root = Path(__file__).resolve().parents[1]
    load_dotenv(dotenv_path=repo_root / ".env")


def _get_concurrency() -> int:
    try:
        return max(1, int(os.getenv("THORDATA_CONCURRENCY", "10")))
    except ValueError:
        return 10


async def _serp_once(
    client: AsyncThordataClient, query: str, request_id: int
) -> Dict[str, Any]:
    try:
        data = await client.serp_search(query=query, num=3, engine="google")
        organic = data.get("organic", [])
        return {"id": request_id, "status": "success", "organic_count": len(organic)}
    except ThordataError as e:
        return {"id": request_id, "status": f"thordata_error: {e}", "organic_count": 0}
    except Exception as e:
        return {"id": request_id, "status": f"unexpected: {e}", "organic_count": 0}


async def demo_concurrent_serp(scraper_token: str) -> None:
    concurrency = _get_concurrency()
    query = os.getenv("THORDATA_DEMO_QUERY", "python")

    print("\n" + "=" * 50)
    print(f"Launching {concurrency} concurrent SERP requests")
    print("=" * 50)

    start_time = time.perf_counter()

    async with AsyncThordataClient(scraper_token=scraper_token) as client:
        tasks = [
            _serp_once(client, query=query, request_id=i + 1)
            for i in range(concurrency)
        ]
        results = await asyncio.gather(*tasks)

    elapsed = time.perf_counter() - start_time

    success_count = sum(1 for r in results if r["status"] == "success")
    counts = [r["organic_count"] for r in results if r["status"] == "success"]

    print("\nResults:")
    for r in results[: min(10, len(results))]:
        # Print only first 10 lines to keep output readable
        print(f"  Request {r['id']:02d}: {r['status']} (organic={r['organic_count']})")

    print("\n" + "-" * 50)
    print("Summary:")
    print(f"  Total requests:  {concurrency}")
    print(f"  Successful:      {success_count}")
    print(f"  Failed:          {concurrency - success_count}")
    print(
        f"  Avg organic:     {(sum(counts) / len(counts)):.2f}"
        if counts
        else "  Avg organic:     N/A"
    )
    print(f"  Total time:      {elapsed:.2f}s")
    print(
        f"  Requests/second: {(concurrency / elapsed):.2f}"
        if elapsed > 0
        else "  Requests/second: N/A"
    )


def main() -> int:
    _configure_stdio()
    _load_env()

    scraper_token = os.getenv("THORDATA_SCRAPER_TOKEN")
    if not scraper_token:
        print("Error: THORDATA_SCRAPER_TOKEN is missing.")
        return 1

    print("=" * 50)
    print("Thordata SDK - Async Concurrency Demo")
    print("=" * 50)

    asyncio.run(demo_concurrent_serp(scraper_token))

    print("\n" + "=" * 50)
    print("Demo complete.")
    print("=" * 50)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())


========================================================================
FILE: examples\demo_scraping_browser.py
SIZE: 7727
TRUNCATED: no
========================================================================
"""
Scraping Browser Demo - Remote Browser Automation

Demonstrates:
- Connecting to Thordata's Scraping Browser via WebSocket
- Using Puppeteer/Playwright for complex page interactions
- Handling dynamic content and JavaScript-heavy pages

Requirements:
    pip install playwright
    playwright install chromium

Note: This example uses Playwright. You can also use Puppeteer with Node.js.

Usage:
    python examples/demo_scraping_browser.py
"""

import asyncio
import logging
import os
import sys
from pathlib import Path

from dotenv import load_dotenv

logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")

load_dotenv(dotenv_path=Path(__file__).resolve().parents[1] / ".env")

# Scraping Browser WebSocket endpoint
# Format: wss://td-customer-{USERNAME}:{PASSWORD}@ws-browser.thordata.com
USERNAME = os.getenv("THORDATA_BROWSER_USERNAME")
PASSWORD = os.getenv("THORDATA_BROWSER_PASSWORD")

if not USERNAME or not PASSWORD:
    print("âŒ Error: Scraping Browser credentials required")
    print("   Set THORDATA_BROWSER_USERNAME and THORDATA_BROWSER_PASSWORD in .env")
    print("")
    print("   Note: Scraping Browser uses separate credentials from the Proxy API.")
    print("   Get them from: Thordata Dashboard > Scraping > Scraping Browser")
    sys.exit(1)

WS_ENDPOINT = f"wss://td-customer-{USERNAME}:{PASSWORD}@ws-browser.thordata.com"


async def demo_basic_navigation():
    """Basic page navigation with Scraping Browser."""
    print("\n" + "=" * 50)
    print("1ï¸âƒ£  Basic Navigation")
    print("=" * 50)

    try:
        from playwright.async_api import async_playwright
    except ImportError:
        print("âš ï¸  Playwright not installed. Run:")
        print("   pip install playwright")
        print("   playwright install chromium")
        return

    try:
        async with async_playwright() as p:
            print("   Connecting to Scraping Browser...")

            # Connect to remote browser
            browser = await p.chromium.connect_over_cdp(WS_ENDPOINT)

            print("   âœ… Connected!")

            # Create a new page
            context = await browser.new_context(viewport={"width": 1366, "height": 768})
            page = await context.new_page()

            # Navigate to a page
            url = "https://example.com"
            print(f"   Navigating to {url}...")

            await page.goto(url, wait_until="domcontentloaded")

            # Get page title
            title = await page.title()
            print(f"   Page title: {title}")

            # Get page content
            content = await page.content()
            print(f"   Content length: {len(content)} characters")

            # Take screenshot
            screenshot = await page.screenshot()
            with open("browser_screenshot.png", "wb") as f:
                f.write(screenshot)
            print("   Screenshot saved: browser_screenshot.png")

            await browser.close()
            print("   âœ… Browser closed")

    except Exception as e:
        print(f"âŒ Error: {e}")


async def demo_search_interaction():
    """Interactive search with form filling."""
    print("\n" + "=" * 50)
    print("2ï¸âƒ£  Search Interaction")
    print("=" * 50)

    try:
        from playwright.async_api import async_playwright
    except ImportError:
        print("âš ï¸  Playwright not installed")
        return

    try:
        async with async_playwright() as p:
            browser = await p.chromium.connect_over_cdp(WS_ENDPOINT)
            context = await browser.new_context()
            page = await context.new_page()

            # Go to Bing
            print("   Navigating to Bing...")
            await page.goto("https://www.bing.com", wait_until="networkidle")

            # Search for something
            search_query = "Python programming"
            print(f"   Searching for: {search_query}")

            # Type in search box
            await page.fill('input[name="q"]', search_query)
            await page.keyboard.press("Enter")

            # Wait for results
            await page.wait_for_load_state("networkidle")

            # Get search results
            results = await page.query_selector_all("li.b_algo h2 a")
            print(f"   Found {len(results)} search results")

            for i, result in enumerate(results[:5], 1):
                title = await result.inner_text()
                await result.get_attribute("href")
                print(f"   {i}. {title[:50]}...")

            await browser.close()
            print("   âœ… Complete")

    except Exception as e:
        print(f"âŒ Error: {e}")


async def demo_dynamic_content():
    """Handling JavaScript-rendered dynamic content."""
    print("\n" + "=" * 50)
    print("3ï¸âƒ£  Dynamic Content (JS Rendering)")
    print("=" * 50)

    try:
        from playwright.async_api import async_playwright
    except ImportError:
        print("âš ï¸  Playwright not installed")
        return

    try:
        async with async_playwright() as p:
            browser = await p.chromium.connect_over_cdp(WS_ENDPOINT)
            context = await browser.new_context()
            page = await context.new_page()

            # Navigate to a JS-heavy page
            url = "https://news.ycombinator.com"
            print(f"   Navigating to {url}...")

            await page.goto(url, wait_until="networkidle")

            # Wait for specific element
            await page.wait_for_selector(".itemlist", timeout=10000)

            # Extract news items
            items = await page.query_selector_all(".athing .titleline > a")
            print(f"   Found {len(items)} news items")

            for i, item in enumerate(items[:5], 1):
                title = await item.inner_text()
                print(f"   {i}. {title[:60]}...")

            await browser.close()
            print("   âœ… Complete")

    except Exception as e:
        print(f"âŒ Error: {e}")


def show_puppeteer_example():
    """Show equivalent Puppeteer (Node.js) code."""
    print("\n" + "=" * 50)
    print("ðŸ“ Puppeteer (Node.js) Example")
    print("=" * 50)

    code = """
const puppeteer = require('puppeteer-core');

const WS_ENDPOINT = 'wss://td-customer-USERNAME:PASSWORD@ws-browser.thordata.com';

async function main() {
    const browser = await puppeteer.connect({
        browserWSEndpoint: WS_ENDPOINT,
        defaultViewport: { width: 1366, height: 768 }
    });

    const page = await browser.newPage();
    await page.goto('https://example.com');

    const title = await page.title();
    console.log('Page title:', title);

    await browser.close();
}

main();
"""
    print(code)


if __name__ == "__main__":
    print("=" * 50)
    print("   Thordata SDK - Scraping Browser Demo")
    print("=" * 50)

    # Check if playwright is available
    try:
        import playwright

        HAS_PLAYWRIGHT = True
    except ImportError:
        HAS_PLAYWRIGHT = False
        print("\nâš ï¸  Playwright not installed.")
        print("   To run the interactive demos, install it:")
        print("   pip install playwright")
        print("   playwright install chromium")

    if HAS_PLAYWRIGHT:
        asyncio.run(demo_basic_navigation())
        asyncio.run(demo_search_interaction())
        asyncio.run(demo_dynamic_content())

    # Always show the Node.js example
    show_puppeteer_example()

    print("\n" + "=" * 50)
    print("   Demo Complete!")
    print("=" * 50)


========================================================================
FILE: examples\demo_serp_api.py
SIZE: 7571
TRUNCATED: no
========================================================================
"""
SERP API Demo - Search Engine Results

Demonstrates:
- Google, Bing, Yandex search using Engine enum
- Search types (Shopping, News, Images)
- Advanced search with SerpRequest
- Async SERP search

Usage:
    python examples/demo_serp_api.py
"""

import asyncio
import logging
import os
import sys
from datetime import date

if hasattr(sys.stdout, "reconfigure"):
    sys.stdout.reconfigure(encoding="utf-8", errors="replace")
if hasattr(sys.stderr, "reconfigure"):
    sys.stderr.reconfigure(encoding="utf-8", errors="replace")

try:
    from dotenv import load_dotenv
except ImportError:
    load_dotenv = None

from thordata import (
    AsyncThordataClient,
    Engine,
    SerpRequest,
    ThordataAuthError,
    ThordataClient,
    ThordataRateLimitError,
)

logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")


def _load_env() -> None:
    if load_dotenv is None:
        return
    from pathlib import Path

    repo_root = Path(__file__).resolve().parents[1]
    load_dotenv(dotenv_path=repo_root / ".env")


def _print_results(results: dict, result_key: str, alt_keys: list = None) -> int:
    """Helper to find and print results from various possible keys."""
    alt_keys = alt_keys or []

    # Try primary key first, then alternatives
    data = results.get(result_key, [])
    if not data:
        for key in alt_keys:
            data = results.get(key, [])
            if data:
                break

    # If still no data, show available keys for debugging
    if not data:
        available_keys = [k for k in results.keys() if not k.startswith("search_")]
        print(f"   Available keys: {available_keys[:10]}")  # Show first 10

    return len(data) if isinstance(data, list) else 0


def demo_basic_search(scraper_token: str) -> None:
    """Basic Google search."""
    print("\n" + "=" * 50)
    print("1)  Basic Google Search")
    print("=" * 50)

    client = ThordataClient(scraper_token=scraper_token)

    try:
        results = client.serp_search(
            query="Python programming",
            engine=Engine.GOOGLE,
            num=10,  # Request more results
        )

        organic = results.get("organic", results.get("organic_results", []))
        print(f"[OK] Found {len(organic)} organic results:\n")

        for i, item in enumerate(organic[:5], 1):  # Show up to 5
            title = item.get("title", "No title")
            link = item.get("link", "")
            print(f"   {i}. {title}")
            print(f"      {link}\n")

    except ThordataRateLimitError as e:
        print(f"[ERROR] Rate limited: {e}")
    except ThordataAuthError as e:
        print(f"[ERROR] Auth error: {e}")
    except Exception as e:
        print(f"[ERROR] Search failed: {e}")


def demo_bing_search(scraper_token: str) -> None:
    """Bing search example."""
    print("\n" + "=" * 50)
    print("2)  Bing Search")
    print("=" * 50)

    client = ThordataClient(scraper_token=scraper_token)

    try:
        results = client.serp_search(
            query="machine learning",
            engine=Engine.BING,
            num=10,
        )

        organic = results.get("organic", results.get("organic_results", []))
        print(f"[OK] Bing returned {len(organic)} results")

    except Exception as e:
        print(f"[ERROR] Bing search failed: {e}")


def demo_google_shopping(scraper_token: str) -> None:
    """Google Shopping search."""
    print("\n" + "=" * 50)
    print("3)  Google Shopping Search")
    print("=" * 50)

    client = ThordataClient(scraper_token=scraper_token)

    try:
        results = client.serp_search(
            query="laptop",
            engine="google_shopping",
            country="us",
            num=10,
        )

        # Try multiple possible keys for shopping results
        shopping = (
            results.get("shopping")
            or results.get("shopping_results")
            or results.get("inline_shopping")
            or results.get("immersive_products")
            or results.get("products")
            or []
        )

        print(f"[OK] Found {len(shopping)} shopping results")

        if shopping:
            for item in shopping[:3]:
                title = item.get("title", "Unknown")
                price = item.get("price", item.get("extracted_price", "N/A"))
                print(f"   - {title} - {price}")
        else:
            # Debug: show what keys we got
            print(f"   (Debug) Response keys: {list(results.keys())[:10]}")

    except Exception as e:
        print(f"[ERROR] Shopping search failed: {e}")


def demo_google_news(scraper_token: str) -> None:
    """Google News search."""
    print("\n" + "=" * 50)
    print("4)  Google News Search")
    print("=" * 50)

    client = ThordataClient(scraper_token=scraper_token)

    try:
        results = client.serp_search(
            query="AI regulation 2024",
            engine="google_news",
            country="us",
            language="en",
            num=10,
        )

        # Try multiple possible keys for news results
        news = (
            results.get("news_results")
            or results.get("top_stories")
            or results.get("news")
            or results.get("organic")
            or []
        )

        print(f"[OK] Found {len(news)} news results")

        if news:
            for item in news[:3]:
                title = item.get("title", "Unknown")
                source = item.get("source", item.get("publisher", ""))
                print(f"   - {title}")
                if source:
                    print(f"     Source: {source}")
        else:
            print(f"   (Debug) Response keys: {list(results.keys())[:10]}")

    except Exception as e:
        print(f"[ERROR] News search failed: {e}")


async def demo_async_search(scraper_token: str) -> None:
    """Async SERP search with extended timeout."""
    print("\n" + "=" * 50)
    print("5)  Async Search (Yandex)")
    print("=" * 50)

    # Use longer timeout for Yandex
    async with AsyncThordataClient(
        scraper_token=scraper_token,
        timeout=60,  # Extended timeout
    ) as client:
        try:
            results = await client.serp_search(
                query="Python tutorial",
                engine=Engine.YANDEX,
                num=5,
            )

            organic = results.get("organic", results.get("organic_results", []))
            print(f"[OK] Yandex returned {len(organic)} results")

        except Exception as e:
            print(f"[WARN] Yandex search issue: {type(e).__name__}: {e}")
            print("       (Yandex may have slower response times)")


def main() -> int:
    _load_env()

    scraper_token = os.getenv("THORDATA_SCRAPER_TOKEN")
    if not scraper_token:
        print("Error: THORDATA_SCRAPER_TOKEN is missing.")
        return 1

    print("=" * 50)
    print("Thordata SDK - SERP API Demo")
    print(f"Date: {date.today().isoformat()}")
    print("=" * 50)

    demo_basic_search(scraper_token)
    demo_bing_search(scraper_token)
    demo_google_shopping(scraper_token)
    demo_google_news(scraper_token)
    asyncio.run(demo_async_search(scraper_token))

    print("\n" + "=" * 50)
    print("Demo complete.")
    print("=" * 50)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())


========================================================================
FILE: examples\demo_serp_google_news.py
SIZE: 4533
TRUNCATED: no
========================================================================
"""
Demo: Google News via Thordata SERP API

Demonstrates:
- Basic Google News search using dedicated engine
- Using topic_token / publication_token / section_token / story_token
- Sorting by relevance/date (so parameter)

Usage:
    1. Set THORDATA_SCRAPER_TOKEN in .env
    2. (Optional) Set country/language for localization
    3. Run:
        python examples/demo_serp_google_news.py
"""

import logging
import os
import sys
from pathlib import Path

from dotenv import load_dotenv

from thordata import Engine, ThordataClient

logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")

# Load credentials
load_dotenv(dotenv_path=Path(__file__).resolve().parents[1] / ".env")

SCRAPER_TOKEN = os.getenv("THORDATA_SCRAPER_TOKEN")
COUNTRY = os.getenv("THORDATA_SERP_COUNTRY", "us")
LANGUAGE = os.getenv("THORDATA_SERP_LANGUAGE", "en")

if not SCRAPER_TOKEN:
    print("âŒ Error: THORDATA_SCRAPER_TOKEN is missing in .env")
    sys.exit(1)


def print_news_results(results: dict, max_items: int = 5) -> None:
    """Pretty-print Google News results."""
    news = (
        results.get("news_results")
        or results.get("news")
        or results.get("organic")
        or []
    )
    print(f"Found {len(news)} news results.\n")

    for i, item in enumerate(news[:max_items], 1):
        title = item.get("title", "No title")
        link = item.get("link", "")
        source = item.get("source") or item.get("publisher", "")
        snippet = item.get("snippet", "")[:120]

        print(f"{i}. {title}")
        if source:
            print(f"   Source: {source}")
        if snippet:
            print(f"   Snippet: {snippet}...")
        if link:
            print(f"   Link   : {link}")
        print()


def demo_basic_news_search() -> None:
    """Basic Google News search using dedicated engine."""
    print("\n" + "=" * 60)
    print("1) Basic Google News search")
    print("=" * 60)

    client = ThordataClient(scraper_token=SCRAPER_TOKEN)

    query = "AI regulation"

    print(f"Query   : {query}")
    print("Engine  : google_news (dedicated)")
    print(f"Country : {COUNTRY}")
    print(f"Language: {LANGUAGE}")

    # Use dedicated google_news engine instead of google + search_type
    results = client.serp_search(
        query=query,
        engine=Engine.GOOGLE_NEWS,
        country=COUNTRY,
        language=LANGUAGE,
        num=10,
    )

    print_news_results(results)


def demo_advanced_news_filters() -> None:
    """
    Advanced Google News filters:
    - topic_token / publication_token / section_token / story_token
    - so (sort by relevance/date)
    """
    print("\n" + "=" * 60)
    print("2) Advanced Google News filters")
    print("=" * 60)

    client = ThordataClient(scraper_token=SCRAPER_TOKEN)

    query = "Artificial Intelligence"

    # These are placeholder tokens - replace with actual values from Dashboard
    topic_token = os.getenv("THORDATA_NEWS_TOPIC_TOKEN", "")
    publication_token = os.getenv("THORDATA_NEWS_PUBLICATION_TOKEN", "")
    section_token = os.getenv("THORDATA_NEWS_SECTION_TOKEN", "")
    story_token = os.getenv("THORDATA_NEWS_STORY_TOKEN", "")

    print(f"Query : {query}")
    print("Engine: google_news (dedicated)")
    print(f"Topic token       : {topic_token or '(not set)'}")
    print(f"Publication token : {publication_token or '(not set)'}")
    print(f"Section token     : {section_token or '(not set)'}")
    print(f"Story token       : {story_token or '(not set)'}")
    print("Sort by           : Date (so=1)")
    print()

    # Build kwargs for optional tokens
    kwargs: dict = {"so": 1}  # 0=Relevance, 1=Date
    if topic_token:
        kwargs["topic_token"] = topic_token
    if publication_token:
        kwargs["publication_token"] = publication_token
    if section_token:
        kwargs["section_token"] = section_token
    if story_token:
        kwargs["story_token"] = story_token

    # Use dedicated google_news engine
    results = client.serp_search(
        query=query,
        engine=Engine.GOOGLE_NEWS,
        country=COUNTRY,
        language=LANGUAGE,
        num=10,
        **kwargs,
    )

    print_news_results(results)


if __name__ == "__main__":
    print("=" * 60)
    print("Thordata SERP API - Google News Demo")
    print("=" * 60)

    demo_basic_news_search()
    demo_advanced_news_filters()

    print("\nDone.")


========================================================================
FILE: examples\demo_universal.py
SIZE: 3616
TRUNCATED: no
========================================================================
"""
Universal API Demo - Web Unlocker / Universal Scrape

Demonstrates:
- Fetching HTML via Universal API
- Fetching screenshot (PNG) via Universal API
- Optional async usage

Environment variables:
- THORDATA_SCRAPER_TOKEN (required)
- THORDATA_DEMO_OUTPUT_DIR (optional) if set, writes universal.html and screenshot.png

Usage:
    python examples/demo_universal.py
"""

from __future__ import annotations

import asyncio
import os
import sys

try:
    from dotenv import load_dotenv
except ImportError:
    load_dotenv = None

from thordata import AsyncThordataClient, ThordataClient


def _configure_stdio() -> None:
    # Avoid UnicodeEncodeError on Windows consoles with legacy encodings.
    if hasattr(sys.stdout, "reconfigure"):
        sys.stdout.reconfigure(encoding="utf-8", errors="replace")
    if hasattr(sys.stderr, "reconfigure"):
        sys.stderr.reconfigure(encoding="utf-8", errors="replace")


def _load_env() -> None:
    if load_dotenv is None:
        return
    from pathlib import Path

    repo_root = Path(__file__).resolve().parents[1]
    load_dotenv(dotenv_path=repo_root / ".env")


def _maybe_write_files(html: str, png: bytes) -> None:
    out_dir = os.getenv("THORDATA_DEMO_OUTPUT_DIR")
    if not out_dir:
        return

    os.makedirs(out_dir, exist_ok=True)

    html_path = os.path.join(out_dir, "universal.html")
    png_path = os.path.join(out_dir, "screenshot.png")

    with open(html_path, "w", encoding="utf-8") as f:
        f.write(html)

    with open(png_path, "wb") as f:
        f.write(png)

    print(f"Wrote demo outputs to: {out_dir}")


def demo_sync(scraper_token: str) -> None:
    print("=" * 50)
    print("1) Sync Universal Scrape")
    print("=" * 50)

    with ThordataClient(scraper_token=scraper_token) as client:
        html = client.universal_scrape(
            "https://example.com",
            js_render=False,
            output_format="html",
        )
        png = client.universal_scrape(
            "https://example.com",
            js_render=True,
            output_format="png",
        )

    assert isinstance(html, str)
    assert isinstance(png, (bytes, bytearray))

    print(f"HTML length: {len(html)}")
    print(f"PNG bytes: {len(png)}")

    _maybe_write_files(html, bytes(png))


async def demo_async(scraper_token: str) -> None:
    print("=" * 50)
    print("2) Async Universal Scrape")
    print("=" * 50)

    async with AsyncThordataClient(scraper_token=scraper_token) as client:
        html = await client.universal_scrape(
            "https://example.com",
            js_render=False,
            output_format="html",
        )
        png = await client.universal_scrape(
            "https://example.com",
            js_render=True,
            output_format="png",
        )

    assert isinstance(html, str)
    assert isinstance(png, (bytes, bytearray))

    print(f"Async HTML length: {len(html)}")
    print(f"Async PNG bytes: {len(png)}")


def main() -> int:
    _configure_stdio()
    _load_env()

    scraper_token = os.getenv("THORDATA_SCRAPER_TOKEN")
    if not scraper_token:
        print("Error: THORDATA_SCRAPER_TOKEN is missing.")
        return 1

    print("=" * 50)
    print("Thordata SDK - Universal API Demo")
    print("=" * 50)

    demo_sync(scraper_token)
    asyncio.run(demo_async(scraper_token))

    print("=" * 50)
    print("Demo complete.")
    print("=" * 50)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())


========================================================================
FILE: examples\demo_web_scraper_api.py
SIZE: 5612
TRUNCATED: no
========================================================================
"""
Web Scraper API Demo - Task-Based Scraping (Offline-test friendly)

Demonstrates:
- Creating async scraping tasks
- Checking task status
- Waiting for completion
- Retrieving result download URL
- Using ScraperTaskConfig

Environment variables:
- THORDATA_SCRAPER_TOKEN (required)
- THORDATA_PUBLIC_TOKEN (required)
- THORDATA_PUBLIC_KEY (required)

Usage:
    python examples/demo_web_scraper_api.py
"""

from __future__ import annotations

import logging
import os
import sys
from typing import Optional

try:
    from dotenv import load_dotenv
except ImportError:
    load_dotenv = None

from thordata import ScraperTaskConfig, ThordataClient, ThordataError

logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")


def _configure_stdio() -> None:
    # Avoid UnicodeEncodeError on Windows consoles with legacy encodings.
    if hasattr(sys.stdout, "reconfigure"):
        sys.stdout.reconfigure(encoding="utf-8", errors="replace")
    if hasattr(sys.stderr, "reconfigure"):
        sys.stderr.reconfigure(encoding="utf-8", errors="replace")


def _load_env() -> None:
    if load_dotenv is None:
        return
    from pathlib import Path

    repo_root = Path(__file__).resolve().parents[1]
    load_dotenv(dotenv_path=repo_root / ".env")


def demo_create_task(client: ThordataClient) -> Optional[str]:
    print("\n" + "=" * 50)
    print("1) Create Scraper Task")
    print("=" * 50)

    print("Creating task for YouTube scraping...")
    print("(Example only - adjust spider_id/spider_name for your use case.)")

    try:
        task_id = client.create_scraper_task(
            file_name="demo_youtube_data",
            spider_id="youtube_video-post_by-url",
            spider_name="youtube.com",
            parameters={
                "url": "https://www.youtube.com/@YouTube/videos",
                "num_of_posts": "5",
            },
        )

        print("Task created.")
        print(f"Task ID: {task_id}")
        return task_id

    except ThordataError as e:
        print(f"Task creation failed: {e}")
        return None


def demo_poll_status(client: ThordataClient, task_id: str) -> Optional[str]:
    print("\n" + "=" * 50)
    print("2) Check Task Status")
    print("=" * 50)

    print(f"Checking status for: {task_id}")

    try:
        status = client.get_task_status(task_id)
        print(f"Current status: {status}")
        return status
    except ThordataError as e:
        print(f"Status check failed: {e}")
        return None


def demo_wait_for_completion(client: ThordataClient, task_id: str) -> bool:
    print("\n" + "=" * 50)
    print("3) Wait for Completion")
    print("=" * 50)

    print(f"Waiting for task {task_id}...")

    try:
        status = client.wait_for_task(
            task_id,
            poll_interval=5.0,
            max_wait=60.0,
        )

        if status.lower() in ("ready", "success", "finished"):
            print(f"Task completed: {status}")
            return True

        print(f"Task ended with status: {status}")
        return False

    except TimeoutError:
        print("Task did not complete within the time limit.")
        return False
    except ThordataError as e:
        print(f"Error while waiting: {e}")
        return False


def demo_get_result(client: ThordataClient, task_id: str) -> None:
    print("\n" + "=" * 50)
    print("4) Get Result Download URL")
    print("=" * 50)

    try:
        download_url = client.get_task_result(task_id, file_type="json")
        print("Result ready.")
        print(f"Download URL: {download_url}")
    except ThordataError as e:
        print(f"Get result failed: {e}")


def demo_using_config() -> None:
    print("\n" + "=" * 50)
    print("5) Using ScraperTaskConfig")
    print("=" * 50)

    config = ScraperTaskConfig(
        file_name="demo_config_task",
        spider_id="example-spider-id",
        spider_name="example.com",
        parameters={"url": "https://example.com", "depth": "1"},
        include_errors=True,
    )

    print("ScraperTaskConfig created:")
    print(f"- file_name: {config.file_name}")
    print(f"- spider_id: {config.spider_id}")
    print(f"- spider_name: {config.spider_name}")
    print("To create a task with this config, call:")
    print("client.create_scraper_task_advanced(config)")


def main() -> int:
    _configure_stdio()
    _load_env()

    scraper_token = os.getenv("THORDATA_SCRAPER_TOKEN")
    public_token = os.getenv("THORDATA_PUBLIC_TOKEN")
    public_key = os.getenv("THORDATA_PUBLIC_KEY")

    if not scraper_token:
        print("Error: THORDATA_SCRAPER_TOKEN is missing.")
        return 1

    if not public_token or not public_key:
        print("Error: THORDATA_PUBLIC_TOKEN and THORDATA_PUBLIC_KEY are required.")
        return 1

    print("=" * 50)
    print("Thordata SDK - Web Scraper API Demo")
    print("=" * 50)

    client = ThordataClient(
        scraper_token=scraper_token,
        public_token=public_token,
        public_key=public_key,
    )

    task_id = demo_create_task(client)

    if task_id:
        demo_poll_status(client, task_id)
        completed = demo_wait_for_completion(client, task_id)
        if completed:
            demo_get_result(client, task_id)

    demo_using_config()

    print("\n" + "=" * 50)
    print("Demo complete.")
    print("=" * 50)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())


========================================================================
FILE: examples\proxy_datacenter.py
SIZE: 1152
TRUNCATED: no
========================================================================
"""
Datacenter Proxy Demo

Usage:
    python examples/proxy_datacenter.py
"""

import os
from pathlib import Path

import requests
from dotenv import load_dotenv

from thordata import ProxyConfig, ProxyProduct

load_dotenv(dotenv_path=Path(__file__).resolve().parents[1] / ".env")

USERNAME = os.getenv("THORDATA_DATACENTER_USERNAME")
PASSWORD = os.getenv("THORDATA_DATACENTER_PASSWORD")


def main():
    if not USERNAME or not PASSWORD:
        print("Datacenter Proxy Demo - Skipped")
        print(
            "Set THORDATA_DATACENTER_USERNAME and THORDATA_DATACENTER_PASSWORD in .env"
        )
        return

    print("Datacenter Proxy Demo\n")

    proxy = ProxyConfig(
        username=USERNAME,
        password=PASSWORD,
        product=ProxyProduct.DATACENTER,
    )

    try:
        response = requests.get(
            "http://httpbin.org/ip",
            proxies=proxy.to_proxies_dict(),
            timeout=30,
        )
        print(f"Datacenter IP: {response.json().get('origin')}")
    except Exception as e:
        print(f"Failed: {e}")


if __name__ == "__main__":
    main()


========================================================================
FILE: examples\proxy_isp.py
SIZE: 1436
TRUNCATED: no
========================================================================
"""
Static ISP Proxy Demo

Static ISP proxies use direct IP connection (not gateway).

Usage:
    python examples/proxy_isp.py
"""

import os
from pathlib import Path

import requests
from dotenv import load_dotenv

from thordata import StaticISPProxy

load_dotenv(dotenv_path=Path(__file__).resolve().parents[1] / ".env")

HOST = os.getenv("THORDATA_ISP_HOST")
USERNAME = os.getenv("THORDATA_ISP_USERNAME")
PASSWORD = os.getenv("THORDATA_ISP_PASSWORD")


def main():
    if not HOST or not USERNAME or not PASSWORD:
        print("Static ISP Proxy Demo - Skipped")
        print(
            "Set THORDATA_ISP_HOST, THORDATA_ISP_USERNAME, and THORDATA_ISP_PASSWORD in .env"
        )
        return

    print("Static ISP Proxy Demo\n")

    proxy = StaticISPProxy(
        host=HOST,
        username=USERNAME,
        password=PASSWORD,
    )

    print(f"Expected IP: {HOST}")

    try:
        response = requests.get(
            "http://httpbin.org/ip",
            proxies=proxy.to_proxies_dict(),
            timeout=30,
        )
        actual_ip = response.json().get("origin")
        print(f"Actual IP  : {actual_ip}")

        if actual_ip == HOST:
            print("Success! IP matches.")
        else:
            print("Warning: IP does not match expected.")
    except Exception as e:
        print(f"Failed: {e}")


if __name__ == "__main__":
    main()


========================================================================
FILE: examples\proxy_mobile.py
SIZE: 1114
TRUNCATED: no
========================================================================
"""
Mobile Proxy Demo

Usage:
    python examples/proxy_mobile.py
"""

import os
from pathlib import Path

import requests
from dotenv import load_dotenv

from thordata import ProxyConfig, ProxyProduct

load_dotenv(dotenv_path=Path(__file__).resolve().parents[1] / ".env")

USERNAME = os.getenv("THORDATA_MOBILE_USERNAME")
PASSWORD = os.getenv("THORDATA_MOBILE_PASSWORD")


def main():
    if not USERNAME or not PASSWORD:
        print("Mobile Proxy Demo - Skipped")
        print("Set THORDATA_MOBILE_USERNAME and THORDATA_MOBILE_PASSWORD in .env")
        return

    print("Mobile Proxy Demo\n")

    proxy = ProxyConfig(
        username=USERNAME,
        password=PASSWORD,
        product=ProxyProduct.MOBILE,
        country="gb",
    )

    try:
        response = requests.get(
            "http://httpbin.org/ip",
            proxies=proxy.to_proxies_dict(),
            timeout=30,
        )
        print(f"UK Mobile IP: {response.json().get('origin')}")
    except Exception as e:
        print(f"Failed: {e}")


if __name__ == "__main__":
    main()


========================================================================
FILE: examples\proxy_residential.py
SIZE: 2019
TRUNCATED: no
========================================================================
"""
Residential Proxy Demo

Demonstrates residential proxy with geo-targeting and sticky sessions.

Usage:
    python examples/proxy_residential.py
"""

import os
import sys
from pathlib import Path

import requests
from dotenv import load_dotenv

from thordata import ProxyConfig, ProxyProduct, StickySession

load_dotenv(dotenv_path=Path(__file__).resolve().parents[1] / ".env")

USERNAME = os.getenv("THORDATA_RESIDENTIAL_USERNAME")
PASSWORD = os.getenv("THORDATA_RESIDENTIAL_PASSWORD")


def main():
    if not USERNAME or not PASSWORD:
        print("Residential Proxy Demo - Skipped")
        print(
            "Set THORDATA_RESIDENTIAL_USERNAME and THORDATA_RESIDENTIAL_PASSWORD in .env"
        )
        return

    print("Residential Proxy Demo\n")

    # Basic geo-targeted request
    proxy = ProxyConfig(
        username=USERNAME,
        password=PASSWORD,
        product=ProxyProduct.RESIDENTIAL,
        country="us",
    )

    print("1. US Residential Request")
    try:
        response = requests.get(
            "http://httpbin.org/ip",
            proxies=proxy.to_proxies_dict(),
            timeout=30,
        )
        print(f"   IP: {response.json().get('origin')}")
    except Exception as e:
        print(f"   Failed: {e}")

    # Sticky session
    print("\n2. Sticky Session (Tokyo, 10 min)")
    session = StickySession(
        username=USERNAME,
        password=PASSWORD,
        product=ProxyProduct.RESIDENTIAL,
        country="jp",
        city="tokyo",
        duration_minutes=10,
    )

    for i in range(2):
        try:
            response = requests.get(
                "http://httpbin.org/ip",
                proxies=session.to_proxies_dict(),
                timeout=30,
            )
            print(f"   Request {i + 1}: {response.json().get('origin')}")
        except Exception as e:
            print(f"   Request {i + 1}: Failed - {e}")


if __name__ == "__main__":
    main()


========================================================================
FILE: examples\verify_new_features.py
SIZE: 6925
TRUNCATED: no
========================================================================
"""
Verification script for newly added features.

This script allows you to selectively test new API endpoints
with your real credentials.

Usage:
    # Test specific feature
    python examples/verify_new_features.py --test usage_stats

    # Test all features
    python examples/verify_new_features.py --test all

Available tests:
    - video_task: Video/audio download task creation
    - usage_stats: Account usage statistics
    - proxy_users: Proxy user management
    - whitelist: IP whitelist management
    - proxy_servers: ISP/DC proxy list
"""

import argparse
import os
import sys
from datetime import date, timedelta
from pathlib import Path

try:
    from dotenv import load_dotenv

    load_dotenv(dotenv_path=Path(__file__).resolve().parents[1] / ".env")
except ImportError:
    pass

from thordata import (
    CommonSettings,
    ProxyType,
    ThordataClient,
)


def test_video_task(client: ThordataClient) -> bool:
    """Test video_builder endpoint."""
    print("\n" + "=" * 60)
    print("Testing: Video Task Creation")
    print("=" * 60)

    try:
        # Note: This will actually create a task!
        # Adjust URL to a test video or comment out if you don't want to create real tasks
        task_id = client.create_video_task(
            file_name="test_{{VideoID}}",
            spider_id="youtube_video_by-url",
            spider_name="youtube.com",
            parameters={
                "url": "https://www.youtube.com/watch?v=dQw4w9WgXcQ",  # Example
            },
            common_settings=CommonSettings(
                resolution="720p",
                is_subtitles="false",
            ),
        )
        print(f"âœ… Video task created: {task_id}")
        return True
    except Exception as e:
        print(f"âŒ Failed: {e}")
        return False


def test_usage_stats(client: ThordataClient) -> bool:
    """Test usage statistics API."""
    print("\n" + "=" * 60)
    print("Testing: Usage Statistics")
    print("=" * 60)

    try:
        today = date.today()
        week_ago = today - timedelta(days=7)

        stats = client.get_usage_statistics(week_ago, today)

        print("âœ… Usage Statistics Retrieved:")
        print(f"   Total Usage: {stats.total_usage_gb():.2f} GB")
        print(f"   Balance: {stats.balance_gb():.2f} GB")
        print(f"   Range Usage: {stats.range_usage_gb():.2f} GB")
        print(f"   Query Days: {stats.query_days}")
        return True
    except Exception as e:
        print(f"âŒ Failed: {e}")
        return False


def test_proxy_users(client: ThordataClient) -> bool:
    """Test proxy user management."""
    print("\n" + "=" * 60)
    print("Testing: Proxy Users Management")
    print("=" * 60)

    try:
        users = client.list_proxy_users(proxy_type=ProxyType.RESIDENTIAL)

        print("âœ… Proxy Users Retrieved:")
        print(f"   Total Users: {users.user_count}")
        print(f"   Limit: {users.limit / (1024**2):.2f} MB")
        print(f"   Remaining: {users.remaining_limit / (1024**2):.2f} MB")

        for i, user in enumerate(users.users[:3], 1):
            print(f"   User {i}: {user.username}")
            print(f"      Status: {'Enabled' if user.status else 'Disabled'}")
            print(f"      Usage: {user.usage_gb():.2f} GB")

        return True
    except Exception as e:
        print(f"âŒ Failed: {e}")
        return False


def test_whitelist(client: ThordataClient) -> bool:
    """Test whitelist IP management."""
    print("\n" + "=" * 60)
    print("Testing: Whitelist IP Management")
    print("=" * 60)

    # This is read-only test - we don't actually add IPs
    print("âš ï¸  Skipping actual IP addition (would modify account)")
    print("    To test, uncomment and provide a test IP:")
    print("    # result = client.add_whitelist_ip('1.2.3.4', status=False)")
    return True


def test_proxy_servers(client: ThordataClient) -> bool:
    """Test proxy server list."""
    print("\n" + "=" * 60)
    print("Testing: Proxy Servers List")
    print("=" * 60)

    try:
        # Try ISP proxies (type=1)
        servers = client.list_proxy_servers(proxy_type=1)

        print("âœ… ISP Proxy Servers Retrieved:")
        print(f"   Total Servers: {len(servers)}")

        for i, server in enumerate(servers[:3], 1):
            print(f"   Server {i}: {server.ip}:{server.port}")
            print(f"      Username: {server.username}")
            print(f"      Expiration: {server.expiration_time}")

        return True
    except Exception as e:
        print(f"âŒ Failed (may not have ISP proxies): {e}")
        return False


def main():
    parser = argparse.ArgumentParser(description="Verify new Thordata SDK features")
    parser.add_argument(
        "--test",
        choices=[
            "video_task",
            "usage_stats",
            "proxy_users",
            "whitelist",
            "proxy_servers",
            "all",
        ],
        default="all",
        help="Which test to run",
    )

    args = parser.parse_args()

    # Check credentials
    scraper_token = os.getenv("THORDATA_SCRAPER_TOKEN")
    public_token = os.getenv("THORDATA_PUBLIC_TOKEN")
    public_key = os.getenv("THORDATA_PUBLIC_KEY")

    if not scraper_token:
        print("âŒ Error: THORDATA_SCRAPER_TOKEN is required")
        return 1

    print("=" * 60)
    print("Thordata SDK - New Features Verification")
    print("=" * 60)
    print("Credentials loaded:")
    print(f"  SCRAPER_TOKEN: {'âœ“' if scraper_token else 'âœ—'}")
    print(f"  PUBLIC_TOKEN:  {'âœ“' if public_token else 'âœ—'}")
    print(f"  PUBLIC_KEY:    {'âœ“' if public_key else 'âœ—'}")

    # Create client
    client = ThordataClient(
        scraper_token=scraper_token,
        public_token=public_token,
        public_key=public_key,
    )

    # Test map
    tests = {
        "video_task": test_video_task,
        "usage_stats": test_usage_stats,
        "proxy_users": test_proxy_users,
        "whitelist": test_whitelist,
        "proxy_servers": test_proxy_servers,
    }

    # Run tests
    results = {}

    if args.test == "all":
        for name, func in tests.items():
            results[name] = func(client)
    else:
        results[args.test] = tests[args.test](client)

    # Summary
    print("\n" + "=" * 60)
    print("Test Summary")
    print("=" * 60)

    for name, passed in results.items():
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"{status} - {name}")

    passed_count = sum(results.values())
    total_count = len(results)
    print(f"\nTotal: {passed_count}/{total_count} passed")

    return 0 if passed_count == total_count else 1


if __name__ == "__main__":
    sys.exit(main())


========================================================================
FILE: sdk-spec\.gitattributes
SIZE: 78
TRUNCATED: no
========================================================================
*.yaml text eol=lf
*.yml  text eol=lf
*.json text eol=lf
*.md   text eol=lf

========================================================================
FILE: sdk-spec\.gitignore
SIZE: 46
TRUNCATED: no
========================================================================
v1.generated.json
.venv/
__pycache__/
*.pyc

========================================================================
FILE: sdk-spec\README.md
SIZE: 1943
TRUNCATED: no
========================================================================
# thordata-sdk-spec

A versioned specification repository used to keep Thordata SDKs (Python, Node.js, Go, Java) consistent.

This repository defines the canonical defaults, parameter conventions, and shared logic for:
- API base URLs and environment variables
- Endpoint paths and authentication methods
- Proxy gateway conventions (hosts, ports, username format)
- SERP parameter normalization
- Error code handling precedence and mappings
- Webhook payload definitions
- â€œPublic API specificationsâ€ + â€œProxy Network conventions (including HTTPS proxy endpoints)â€

Any change to `v1.json` requires bumping the sdk-spec submodule in all SDK repos.

## Structure

### `v1.json` (Canonical artifact)
The single source of truth consumed by SDK parity tests. Generated from the YAML sources in `spec/v1/`.

### `spec/v1/*.yaml` (Human-readable sources)
- `auth.yaml`: Authentication modes (Bearer, Header Token, Public Token+Key, Proxy Basic Auth)
- `endpoints.yaml`: API paths and methods
- `env.yaml`: Environment variable names and defaults
- `errors.yaml`: Error code mappings (300, 4xx, 5xx)
- `proxy.yaml`: Proxy hosts, ports, and username construction rules
- `serp.yaml`: Search engine parameters and mappings
- `tasks.yaml`: Web Scraper API definitions (Builder, Video Builder, List, Status)
- `public_api.yaml`: Public API endpoints (Usage, Users, Whitelist)
- `public_api_new.yaml`: API NEW endpoints (Sign/ApiKey auth)

### `tools/`
Scripts to build (`build_v1_json.py`) and validate (`validate_v1_json.py`) the JSON spec.

## Versioning Policy

- `v1` is considered stable.
- Backward-compatible changes update `v1.json` (and git tag).
- Breaking changes require a new major version (`v2`).

## Change Workflow

1. Update YAML sources in `spec/v1/`.
2. Run `python tools/build_v1_json.py` to regenerate `v1.json`.
3. Verify with `python tools/validate_v1_json.py`.
4. Commit changes.
5. Update submodules in SDK repositories.

========================================================================
FILE: sdk-spec\v1.json
SIZE: 22275
TRUNCATED: no
========================================================================
{
  "auth": {
    "apiAuth": {
      "builder": {
        "headers": {
          "Authorization": "Bearer {scraperToken}",
          "key": "{publicKey}",
          "token": "{publicToken}"
        },
        "note": "Requires ALL THREE headers",
        "required": [
          "scraperToken",
          "publicToken",
          "publicKey"
        ]
      },
      "locations": {
        "method": "query params",
        "params": {
          "key": "{publicKey}",
          "token": "{publicToken}"
        },
        "required": [
          "publicToken",
          "publicKey"
        ]
      },
      "publicApi": {
        "headers": {
          "key": "{publicKey}",
          "token": "{publicToken}"
        },
        "note": "Account management, proxy users, whitelist",
        "required": [
          "publicToken",
          "publicKey"
        ]
      },
      "serp": {
        "headers": {
          "bearer": {
            "Authorization": "Bearer {scraperToken}"
          },
          "headerToken": {
            "token": "{scraperToken}"
          }
        },
        "mode": "bearer or headerToken",
        "required": [
          "scraperToken"
        ]
      },
      "tasksDownload": {
        "headers": {
          "key": "{publicKey}",
          "token": "{publicToken}"
        },
        "required": [
          "publicToken",
          "publicKey"
        ]
      },
      "tasksList": {
        "headers": {
          "key": "{publicKey}",
          "token": "{publicToken}"
        },
        "required": [
          "publicToken",
          "publicKey"
        ]
      },
      "tasksStatus": {
        "headers": {
          "key": "{publicKey}",
          "token": "{publicToken}"
        },
        "required": [
          "publicToken",
          "publicKey"
        ]
      },
      "universal": {
        "headers": {
          "bearer": {
            "Authorization": "Bearer {scraperToken}"
          },
          "headerToken": {
            "token": "{scraperToken}"
          }
        },
        "mode": "bearer or headerToken",
        "required": [
          "scraperToken"
        ]
      },
      "videoBuilder": {
        "headers": {
          "Authorization": "Bearer {scraperToken}",
          "key": "{publicKey}",
          "token": "{publicToken}"
        },
        "note": "Same as builder",
        "required": [
          "scraperToken",
          "publicToken",
          "publicKey"
        ]
      }
    },
    "credentials": {
      "publicKey": {
        "envVar": "THORDATA_PUBLIC_KEY",
        "required": false,
        "source": "Dashboard -> My Account",
        "usedBy": [
          "publicApi",
          "locations",
          "tasks"
        ]
      },
      "publicToken": {
        "envVar": "THORDATA_PUBLIC_TOKEN",
        "required": false,
        "source": "Dashboard -> My Account",
        "usedBy": [
          "publicApi",
          "locations",
          "tasks"
        ]
      },
      "scraperToken": {
        "envVar": "THORDATA_SCRAPER_TOKEN",
        "required": true,
        "source": "Dashboard -> Account Settings",
        "usedBy": [
          "serp",
          "universal",
          "builder"
        ]
      }
    },
    "decisionTree": {
      "answers": {
        "Account management (usage, users, whitelist, proxy list)": {
          "useCredential": [
            "publicToken",
            "publicKey"
          ]
        },
        "Locations API": {
          "method": "query params",
          "useCredential": [
            "publicToken",
            "publicKey"
          ]
        },
        "SERP or Universal": {
          "authMode": "bearer (default) or headerToken",
          "useCredential": "scraperToken"
        },
        "Task status/download/list": {
          "useCredential": [
            "publicToken",
            "publicKey"
          ]
        },
        "Web Scraper builder/video_builder": {
          "note": "Need all three",
          "useCredential": [
            "scraperToken",
            "publicToken",
            "publicKey"
          ]
        }
      },
      "question": "Which API are you calling?"
    },
    "serpUniversalAuth": {
      "modes": {
        "bearer": {
          "default": true,
          "documentation": "Thordata Docs examples",
          "format": "Bearer {scraperToken}",
          "header": "Authorization"
        },
        "headerToken": {
          "default": false,
          "documentation": "Interface documentation",
          "format": "{scraperToken}",
          "header": "token"
        }
      }
    }
  },
  "endpoints": {
    "locations": {
      "asnPath": "/asn",
      "citiesPath": "/cities",
      "countriesPath": "/countries",
      "statesPath": "/states"
    },
    "serp": {
      "builderPath": "/builder",
      "requestPath": "/request"
    },
    "universal": {
      "requestPath": "/request"
    },
    "webScraper": {
      "downloadPath": "/tasks-download",
      "statusPath": "/tasks-status"
    }
  },
  "env": {
    "baseUrls": {
      "locations": {
        "default": "https://openapi.thordata.com/api/locations",
        "env": "THORDATA_LOCATIONS_BASE_URL"
      },
      "scraperapi": {
        "default": "https://scraperapi.thordata.com",
        "env": "THORDATA_SCRAPERAPI_BASE_URL"
      },
      "universalapi": {
        "default": "https://universalapi.thordata.com",
        "env": "THORDATA_UNIVERSALAPI_BASE_URL"
      },
      "webScraperApi": {
        "default": "https://openapi.thordata.com/api/web-scraper-api",
        "env": "THORDATA_WEB_SCRAPER_API_BASE_URL"
      }
    },
    "proxyEndpoint": {
      "host": {
        "env": "THORDATA_PROXY_HOST",
        "required": false
      },
      "port": {
        "default": 9999,
        "env": "THORDATA_PROXY_PORT",
        "required": false
      },
      "protocol": {
        "default": "https",
        "enum": [
          "http",
          "https"
        ],
        "env": "THORDATA_PROXY_PROTOCOL",
        "required": false
      }
    },
    "proxyEndpointPerProduct": {
      "datacenter": {
        "hostEnv": "THORDATA_DATACENTER_PROXY_HOST",
        "portEnv": "THORDATA_DATACENTER_PROXY_PORT",
        "protocolEnv": "THORDATA_DATACENTER_PROXY_PROTOCOL"
      },
      "mobile": {
        "hostEnv": "THORDATA_MOBILE_PROXY_HOST",
        "portEnv": "THORDATA_MOBILE_PROXY_PORT",
        "protocolEnv": "THORDATA_MOBILE_PROXY_PROTOCOL"
      },
      "residential": {
        "hostEnv": "THORDATA_RESIDENTIAL_PROXY_HOST",
        "portEnv": "THORDATA_RESIDENTIAL_PROXY_PORT",
        "protocolEnv": "THORDATA_RESIDENTIAL_PROXY_PROTOCOL"
      }
    },
    "tokens": {
      "publicKey": {
        "env": "THORDATA_PUBLIC_KEY",
        "required": false
      },
      "publicToken": {
        "env": "THORDATA_PUBLIC_TOKEN",
        "required": false
      },
      "scraperToken": {
        "env": "THORDATA_SCRAPER_TOKEN",
        "required": true
      }
    }
  },
  "errors": {
    "apiCodes": {
      "notCollected": {
        "code": 300,
        "description": "Request accepted but no data collected (not billed)",
        "retryable": true
      }
    },
    "httpStatus": {
      "auth": [
        401,
        403
      ],
      "rateLimit": [
        402,
        429
      ],
      "server": [
        500,
        502,
        503,
        504
      ],
      "validation": [
        400,
        404,
        422
      ]
    },
    "payload": {
      "codeField": "code",
      "messageFieldCandidates": [
        "msg",
        "message",
        "error",
        "detail",
        "description"
      ],
      "successCode": 200
    },
    "precedence": {
      "effectiveCodeRule": "payload_code_if_present_and_not_200_else_http_status"
    },
    "publicApiCodes": {
      "10000": {
        "message": "Parameter error",
        "retryable": false
      },
      "10001": {
        "message": "Unknown error",
        "retryable": false
      },
      "10011": {
        "message": "Frequent operations, please try again later",
        "retryable": true,
        "suggestedAction": "exponential_backoff"
      },
      "10013": {
        "category": "auth",
        "message": "public token error",
        "retryable": false
      },
      "10014": {
        "category": "auth",
        "message": "public key error",
        "retryable": false
      },
      "10018": {
        "category": "validation",
        "message": "The username already exists",
        "retryable": false
      },
      "10021": {
        "category": "validation",
        "message": "The query date cannot exceed 180 days",
        "retryable": false
      }
    },
    "serpCodes": [
      200,
      300,
      400,
      401,
      403,
      404,
      429,
      500,
      504
    ],
    "universalCodes": [
      200,
      300,
      400,
      401,
      429,
      500,
      504
    ]
  },
  "network": {
    "apiTraffic": {
      "notes": [
        "API calls may respect HTTP(S)_PROXY environment variables for users behind local proxies."
      ],
      "trustEnv": true
    },
    "chinaNotes": [
      "In restricted networks, prefer system-level tunneling (e.g., TUN mode) or routing that allows reaching Thordata gateway hosts."
    ],
    "proxyGatewayTraffic": {
      "notes": [
        "Proxy gateway traffic may bypass environment proxies by design to avoid routing API calls through the proxy gateway."
      ],
      "trustEnv": false
    }
  },
  "proxy": {
    "auth": {
      "passwordMayBeEmpty": true,
      "usernamePrefix": "td-customer-"
    },
    "credentialsEnv": {
      "datacenter": {
        "password": "THORDATA_DATACENTER_PASSWORD",
        "username": "THORDATA_DATACENTER_USERNAME"
      },
      "isp": {
        "host": "THORDATA_ISP_HOST",
        "password": "THORDATA_ISP_PASSWORD",
        "username": "THORDATA_ISP_USERNAME"
      },
      "mobile": {
        "password": "THORDATA_MOBILE_PASSWORD",
        "username": "THORDATA_MOBILE_USERNAME"
      },
      "residential": {
        "password": "THORDATA_RESIDENTIAL_PASSWORD",
        "username": "THORDATA_RESIDENTIAL_USERNAME"
      }
    },
    "gateway": {
      "compatibilityHosts": [
        "pr.thordata.net",
        "t.pr.thordata.net",
        "t.na.thordata.net",
        "t.eu.thordata.net",
        "t.as.thordata.net",
        "dc.pr.thordata.net",
        "m.pr.thordata.net",
        "isp.pr.thordata.net"
      ],
      "dashboardHostPatterns": [
        "{shard}.pr.thordata.net",
        "{shard}.{region}.thordata.net"
      ],
      "defaultHost": "pr.thordata.net",
      "endpointEnv": {
        "host": "THORDATA_PROXY_HOST",
        "port": "THORDATA_PROXY_PORT",
        "protocol": "THORDATA_PROXY_PROTOCOL"
      },
      "notes": [
        "Some accounts require HTTPS proxy endpoints (proxy URL must start with https://).",
        "SDK implementations must support TLS to proxy / TLS-in-TLS for HTTPS proxies."
      ],
      "regions": {
        "common": [
          "na",
          "eu",
          "as"
        ],
        "defaultRegionAlias": "pr",
        "residentialExtra": [
          "va"
        ]
      }
    },
    "products": {
      "datacenter": {
        "port": 7777
      },
      "isp": {
        "mode": "direct_or_gateway",
        "port": 6666
      },
      "mobile": {
        "port": 5555
      },
      "residential": {
        "port": 9999
      }
    },
    "usernameSegments": {
      "keys": {
        "asn": "asn",
        "city": "city",
        "continent": "continent",
        "country": "country",
        "sessionDurationMinutes": "sesstime",
        "sessionId": "sessid",
        "state": "state"
      },
      "order": [
        "continent",
        "country",
        "state",
        "city",
        "asn",
        "sessid",
        "sesstime"
      ]
    }
  },
  "publicApi": {
    "auth": {
      "headers": {
        "key": "publicKey",
        "token": "publicToken"
      }
    },
    "proxyExpiration": {
      "baseUrl": "https://openapi.thordata.com/api",
      "endpoint": "/proxy/expiration-time",
      "method": "GET",
      "params": [
        "token",
        "key",
        "proxy_type",
        "ips"
      ]
    },
    "proxyList": {
      "baseUrl": "https://api.thordata.com/api",
      "endpoint": "/proxy/proxy-list",
      "method": "GET",
      "params": [
        "token",
        "key",
        "proxy_type"
      ],
      "proxyTypes": {
        "1": "ISP",
        "2": "Datacenter"
      },
      "responseFields": [
        "ip",
        "port",
        "username",
        "password",
        "expiration_time"
      ]
    },
    "proxyUsers": {
      "create": {
        "baseUrl": "https://openapi.thordata.com/api",
        "endpoint": "/proxy-users/create-user",
        "method": "POST",
        "requestFields": {
          "password": {
            "required": true,
            "type": "string"
          },
          "proxy_type": {
            "required": true,
            "type": "integer"
          },
          "status": {
            "enum": [
              "true",
              "false"
            ],
            "type": "string"
          },
          "traffic_limit": {
            "description": "Traffic limit in MB (0 = unlimited, min 100)",
            "type": "integer"
          },
          "username": {
            "required": true,
            "type": "string"
          }
        }
      },
      "list": {
        "baseUrl": "https://openapi.thordata.com/api",
        "endpoint": "/proxy-users/user-list",
        "method": "GET",
        "params": [
          "token",
          "key",
          "proxy_type"
        ],
        "proxyTypes": {
          "1": "Residential",
          "2": "Unlimited"
        },
        "responseFields": {
          "limit": {
            "type": "number",
            "unit": "KB"
          },
          "list": {
            "items": {
              "password": "string",
              "status": "boolean",
              "traffic_limit": "integer",
              "usage_traffic": "number",
              "username": "string"
            },
            "type": "array"
          },
          "remaining_limit": {
            "type": "number",
            "unit": "KB"
          },
          "user_count": {
            "type": "integer"
          }
        }
      }
    },
    "usageStatistics": {
      "baseUrl": "https://openapi.thordata.com/api",
      "endpoint": "/account/usage-statistics",
      "method": "GET",
      "params": [
        "token",
        "key",
        "from_date",
        "to_date"
      ],
      "responseFields": {
        "data": {
          "description": "Daily usage breakdown",
          "type": "array"
        },
        "query_days": {
          "type": "integer"
        },
        "range_usage_traffic": {
          "type": "number",
          "unit": "KB"
        },
        "total_usage_traffic": {
          "type": "number",
          "unit": "KB"
        },
        "traffic_balance": {
          "type": "number",
          "unit": "KB"
        }
      }
    },
    "whitelist": {
      "addIp": {
        "baseUrl": "https://api.thordata.com/api",
        "endpoint": "/whitelisted-ips/add-ip",
        "method": "POST",
        "requestFields": {
          "ip": {
            "required": true,
            "type": "string"
          },
          "proxy_type": {
            "description": "1=Residential, 2=Unlimited, 9=Mobile",
            "type": "integer"
          },
          "status": {
            "enum": [
              "true",
              "false"
            ],
            "type": "string"
          }
        }
      }
    }
  },
  "serp": {
    "engines": {
      "aliases": {
        "bing_search": "bing",
        "duckduckgo_search": "duckduckgo",
        "google_search": "google",
        "yandex_search": "yandex"
      }
    },
    "mappings": {
      "searchTypeToTbm": {
        "images": "isch",
        "news": "nws",
        "shopping": "shop",
        "videos": "vid"
      },
      "timeFilterToTbs": {
        "day": "qdr:d",
        "hour": "qdr:h",
        "month": "qdr:m",
        "week": "qdr:w",
        "year": "qdr:y"
      }
    },
    "request": {
      "fields": {
        "engine": {
          "required": true,
          "type": "string"
        },
        "gl": {
          "type": "string"
        },
        "hl": {
          "type": "string"
        },
        "json": {
          "enum": [
            "0",
            "1"
          ],
          "meaning": "1=json,0=html",
          "type": "string"
        },
        "no_cache": {
          "enum": [
            "True",
            "False"
          ],
          "type": "string"
        },
        "num": {
          "type": "string"
        },
        "q": {
          "requiredIfEngineNot": [
            "yandex"
          ],
          "type": "string"
        },
        "render_js": {
          "enum": [
            "True",
            "False"
          ],
          "type": "string"
        },
        "start": {
          "type": "string"
        },
        "tbm": {
          "type": "string"
        },
        "tbs": {
          "type": "string"
        },
        "text": {
          "requiredIfEngine": [
            "yandex"
          ],
          "type": "string"
        }
      }
    }
  },
  "tasks": {
    "builder": {
      "auth": {
        "headers": {
          "AuthorizationBearer": "scraperToken",
          "key": "publicKey",
          "token": "publicToken"
        }
      },
      "baseUrl": "https://scraperapi.thordata.com",
      "endpoint": "/builder",
      "method": "POST",
      "requestFields": [
        "file_name",
        "spider_id",
        "spider_name",
        "spider_parameters",
        "spider_errors",
        "spider_universal"
      ]
    },
    "download": {
      "auth": {
        "headers": {
          "key": "publicKey",
          "token": "publicToken"
        }
      },
      "baseUrl": "https://openapi.thordata.com/api/web-scraper-api",
      "downloadTypes": [
        "json",
        "csv",
        "video",
        "subtitle",
        "audio"
      ],
      "endpoint": "/tasks-download",
      "method": "POST",
      "requestFields": [
        "tasks_id",
        "type"
      ]
    },
    "list": {
      "auth": {
        "headers": {
          "key": "publicKey",
          "token": "publicToken"
        }
      },
      "baseUrl": "https://openapi.thordata.com/api/web-scraper-api",
      "endpoint": "/tasks-list",
      "method": "POST",
      "requestFields": [
        "page",
        "size"
      ],
      "responseFields": [
        "count",
        "list"
      ]
    },
    "status": {
      "auth": {
        "headers": {
          "key": "publicKey",
          "token": "publicToken"
        }
      },
      "baseUrl": "https://openapi.thordata.com/api/web-scraper-api",
      "endpoint": "/tasks-status",
      "method": "POST",
      "requestFields": [
        "tasks_ids"
      ]
    },
    "videoBuilder": {
      "auth": {
        "headers": {
          "AuthorizationBearer": "scraperToken",
          "key": "publicKey",
          "token": "publicToken"
        }
      },
      "baseUrl": "https://scraperapi.thordata.com",
      "commonSettings": {
        "description": "YouTube video/audio common settings (JSON string)",
        "fields": {
          "audio_format": {
            "description": "Audio format (required for audio spider)",
            "enum": [
              "opus",
              "mp3"
            ],
            "type": "string"
          },
          "bitrate": {
            "description": "Audio bitrate (48/64/128/160/256/320 or with Kbps suffix)",
            "type": "string"
          },
          "is_subtitles": {
            "description": "Download subtitles (required)",
            "enum": [
              "true",
              "false"
            ],
            "type": "string"
          },
          "resolution": {
            "description": "Video resolution (optional, auto-downgrades if unavailable)",
            "enum": [
              "360p",
              "480p",
              "720p",
              "1080p",
              "1440p",
              "2160p"
            ],
            "type": "string"
          },
          "subtitles_language": {
            "description": "Subtitle language code (e.g., 'en', 'zh-Hans')",
            "type": "string"
          }
        }
      },
      "endpoint": "/video_builder",
      "method": "POST",
      "requestFields": [
        "file_name",
        "spider_id",
        "spider_name",
        "spider_parameters",
        "spider_errors",
        "common_settings"
      ]
    },
    "webhook": {
      "description": "Webhook events sent by Thordata when task status changes",
      "events": [
        "Running",
        "Task Succeeded",
        "Task Failed"
      ],
      "payload": {
        "apiCode": {
          "description": "API response code",
          "type": "integer"
        },
        "apiErrorMsg": {
          "description": "Error message if any",
          "type": "string"
        },
        "audioUrl": {
          "description": "Audio download URL (for audio tasks)",
          "type": "string"
        },
        "createdTime": {
          "description": "Task creation time",
          "type": "string"
        },
        "csvUrl": {
          "description": "CSV result download URL",
          "type": "string"
        },
        "endTime": {
          "description": "Task completion time",
          "type": "string"
        },
        "jsonUrl": {
          "description": "JSON result download URL",
          "type": "string"
        },
        "prodect_id": {
          "description": "Product ID",
          "type": "string"
        },
        "status": {
          "description": "Task status",
          "type": "string"
        },
        "subtitleUrl": {
          "description": "Subtitle download URL",
          "type": "string"
        },
        "taskId": {
          "description": "Task ID",
          "type": "string"
        },
        "videoUrl": {
          "description": "Video download URL (for video tasks)",
          "type": "string"
        }
      }
    }
  },
  "version": 1
}


========================================================================
FILE: sdk-spec\.github\workflows\ci.yml
SIZE: 725
TRUNCATED: no
========================================================================
name: CI

on:
  push:
  pull_request:

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: python -m pip install -r tools/requirements.txt

      - name: Build generated spec
        run: python tools/build_v1_json.py --out v1.generated.json

      - name: Validate generated spec
        run: python tools/validate_v1_json.py --spec v1.generated.json --schema schema/v1.schema.json

      - name: Ensure v1.json is in sync with YAML sources
        run: python tools/check_in_sync.py --generated v1.generated.json --canonical v1.json

========================================================================
FILE: sdk-spec\schema\v1.schema.json
SIZE: 560
TRUNCATED: no
========================================================================
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "object",
  "required": ["version", "auth", "env", "endpoints", "proxy", "serp", "tasks", "errors", "network"],
  "properties": {
    "version": { "type": "integer" },
    "auth": { "type": "object" },
    "env": { "type": "object" },
    "endpoints": { "type": "object" },
    "proxy": { "type": "object" },
    "serp": { "type": "object" },
    "tasks": { "type": "object" },
    "errors": { "type": "object" },
    "network": { "type": "object" }
  },
  "additionalProperties": true
}

========================================================================
FILE: sdk-spec\spec\v1\auth.yaml
SIZE: 3193
TRUNCATED: no
========================================================================
# spec/v1/auth.yaml
# Authentication modes and credential requirements for all Thordata APIs

# Token/credential sources
credentials:
  scraperToken:
    envVar: "THORDATA_SCRAPER_TOKEN"
    source: "Dashboard -> Account Settings"
    required: true
    usedBy: ["serp", "universal", "builder"]

  publicToken:
    envVar: "THORDATA_PUBLIC_TOKEN"
    source: "Dashboard -> My Account"
    required: false
    usedBy: ["publicApi", "locations", "tasks"]

  publicKey:
    envVar: "THORDATA_PUBLIC_KEY"
    source: "Dashboard -> My Account"
    required: false
    usedBy: ["publicApi", "locations", "tasks"]

# Authentication modes for SERP/Universal
serpUniversalAuth:
  modes:
    bearer:
      header: "Authorization"
      format: "Bearer {scraperToken}"
      documentation: "Thordata Docs examples"
      default: true
    headerToken:
      header: "token"
      format: "{scraperToken}"
      documentation: "Interface documentation"
      default: false

# API family authentication requirements
apiAuth:
  serp:
    required: ["scraperToken"]
    mode: "bearer or headerToken"
    headers:
      bearer: { Authorization: "Bearer {scraperToken}" }
      headerToken: { token: "{scraperToken}" }

  universal:
    required: ["scraperToken"]
    mode: "bearer or headerToken"
    headers:
      bearer: { Authorization: "Bearer {scraperToken}" }
      headerToken: { token: "{scraperToken}" }

  builder:
    required: ["scraperToken", "publicToken", "publicKey"]
    note: "Requires ALL THREE headers"
    headers:
      token: "{publicToken}"
      key: "{publicKey}"
      Authorization: "Bearer {scraperToken}"

  videoBuilder:
    required: ["scraperToken", "publicToken", "publicKey"]
    note: "Same as builder"
    headers:
      token: "{publicToken}"
      key: "{publicKey}"
      Authorization: "Bearer {scraperToken}"

  tasksStatus:
    required: ["publicToken", "publicKey"]
    headers:
      token: "{publicToken}"
      key: "{publicKey}"

  tasksDownload:
    required: ["publicToken", "publicKey"]
    headers:
      token: "{publicToken}"
      key: "{publicKey}"

  tasksList:
    required: ["publicToken", "publicKey"]
    headers:
      token: "{publicToken}"
      key: "{publicKey}"

  locations:
    required: ["publicToken", "publicKey"]
    method: "query params"
    params:
      token: "{publicToken}"
      key: "{publicKey}"

  publicApi:
    required: ["publicToken", "publicKey"]
    note: "Account management, proxy users, whitelist"
    headers:
      token: "{publicToken}"
      key: "{publicKey}"

# Decision tree for SDK implementers
decisionTree:
  question: "Which API are you calling?"
  answers:
    "SERP or Universal":
      useCredential: "scraperToken"
      authMode: "bearer (default) or headerToken"
    "Web Scraper builder/video_builder":
      useCredential: ["scraperToken", "publicToken", "publicKey"]
      note: "Need all three"
    "Task status/download/list":
      useCredential: ["publicToken", "publicKey"]
    "Locations API":
      useCredential: ["publicToken", "publicKey"]
      method: "query params"
    "Account management (usage, users, whitelist, proxy list)":
      useCredential: ["publicToken", "publicKey"]



========================================================================
FILE: sdk-spec\spec\v1\endpoints.yaml
SIZE: 295
TRUNCATED: no
========================================================================
serp:
  requestPath: "/request"
  builderPath: "/builder"

universal:
  requestPath: "/request"

webScraper:
  statusPath: "/tasks-status"
  downloadPath: "/tasks-download"

locations:
  countriesPath: "/countries"
  statesPath: "/states"
  citiesPath: "/cities"
  asnPath: "/asn"

========================================================================
FILE: sdk-spec\spec\v1\env.yaml
SIZE: 1387
TRUNCATED: no
========================================================================
tokens:
  scraperToken:
    env: THORDATA_SCRAPER_TOKEN
    required: true
  publicToken:
    env: THORDATA_PUBLIC_TOKEN
    required: false
  publicKey:
    env: THORDATA_PUBLIC_KEY
    required: false

baseUrls:
  scraperapi:
    env: THORDATA_SCRAPERAPI_BASE_URL
    default: "https://scraperapi.thordata.com"
  universalapi:
    env: THORDATA_UNIVERSALAPI_BASE_URL
    default: "https://universalapi.thordata.com"
  webScraperApi:
    env: THORDATA_WEB_SCRAPER_API_BASE_URL
    default: "https://openapi.thordata.com/api/web-scraper-api"
  locations:
    env: THORDATA_LOCATIONS_BASE_URL
    default: "https://openapi.thordata.com/api/locations"

proxyEndpoint:
  host:
    env: THORDATA_PROXY_HOST
    required: false
  port:
    env: THORDATA_PROXY_PORT
    required: false
    default: 9999
  protocol:
    env: THORDATA_PROXY_PROTOCOL
    required: false
    default: "https"
    enum: ["http", "https"]

proxyEndpointPerProduct:
  residential:
    hostEnv: THORDATA_RESIDENTIAL_PROXY_HOST
    portEnv: THORDATA_RESIDENTIAL_PROXY_PORT
    protocolEnv: THORDATA_RESIDENTIAL_PROXY_PROTOCOL
  datacenter:
    hostEnv: THORDATA_DATACENTER_PROXY_HOST
    portEnv: THORDATA_DATACENTER_PROXY_PORT
    protocolEnv: THORDATA_DATACENTER_PROXY_PROTOCOL
  mobile:
    hostEnv: THORDATA_MOBILE_PROXY_HOST
    portEnv: THORDATA_MOBILE_PROXY_PORT
    protocolEnv: THORDATA_MOBILE_PROXY_PROTOCOL

========================================================================
FILE: sdk-spec\spec\v1\errors.yaml
SIZE: 1703
TRUNCATED: no
========================================================================
# spec/v1/errors.yaml
# Error code definitions for all Thordata APIs

# Response payload structure
payload:
  successCode: 200
  codeField: "code"
  messageFieldCandidates: ["msg", "message", "error", "detail", "description"]

# Error precedence rule
precedence:
  effectiveCodeRule: "payload_code_if_present_and_not_200_else_http_status"

# API-specific codes (in response payload, NOT HTTP status)
apiCodes:
  notCollected:
    code: 300
    retryable: true
    description: "Request accepted but no data collected (not billed)"

# HTTP status code categories
httpStatus:
  auth: [401, 403]
  rateLimit: [402, 429]
  server: [500, 502, 503, 504]
  validation: [400, 404, 422]

# SERP API response codes
serpCodes:
  - 200  # Success
  - 300  # Not collected
  - 400  # Bad request
  - 401  # Unauthorized
  - 403  # Forbidden
  - 404  # Not found
  - 429  # Rate limit
  - 500  # Server error
  - 504  # Timeout

# Universal API response codes
universalCodes:
  - 200
  - 300
  - 400
  - 401
  - 429
  - 500
  - 504

# Public API error codes (10000 series)
publicApiCodes:
  10000:
    message: "Parameter error"
    retryable: false
  10001:
    message: "Unknown error"
    retryable: false
  10011:
    message: "Frequent operations, please try again later"
    retryable: true
    suggestedAction: "exponential_backoff"
  10013:
    message: "public token error"
    retryable: false
    category: "auth"
  10014:
    message: "public key error"
    retryable: false
    category: "auth"
  10018:
    message: "The username already exists"
    retryable: false
    category: "validation"
  10021:
    message: "The query date cannot exceed 180 days"
    retryable: false
    category: "validation"

========================================================================
FILE: sdk-spec\spec\v1\locations.yaml
SIZE: 0
TRUNCATED: no
========================================================================


========================================================================
FILE: sdk-spec\spec\v1\meta.yaml
SIZE: 78
TRUNCATED: no
========================================================================
version: 1
name: thordata-sdk-spec
status: stable
lastUpdated: "2025-12-23"

========================================================================
FILE: sdk-spec\spec\v1\network.yaml
SIZE: 466
TRUNCATED: no
========================================================================
apiTraffic:
  trustEnv: true
  notes:
    - "API calls may respect HTTP(S)_PROXY environment variables for users behind local proxies."

proxyGatewayTraffic:
  trustEnv: false
  notes:
    - "Proxy gateway traffic may bypass environment proxies by design to avoid routing API calls through the proxy gateway."

chinaNotes:
  - "In restricted networks, prefer system-level tunneling (e.g., TUN mode) or routing that allows reaching Thordata gateway hosts."

========================================================================
FILE: sdk-spec\spec\v1\proxy.yaml
SIZE: 2681
TRUNCATED: no
========================================================================
# spec/v1/proxy.yaml
gateway:
  endpointEnv:
    host: THORDATA_PROXY_HOST
    port: THORDATA_PROXY_PORT
    protocol: THORDATA_PROXY_PROTOCOL
  notes:
    - "Some accounts require HTTPS proxy endpoints (proxy URL must start with https://)."
    - "SDK implementations must support TLS to proxy / TLS-in-TLS for HTTPS proxies."
  # The SDK should allow users to explicitly pass a host; default values are only "fallbacks"
  defaultHost: "pr.thordata.net"

  # Dashboard will provide "account-specific" hosts (strongly recommended for users)
  dashboardHostPatterns:
    - "{shard}.pr.thordata.net"
    - "{shard}.{region}.thordata.net"

  # List of regions (according to dashboard)
  regions:
    common: ["na", "eu", "as"]     # Available for multiple products
    residentialExtra: ["va"]       # Exclusive to residential
    defaultRegionAlias: "pr"       # Exists in dashboard as {shard}.pr.thordata.net

  # Historical/compatible hosts used in the SDK (currently used in the code)
  compatibilityHosts:
    - "pr.thordata.net"
    - "t.pr.thordata.net"
    - "t.na.thordata.net"
    - "t.eu.thordata.net"
    - "t.as.thordata.net"
    - "dc.pr.thordata.net"
    - "m.pr.thordata.net"
    - "isp.pr.thordata.net"

auth:
  usernamePrefix: "td-customer-"
  # The client's default password is empty, but dashboard outputs include passwords
  # Therefore, empty passwords are not treated as standard here, only as "compatible"
  passwordMayBeEmpty: true

products:
  residential: { port: 9999 }
  mobile:      { port: 5555 }
  datacenter:  { port: 7777 }
  isp:         { port: 6666, mode: "direct_or_gateway" }  # In Python, StaticISPProxy is direct

credentialsEnv:
  residential: { username: "THORDATA_RESIDENTIAL_USERNAME", password: "THORDATA_RESIDENTIAL_PASSWORD" }
  mobile:      { username: "THORDATA_MOBILE_USERNAME",      password: "THORDATA_MOBILE_PASSWORD" }
  datacenter:  { username: "THORDATA_DATACENTER_USERNAME",  password: "THORDATA_DATACENTER_PASSWORD" }
  isp:
    host:      "THORDATA_ISP_HOST"
    username:  "THORDATA_ISP_USERNAME"
    password:  "THORDATA_ISP_PASSWORD"

# Username assembly rules (based on Python ProxyConfig.build_username / JS Proxy.buildUsername)
usernameSegments:
  order:
    - "continent"   # Supported in Python, not currently in JS
    - "country"
    - "state"       # Supported in Python, not currently in JS (JS has region field but not implemented)
    - "city"
    - "asn"         # Supported in Python, not currently in JS
    - "sessid"
    - "sesstime"
  keys:
    continent: "continent"
    country: "country"
    state: "state"
    city: "city"
    asn: "asn"
    sessionId: "sessid"
    sessionDurationMinutes: "sesstime"

========================================================================
FILE: sdk-spec\spec\v1\public_api.yaml
SIZE: 2757
TRUNCATED: no
========================================================================
# spec/v1/public_api.yaml
# Public API endpoints for account/proxy management (token + key auth)

auth:
  headers:
    token: "publicToken"
    key: "publicKey"

# Account usage statistics
usageStatistics:
  endpoint: "/account/usage-statistics"
  baseUrl: "https://openapi.thordata.com/api"
  method: "GET"
  params:
    - token
    - key
    - from_date
    - to_date
  responseFields:
    total_usage_traffic:
      type: "number"
      unit: "KB"
    traffic_balance:
      type: "number"
      unit: "KB"
    query_days:
      type: "integer"
    range_usage_traffic:
      type: "number"
      unit: "KB"
    data:
      type: "array"
      description: "Daily usage breakdown"

# Proxy user management
proxyUsers:
  list:
    endpoint: "/proxy-users/user-list"
    baseUrl: "https://openapi.thordata.com/api"
    method: "GET"
    params:
      - token
      - key
      - proxy_type
    proxyTypes:
      1: "Residential"
      2: "Unlimited"
    responseFields:
      limit:
        type: "number"
        unit: "KB"
      remaining_limit:
        type: "number"
        unit: "KB"
      user_count:
        type: "integer"
      list:
        type: "array"
        items:
          username: "string"
          password: "string"
          status: "boolean"
          traffic_limit: "integer"
          usage_traffic: "number"

  create:
    endpoint: "/proxy-users/create-user"
    baseUrl: "https://openapi.thordata.com/api"
    method: "POST"
    requestFields:
      proxy_type:
        type: "integer"
        required: true
      username:
        type: "string"
        required: true
      password:
        type: "string"
        required: true
      traffic_limit:
        type: "integer"
        description: "Traffic limit in MB (0 = unlimited, min 100)"
      status:
        type: "string"
        enum: ["true", "false"]

# Whitelist IP management
whitelist:
  addIp:
    endpoint: "/whitelisted-ips/add-ip"
    baseUrl: "https://api.thordata.com/api"
    method: "POST"
    requestFields:
      proxy_type:
        type: "integer"
        description: "1=Residential, 2=Unlimited, 9=Mobile"
      ip:
        type: "string"
        required: true
      status:
        type: "string"
        enum: ["true", "false"]

# ISP/Datacenter proxy list
proxyList:
  endpoint: "/proxy/proxy-list"
  baseUrl: "https://api.thordata.com/api"
  method: "GET"
  params:
    - token
    - key
    - proxy_type
  proxyTypes:
    1: "ISP"
    2: "Datacenter"
  responseFields:
    - ip
    - port
    - username
    - password
    - expiration_time

# Proxy expiration time
proxyExpiration:
  endpoint: "/proxy/expiration-time"
  baseUrl: "https://openapi.thordata.com/api"
  method: "GET"
  params:
    - token
    - key
    - proxy_type
    - ips

========================================================================
FILE: sdk-spec\spec\v1\serp.yaml
SIZE: 965
TRUNCATED: no
========================================================================
# spec/v1/serp.yaml
engines:
  aliases:
    google_search: google
    bing_search: bing
    yandex_search: yandex
    duckduckgo_search: duckduckgo

request:
  fields:
    engine: { type: "string", required: true }
    q:      { type: "string", requiredIfEngineNot: ["yandex"] }
    text:   { type: "string", requiredIfEngine: ["yandex"] }
    num:    { type: "string" }
    start:  { type: "string" }
    gl:     { type: "string" }
    hl:     { type: "string" }
    json:   { type: "string", enum: ["0", "1"], meaning: "1=json,0=html" }
    tbm:    { type: "string" }
    tbs:    { type: "string" }
    render_js: { type: "string", enum: ["True", "False"] }
    no_cache:  { type: "string", enum: ["True", "False"] }

mappings:
  searchTypeToTbm:
    images: "isch"
    shopping: "shop"
    news: "nws"
    videos: "vid"
  timeFilterToTbs:
    hour: "qdr:h"
    day: "qdr:d"
    week: "qdr:w"
    month: "qdr:m"
    year: "qdr:y"

========================================================================
FILE: sdk-spec\spec\v1\tasks.yaml
SIZE: 3580
TRUNCATED: no
========================================================================
# spec/v1/tasks.yaml
# Web Scraper API task definitions

# Text-based scraping task (existing)
builder:
  endpoint: "/builder"
  baseUrl: "https://scraperapi.thordata.com"
  method: "POST"
  auth:
    headers:
      token: "publicToken"
      key: "publicKey"
      AuthorizationBearer: "scraperToken"
  requestFields:
    - file_name
    - spider_id
    - spider_name
    - spider_parameters
    - spider_errors
    - spider_universal

# YouTube video/audio download task
videoBuilder:
  endpoint: "/video_builder"
  baseUrl: "https://scraperapi.thordata.com"
  method: "POST"
  auth:
    headers:
      token: "publicToken"
      key: "publicKey"
      AuthorizationBearer: "scraperToken"
  requestFields:
    - file_name
    - spider_id
    - spider_name
    - spider_parameters
    - spider_errors
    - common_settings
  commonSettings:
    description: "YouTube video/audio common settings (JSON string)"
    fields:
      resolution:
        type: "string"
        enum: ["360p", "480p", "720p", "1080p", "1440p", "2160p"]
        description: "Video resolution (optional, auto-downgrades if unavailable)"
      audio_format:
        type: "string"
        enum: ["opus", "mp3"]
        description: "Audio format (required for audio spider)"
      bitrate:
        type: "string"
        description: "Audio bitrate (48/64/128/160/256/320 or with Kbps suffix)"
      is_subtitles:
        type: "string"
        enum: ["true", "false"]
        description: "Download subtitles (required)"
      subtitles_language:
        type: "string"
        description: "Subtitle language code (e.g., 'en', 'zh-Hans')"

# Task list query
list:
  endpoint: "/tasks-list"
  baseUrl: "https://openapi.thordata.com/api/web-scraper-api"
  method: "POST"
  auth:
    headers:
      token: "publicToken"
      key: "publicKey"
  requestFields:
    - page
    - size
  responseFields:
    - count
    - list

# Task status query
status:
  endpoint: "/tasks-status"
  baseUrl: "https://openapi.thordata.com/api/web-scraper-api"
  method: "POST"
  auth:
    headers:
      token: "publicToken"
      key: "publicKey"
  requestFields:
    - tasks_ids

# Task download
download:
  endpoint: "/tasks-download"
  baseUrl: "https://openapi.thordata.com/api/web-scraper-api"
  method: "POST"
  auth:
    headers:
      token: "publicToken"
      key: "publicKey"
  requestFields:
    - tasks_id
    - type
  downloadTypes:
    - json
    - csv
    - video
    - subtitle
    - audio

# Webhook payload definition
webhook:
  description: "Webhook events sent by Thordata when task status changes"
  events:
    - Running
    - Task Succeeded
    - Task Failed
  payload:
    taskId:
      type: "string"
      description: "Task ID"
    prodect_id:
      type: "string"
      description: "Product ID"
    apiCode:
      type: "integer"
      description: "API response code"
    apiErrorMsg:
      type: "string"
      description: "Error message if any"
    jsonUrl:
      type: "string"
      description: "JSON result download URL"
    csvUrl:
      type: "string"
      description: "CSV result download URL"
    videoUrl:
      type: "string"
      description: "Video download URL (for video tasks)"
    audioUrl:
      type: "string"
      description: "Audio download URL (for audio tasks)"
    subtitleUrl:
      type: "string"
      description: "Subtitle download URL"
    status:
      type: "string"
      description: "Task status"
    createdTime:
      type: "string"
      description: "Task creation time"
    endTime:
      type: "string"
      description: "Task completion time"

========================================================================
FILE: sdk-spec\spec\v1\universal.yaml
SIZE: 0
TRUNCATED: no
========================================================================


========================================================================
FILE: sdk-spec\tools\build_v1_json.py
SIZE: 1637
TRUNCATED: no
========================================================================
from __future__ import annotations

import argparse
import json
from pathlib import Path

import yaml


def load_yaml(path: Path) -> dict | None:
    text = path.read_text(encoding="utf-8")
    if not text.strip():
        return None
    data = yaml.safe_load(text)
    if data is None:
        return None
    if not isinstance(data, dict):
        raise ValueError(f"{path} must contain a YAML mapping at top-level.")
    return data


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--out", default="v1.generated.json")
    args = ap.parse_args()

    root = Path(__file__).resolve().parents[1]
    v1_dir = root / "spec" / "v1"

    meta = load_yaml(v1_dir / "meta.yaml") or {}
    version = int(meta.get("version", 1))

    out: dict = {"version": version}

    for key, filename in [
        ("auth", "auth.yaml"),
        ("env", "env.yaml"),
        ("endpoints", "endpoints.yaml"),
        ("proxy", "proxy.yaml"),
        ("serp", "serp.yaml"),
        ("universal", "universal.yaml"),
        ("tasks", "tasks.yaml"),
        ("publicApi", "public_api.yaml"),
        ("errors", "errors.yaml"),
        ("network", "network.yaml"),
        ("locations", "locations.yaml"),
    ]:
        data = load_yaml(v1_dir / filename)
        if data is None:
            continue
        out[key] = data

    out_path = root / args.out
    out_path.write_text(
        json.dumps(out, ensure_ascii=False, indent=2, sort_keys=True) + "\n",
        encoding="utf-8",
    )
    print(f"Generated: {out_path}")


if __name__ == "__main__":
    main()

========================================================================
FILE: sdk-spec\tools\check_in_sync.py
SIZE: 845
TRUNCATED: no
========================================================================
from __future__ import annotations

import argparse
import json
from pathlib import Path


def canonical_json(path: Path) -> str:
    obj = json.loads(path.read_text(encoding="utf-8"))
    return json.dumps(obj, ensure_ascii=False, indent=2, sort_keys=True) + "\n"


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--generated", required=True)
    ap.add_argument("--canonical", required=True)
    args = ap.parse_args()

    gen = canonical_json(Path(args.generated))
    can = canonical_json(Path(args.canonical))

    if gen != can:
        raise SystemExit(
            "Spec is out of sync.\n"
            "Run: python tools/build_v1_json.py --out v1.generated.json\n"
            "Then update v1.json to match v1.generated.json.\n"
        )


if __name__ == "__main__":
    main()

========================================================================
FILE: sdk-spec\tools\requirements.txt
SIZE: 33
TRUNCATED: no
========================================================================
PyYAML==6.0.2
jsonschema==4.23.0

========================================================================
FILE: sdk-spec\tools\validate_v1_json.py
SIZE: 1238
TRUNCATED: no
========================================================================
from __future__ import annotations

import argparse
import json
from pathlib import Path

from jsonschema import validate


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--spec", required=True)
    ap.add_argument("--schema", required=True)
    args = ap.parse_args()

    spec = json.loads(Path(args.spec).read_text(encoding="utf-8"))
    schema = json.loads(Path(args.schema).read_text(encoding="utf-8"))

    validate(instance=spec, schema=schema)

    # Lightweight invariants (keep this minimal and stable)
    rule = spec["errors"]["precedence"]["effectiveCodeRule"]
    if rule != "payload_code_if_present_and_not_200_else_http_status":
        raise SystemExit(f"Unexpected effectiveCodeRule: {rule}")

    ports = spec["proxy"]["products"]
    for k in ("residential", "mobile", "datacenter", "isp"):
        p = ports[k]["port"]
        if not isinstance(p, int):
            raise SystemExit(f"proxy.products.{k}.port must be int, got {type(p).__name__}")

    tbm = spec["serp"]["mappings"]["searchTypeToTbm"]
    if tbm.get("news") != "nws":
        raise SystemExit("serp.mappings.searchTypeToTbm.news must be 'nws'")


if __name__ == "__main__":
    main()

========================================================================
FILE: src\thordata\async_client.py
SIZE: 57071
TRUNCATED: no
========================================================================
"""
Asynchronous client for the Thordata API.

This module provides the AsyncThordataClient for high-concurrency workloads,
built on aiohttp.

Example:
    >>> import asyncio
    >>> from thordata import AsyncThordataClient
    >>>
    >>> async def main():
    ...     async with AsyncThordataClient(
    ...         scraper_token="your_token",
    ...         public_token="your_public_token",
    ...         public_key="your_public_key"
    ...     ) as client:
    ...         response = await client.get("https://httpbin.org/ip")
    ...         print(await response.json())
    >>>
    >>> asyncio.run(main())
"""

from __future__ import annotations

import asyncio
import logging
import os
from datetime import date
from typing import Any, Dict, List, Optional, Union

import aiohttp

from . import __version__ as _sdk_version
from ._utils import (
    build_auth_headers,
    build_builder_headers,
    build_public_api_headers,
    build_user_agent,
    decode_base64_image,
    extract_error_message,
    parse_json_response,
)
from .enums import Engine, ProxyType
from .exceptions import (
    ThordataConfigError,
    ThordataNetworkError,
    ThordataTimeoutError,
    raise_for_code,
)
from .models import (
    CommonSettings,
    ProxyConfig,
    ProxyProduct,
    ProxyServer,
    ProxyUserList,
    ScraperTaskConfig,
    SerpRequest,
    UniversalScrapeRequest,
    UsageStatistics,
    VideoTaskConfig,
    WhitelistProxyConfig,
)
from .retry import RetryConfig

logger = logging.getLogger(__name__)


class AsyncThordataClient:
    """
    The official asynchronous Python client for Thordata.

    Designed for high-concurrency AI agents and data pipelines.

    Args:
        scraper_token: The API token from your Dashboard.
        public_token: The public API token.
        public_key: The public API key.
        proxy_host: Custom proxy gateway host.
        proxy_port: Custom proxy gateway port.
        timeout: Default request timeout in seconds.
        retry_config: Configuration for automatic retries.

    Example:
        >>> async with AsyncThordataClient(
        ...     scraper_token="token",
        ...     public_token="pub_token",
        ...     public_key="pub_key"
        ... ) as client:
        ...     results = await client.serp_search("python")
    """

    # API Endpoints (same as sync client)
    BASE_URL = "https://scraperapi.thordata.com"
    UNIVERSAL_URL = "https://universalapi.thordata.com"
    API_URL = "https://openapi.thordata.com/api/web-scraper-api"
    LOCATIONS_URL = "https://openapi.thordata.com/api/locations"

    def __init__(
        self,
        scraper_token: str,
        public_token: Optional[str] = None,
        public_key: Optional[str] = None,
        proxy_host: str = "pr.thordata.net",
        proxy_port: int = 9999,
        timeout: int = 30,
        api_timeout: int = 60,
        retry_config: Optional[RetryConfig] = None,
        auth_mode: str = "bearer",
        scraperapi_base_url: Optional[str] = None,
        universalapi_base_url: Optional[str] = None,
        web_scraper_api_base_url: Optional[str] = None,
        locations_base_url: Optional[str] = None,
    ) -> None:
        """Initialize the Async Thordata Client."""
        if not scraper_token:
            raise ThordataConfigError("scraper_token is required")

        self.scraper_token = scraper_token
        self.public_token = public_token
        self.public_key = public_key

        # Proxy configuration
        self._proxy_host = proxy_host
        self._proxy_port = proxy_port

        # Timeout configuration
        self._default_timeout = aiohttp.ClientTimeout(total=timeout)
        self._api_timeout = aiohttp.ClientTimeout(total=api_timeout)

        # Retry configuration
        self._retry_config = retry_config or RetryConfig()

        # Authentication mode (for scraping APIs)
        self._auth_mode = auth_mode.lower()
        if self._auth_mode not in ("bearer", "header_token"):
            raise ThordataConfigError(
                f"Invalid auth_mode: {auth_mode}. Must be 'bearer' or 'header_token'."
            )

        # Base URLs (allow override via args or env vars for testing and custom routing)
        scraperapi_base = (
            scraperapi_base_url
            or os.getenv("THORDATA_SCRAPERAPI_BASE_URL")
            or self.BASE_URL
        ).rstrip("/")

        universalapi_base = (
            universalapi_base_url
            or os.getenv("THORDATA_UNIVERSALAPI_BASE_URL")
            or self.UNIVERSAL_URL
        ).rstrip("/")

        web_scraper_api_base = (
            web_scraper_api_base_url
            or os.getenv("THORDATA_WEB_SCRAPER_API_BASE_URL")
            or self.API_URL
        ).rstrip("/")

        locations_base = (
            locations_base_url
            or os.getenv("THORDATA_LOCATIONS_BASE_URL")
            or self.LOCATIONS_URL
        ).rstrip("/")

        # Keep these env overrides for now
        gateway_base = os.getenv(
            "THORDATA_GATEWAY_BASE_URL", "https://api.thordata.com/api/gateway"
        )
        child_base = os.getenv(
            "THORDATA_CHILD_BASE_URL", "https://api.thordata.com/api/child"
        )

        self._gateway_base_url = gateway_base
        self._child_base_url = child_base

        self._serp_url = f"{scraperapi_base}/request"
        self._builder_url = f"{scraperapi_base}/builder"
        self._video_builder_url = f"{scraperapi_base}/video_builder"
        self._universal_url = f"{universalapi_base}/request"

        self._status_url = f"{web_scraper_api_base}/tasks-status"
        self._download_url = f"{web_scraper_api_base}/tasks-download"
        self._list_url = f"{web_scraper_api_base}/tasks-list"

        self._locations_base_url = locations_base
        self._usage_stats_url = (
            f"{locations_base.replace('/locations', '')}/account/usage-statistics"
        )
        self._proxy_users_url = (
            f"{locations_base.replace('/locations', '')}/proxy-users"
        )

        whitelist_base = os.getenv(
            "THORDATA_WHITELIST_BASE_URL", "https://api.thordata.com/api"
        )
        self._whitelist_url = f"{whitelist_base}/whitelisted-ips"

        proxy_api_base = os.getenv(
            "THORDATA_PROXY_API_BASE_URL", "https://api.thordata.com/api"
        )
        self._proxy_list_url = f"{proxy_api_base}/proxy/proxy-list"
        self._proxy_expiration_url = f"{proxy_api_base}/proxy/expiration-time"

        # Session initialized lazily
        self._session: Optional[aiohttp.ClientSession] = None

    async def __aenter__(self) -> AsyncThordataClient:
        """Async context manager entry."""
        if self._session is None or self._session.closed:
            self._session = aiohttp.ClientSession(
                timeout=self._api_timeout,
                trust_env=True,
                headers={"User-Agent": build_user_agent(_sdk_version, "aiohttp")},
            )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        """Async context manager exit."""
        await self.close()

    async def close(self) -> None:
        """Close the underlying aiohttp session."""
        if self._session and not self._session.closed:
            await self._session.close()
            self._session = None

    def _get_session(self) -> aiohttp.ClientSession:
        """Get the session, raising if not initialized."""
        if self._session is None or self._session.closed:
            raise RuntimeError(
                "Client session not initialized. "
                "Use 'async with AsyncThordataClient(...) as client:'"
            )
        return self._session

    # =========================================================================
    # Proxy Network Methods
    # =========================================================================

    async def get(
        self,
        url: str,
        *,
        proxy_config: Optional[ProxyConfig] = None,
        **kwargs: Any,
    ) -> aiohttp.ClientResponse:
        """
        Send an async GET request through the Proxy Network.

        Args:
            url: The target URL.
            proxy_config: Custom proxy configuration.
            **kwargs: Additional aiohttp arguments.

        Returns:
            The aiohttp response object.
        """
        session = self._get_session()

        logger.debug(f"Async Proxy GET: {url}")

        if proxy_config is None:
            proxy_config = self._get_default_proxy_config_from_env()

        if proxy_config is None:
            raise ThordataConfigError(
                "Proxy credentials are missing. "
                "Pass proxy_config=ProxyConfig(username=..., password=..., product=...) "
                "or set THORDATA_RESIDENTIAL_USERNAME/THORDATA_RESIDENTIAL_PASSWORD (or DATACENTER/MOBILE)."
            )

        # aiohttp has limited support for "https://" proxies (TLS to proxy / TLS-in-TLS).
        # Your account's proxy endpoint requires HTTPS proxy, so we explicitly block here
        # to avoid confusing "it always fails" behavior.
        if getattr(proxy_config, "protocol", "http").lower() == "https":
            raise ThordataConfigError(
                "Proxy Network requires an HTTPS proxy endpoint (TLS to proxy) for your account. "
                "aiohttp support for 'https://' proxies is limited and may fail. "
                "Please use ThordataClient.get/post (sync client) for Proxy Network requests."
            )
        proxy_url, proxy_auth = proxy_config.to_aiohttp_config()

        try:
            return await session.get(
                url, proxy=proxy_url, proxy_auth=proxy_auth, **kwargs
            )
        except asyncio.TimeoutError as e:
            raise ThordataTimeoutError(
                f"Async request timed out: {e}", original_error=e
            ) from e
        except aiohttp.ClientError as e:
            raise ThordataNetworkError(
                f"Async request failed: {e}", original_error=e
            ) from e

    async def post(
        self,
        url: str,
        *,
        proxy_config: Optional[ProxyConfig] = None,
        **kwargs: Any,
    ) -> aiohttp.ClientResponse:
        """
        Send an async POST request through the Proxy Network.

        Args:
            url: The target URL.
            proxy_config: Custom proxy configuration.
            **kwargs: Additional aiohttp arguments.

        Returns:
            The aiohttp response object.
        """
        session = self._get_session()

        logger.debug(f"Async Proxy POST: {url}")

        if proxy_config is None:
            proxy_config = self._get_default_proxy_config_from_env()

        if proxy_config is None:
            raise ThordataConfigError(
                "Proxy credentials are missing. "
                "Pass proxy_config=ProxyConfig(username=..., password=..., product=...) "
                "or set THORDATA_RESIDENTIAL_USERNAME/THORDATA_RESIDENTIAL_PASSWORD (or DATACENTER/MOBILE)."
            )

        # aiohttp has limited support for "https://" proxies (TLS to proxy / TLS-in-TLS).
        # Your account's proxy endpoint requires HTTPS proxy, so we explicitly block here
        # to avoid confusing "it always fails" behavior.
        if getattr(proxy_config, "protocol", "http").lower() == "https":
            raise ThordataConfigError(
                "Proxy Network requires an HTTPS proxy endpoint (TLS to proxy) for your account. "
                "aiohttp support for 'https://' proxies is limited and may fail. "
                "Please use ThordataClient.get/post (sync client) for Proxy Network requests."
            )
        proxy_url, proxy_auth = proxy_config.to_aiohttp_config()

        try:
            return await session.post(
                url, proxy=proxy_url, proxy_auth=proxy_auth, **kwargs
            )
        except asyncio.TimeoutError as e:
            raise ThordataTimeoutError(
                f"Async request timed out: {e}", original_error=e
            ) from e
        except aiohttp.ClientError as e:
            raise ThordataNetworkError(
                f"Async request failed: {e}", original_error=e
            ) from e

    # =========================================================================
    # SERP API Methods
    # =========================================================================

    async def serp_search(
        self,
        query: str,
        *,
        engine: Union[Engine, str] = Engine.GOOGLE,
        num: int = 10,
        country: Optional[str] = None,
        language: Optional[str] = None,
        search_type: Optional[str] = None,
        device: Optional[str] = None,
        render_js: Optional[bool] = None,
        no_cache: Optional[bool] = None,
        output_format: str = "json",
        **kwargs: Any,
    ) -> Dict[str, Any]:
        """
        Execute an async SERP search.

        Args:
            query: Search keywords.
            engine: Search engine.
            num: Number of results.
            country: Country code for localization.
            language: Language code.
            search_type: Type of search.
            device: Device type ('desktop', 'mobile', 'tablet').
            render_js: Enable JavaScript rendering in SERP.
            no_cache: Disable internal caching.
            output_format: 'json' or 'html'.
            **kwargs: Additional parameters.

        Returns:
            Parsed JSON results or dict with 'html' key.
        """
        session = self._get_session()

        engine_str = engine.value if isinstance(engine, Engine) else engine.lower()

        request = SerpRequest(
            query=query,
            engine=engine_str,
            num=num,
            country=country,
            language=language,
            search_type=search_type,
            device=device,
            render_js=render_js,
            no_cache=no_cache,
            output_format=output_format,
            extra_params=kwargs,
        )

        payload = request.to_payload()
        headers = build_auth_headers(self.scraper_token, mode=self._auth_mode)

        logger.info(f"Async SERP Search: {engine_str} - {query}")

        try:
            async with session.post(
                self._serp_url,
                data=payload,
                headers=headers,
            ) as response:
                response.raise_for_status()

                if output_format.lower() == "json":
                    data = await response.json()

                    if isinstance(data, dict):
                        code = data.get("code")
                        if code is not None and code != 200:
                            msg = extract_error_message(data)
                            raise_for_code(
                                f"SERP API Error: {msg}",
                                code=code,
                                payload=data,
                            )

                    return parse_json_response(data)

                text = await response.text()
                return {"html": text}

        except asyncio.TimeoutError as e:
            raise ThordataTimeoutError(
                f"SERP request timed out: {e}",
                original_error=e,
            ) from e
        except aiohttp.ClientError as e:
            raise ThordataNetworkError(
                f"SERP request failed: {e}",
                original_error=e,
            ) from e

    async def serp_search_advanced(self, request: SerpRequest) -> Dict[str, Any]:
        """
        Execute an async SERP search using a SerpRequest object.
        """
        session = self._get_session()

        payload = request.to_payload()
        headers = build_auth_headers(self.scraper_token, mode=self._auth_mode)

        logger.info(f"Async SERP Advanced: {request.engine} - {request.query}")

        try:
            async with session.post(
                self._serp_url,
                data=payload,
                headers=headers,
            ) as response:
                response.raise_for_status()

                if request.output_format.lower() == "json":
                    data = await response.json()

                    if isinstance(data, dict):
                        code = data.get("code")
                        if code is not None and code != 200:
                            msg = extract_error_message(data)
                            raise_for_code(
                                f"SERP API Error: {msg}",
                                code=code,
                                payload=data,
                            )

                    return parse_json_response(data)

                text = await response.text()
                return {"html": text}

        except asyncio.TimeoutError as e:
            raise ThordataTimeoutError(
                f"SERP request timed out: {e}",
                original_error=e,
            ) from e
        except aiohttp.ClientError as e:
            raise ThordataNetworkError(
                f"SERP request failed: {e}",
                original_error=e,
            ) from e

    # =========================================================================
    # Universal Scraping API Methods
    # =========================================================================

    async def universal_scrape(
        self,
        url: str,
        *,
        js_render: bool = False,
        output_format: str = "html",
        country: Optional[str] = None,
        block_resources: Optional[str] = None,
        wait: Optional[int] = None,
        wait_for: Optional[str] = None,
        **kwargs: Any,
    ) -> Union[str, bytes]:
        """
        Async scrape using Universal API (Web Unlocker).

        Args:
            url: Target URL.
            js_render: Enable JavaScript rendering.
            output_format: "html" or "png".
            country: Geo-targeting country.
            block_resources: Resources to block.
            wait: Wait time in ms.
            wait_for: CSS selector to wait for.

        Returns:
            HTML string or PNG bytes.
        """
        request = UniversalScrapeRequest(
            url=url,
            js_render=js_render,
            output_format=output_format,
            country=country,
            block_resources=block_resources,
            wait=wait,
            wait_for=wait_for,
            extra_params=kwargs,
        )

        return await self.universal_scrape_advanced(request)

    async def universal_scrape_advanced(
        self, request: UniversalScrapeRequest
    ) -> Union[str, bytes]:
        """
        Async scrape using a UniversalScrapeRequest object.
        """
        session = self._get_session()

        payload = request.to_payload()
        headers = build_auth_headers(self.scraper_token, mode=self._auth_mode)

        logger.info(f"Async Universal Scrape: {request.url}")

        try:
            async with session.post(
                self._universal_url, data=payload, headers=headers
            ) as response:
                response.raise_for_status()

                try:
                    resp_json = await response.json()
                except ValueError:
                    if request.output_format.lower() == "png":
                        return await response.read()
                    return await response.text()

                # Check for API errors
                if isinstance(resp_json, dict):
                    code = resp_json.get("code")
                    if code is not None and code != 200:
                        msg = extract_error_message(resp_json)
                        raise_for_code(
                            f"Universal API Error: {msg}", code=code, payload=resp_json
                        )

                if "html" in resp_json:
                    return resp_json["html"]

                if "png" in resp_json:
                    return decode_base64_image(resp_json["png"])

                return str(resp_json)

        except asyncio.TimeoutError as e:
            raise ThordataTimeoutError(
                f"Universal scrape timed out: {e}", original_error=e
            ) from e
        except aiohttp.ClientError as e:
            raise ThordataNetworkError(
                f"Universal scrape failed: {e}", original_error=e
            ) from e

    # =========================================================================
    # Web Scraper API Methods
    # =========================================================================

    async def create_scraper_task(
        self,
        file_name: str,
        spider_id: str,
        spider_name: str,
        parameters: Dict[str, Any],
        universal_params: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        Create an async Web Scraper task.
        """
        config = ScraperTaskConfig(
            file_name=file_name,
            spider_id=spider_id,
            spider_name=spider_name,
            parameters=parameters,
            universal_params=universal_params,
        )

        return await self.create_scraper_task_advanced(config)

    async def create_scraper_task_advanced(self, config: ScraperTaskConfig) -> str:
        """
        Create a task using ScraperTaskConfig.
        """
        self._require_public_credentials()
        session = self._get_session()

        payload = config.to_payload()
        # Builder needs 3 headers: token, key, Authorization Bearer
        headers = build_builder_headers(
            self.scraper_token,
            self.public_token or "",
            self.public_key or "",
        )

        logger.info(f"Async Task Creation: {config.spider_name}")

        try:
            async with session.post(
                self._builder_url, data=payload, headers=headers
            ) as response:
                response.raise_for_status()
                data = await response.json()

                code = data.get("code")
                if code != 200:
                    msg = extract_error_message(data)
                    raise_for_code(
                        f"Task creation failed: {msg}", code=code, payload=data
                    )

                return data["data"]["task_id"]

        except aiohttp.ClientError as e:
            raise ThordataNetworkError(
                f"Task creation failed: {e}", original_error=e
            ) from e

    async def create_video_task(
        self,
        file_name: str,
        spider_id: str,
        spider_name: str,
        parameters: Dict[str, Any],
        common_settings: CommonSettings,
    ) -> str:
        """
        Create a YouTube video/audio download task.
        """

        config = VideoTaskConfig(
            file_name=file_name,
            spider_id=spider_id,
            spider_name=spider_name,
            parameters=parameters,
            common_settings=common_settings,
        )

        return await self.create_video_task_advanced(config)

    async def create_video_task_advanced(self, config: VideoTaskConfig) -> str:
        """
        Create a video task using VideoTaskConfig object.
        """

        self._require_public_credentials()
        session = self._get_session()

        payload = config.to_payload()
        headers = build_builder_headers(
            self.scraper_token,
            self.public_token or "",
            self.public_key or "",
        )

        logger.info(
            f"Async Video Task Creation: {config.spider_name} - {config.spider_id}"
        )

        try:
            async with session.post(
                self._video_builder_url,
                data=payload,
                headers=headers,
                timeout=self._api_timeout,
            ) as response:
                response.raise_for_status()
                data = await response.json()

                code = data.get("code")
                if code != 200:
                    msg = extract_error_message(data)
                    raise_for_code(
                        f"Video task creation failed: {msg}", code=code, payload=data
                    )

                return data["data"]["task_id"]

        except asyncio.TimeoutError as e:
            raise ThordataTimeoutError(
                f"Video task creation timed out: {e}", original_error=e
            ) from e
        except aiohttp.ClientError as e:
            raise ThordataNetworkError(
                f"Video task creation failed: {e}", original_error=e
            ) from e

    async def get_task_status(self, task_id: str) -> str:
        """
        Check async task status.

        Raises:
            ThordataConfigError: If public credentials are missing.
            ThordataAPIError: If API returns a non-200 code in JSON payload.
            ThordataNetworkError: If network/HTTP request fails.
        """
        self._require_public_credentials()
        session = self._get_session()

        headers = build_public_api_headers(
            self.public_token or "", self.public_key or ""
        )
        payload = {"tasks_ids": task_id}

        try:
            async with session.post(
                self._status_url, data=payload, headers=headers
            ) as response:
                response.raise_for_status()
                data = await response.json()

                if isinstance(data, dict):
                    code = data.get("code")
                    if code is not None and code != 200:
                        msg = extract_error_message(data)
                        raise_for_code(
                            f"Task status API Error: {msg}",
                            code=code,
                            payload=data,
                        )

                    items = data.get("data") or []
                    for item in items:
                        if str(item.get("task_id")) == str(task_id):
                            return item.get("status", "unknown")

                    return "unknown"

                raise ThordataNetworkError(
                    f"Unexpected task status response type: {type(data).__name__}",
                    original_error=None,
                )

        except asyncio.TimeoutError as e:
            raise ThordataTimeoutError(
                f"Async status check timed out: {e}", original_error=e
            ) from e
        except aiohttp.ClientError as e:
            raise ThordataNetworkError(
                f"Async status check failed: {e}", original_error=e
            ) from e

    async def safe_get_task_status(self, task_id: str) -> str:
        """
        Backward-compatible status check.

        Returns:
            Status string, or "error" on any exception.
        """
        try:
            return await self.get_task_status(task_id)
        except Exception:
            return "error"

    async def get_task_result(self, task_id: str, file_type: str = "json") -> str:
        """
        Get download URL for completed task.
        """
        self._require_public_credentials()
        session = self._get_session()

        headers = build_public_api_headers(
            self.public_token or "", self.public_key or ""
        )
        payload = {"tasks_id": task_id, "type": file_type}

        logger.info(f"Async getting result for Task: {task_id}")

        try:
            async with session.post(
                self._download_url, data=payload, headers=headers
            ) as response:
                data = await response.json()
                code = data.get("code")

                if code == 200 and data.get("data"):
                    return data["data"]["download"]

                msg = extract_error_message(data)
                raise_for_code(f"Get result failed: {msg}", code=code, payload=data)
                # This line won't be reached, but satisfies mypy
                raise RuntimeError("Unexpected state")

        except aiohttp.ClientError as e:
            raise ThordataNetworkError(
                f"Get result failed: {e}", original_error=e
            ) from e

    async def list_tasks(
        self,
        page: int = 1,
        size: int = 20,
    ) -> Dict[str, Any]:
        """
        List all Web Scraper tasks.

        Args:
            page: Page number (starts from 1).
            size: Number of tasks per page.

        Returns:
            Dict containing 'count' and 'list' of tasks.
        """
        self._require_public_credentials()
        session = self._get_session()

        headers = build_public_api_headers(
            self.public_token or "", self.public_key or ""
        )
        payload: Dict[str, Any] = {}
        if page:
            payload["page"] = str(page)
        if size:
            payload["size"] = str(size)

        logger.info(f"Async listing tasks: page={page}, size={size}")

        try:
            async with session.post(
                self._list_url,
                data=payload,
                headers=headers,
                timeout=self._api_timeout,
            ) as response:
                response.raise_for_status()
                data = await response.json()

                code = data.get("code")
                if code != 200:
                    msg = extract_error_message(data)
                    raise_for_code(f"List tasks failed: {msg}", code=code, payload=data)

                return data.get("data", {"count": 0, "list": []})

        except asyncio.TimeoutError as e:
            raise ThordataTimeoutError(
                f"List tasks timed out: {e}", original_error=e
            ) from e
        except aiohttp.ClientError as e:
            raise ThordataNetworkError(
                f"List tasks failed: {e}", original_error=e
            ) from e

    async def wait_for_task(
        self,
        task_id: str,
        *,
        poll_interval: float = 5.0,
        max_wait: float = 600.0,
    ) -> str:
        """
        Wait for a task to complete.
        """

        import time

        start = time.monotonic()

        while (time.monotonic() - start) < max_wait:
            status = await self.get_task_status(task_id)

            logger.debug(f"Task {task_id} status: {status}")

            terminal_statuses = {
                "ready",
                "success",
                "finished",
                "failed",
                "error",
                "cancelled",
            }

            if status.lower() in terminal_statuses:
                return status

            await asyncio.sleep(poll_interval)

        raise TimeoutError(f"Task {task_id} did not complete within {max_wait} seconds")

    # =========================================================================
    # Proxy Account Management Methods
    # =========================================================================

    async def get_usage_statistics(
        self,
        from_date: Union[str, date],
        to_date: Union[str, date],
    ) -> UsageStatistics:
        """
        Get account usage statistics for a date range.

        Args:
            from_date: Start date (YYYY-MM-DD string or date object).
            to_date: End date (YYYY-MM-DD string or date object).

        Returns:
            UsageStatistics object with traffic data.
        """

        self._require_public_credentials()
        session = self._get_session()

        # Convert dates to strings
        if isinstance(from_date, date):
            from_date = from_date.strftime("%Y-%m-%d")
        if isinstance(to_date, date):
            to_date = to_date.strftime("%Y-%m-%d")

        params = {
            "token": self.public_token,
            "key": self.public_key,
            "from_date": from_date,
            "to_date": to_date,
        }

        logger.info(f"Async getting usage statistics: {from_date} to {to_date}")

        try:
            async with session.get(
                self._usage_stats_url,
                params=params,
                timeout=self._api_timeout,
            ) as response:
                response.raise_for_status()
                data = await response.json()

                if isinstance(data, dict):
                    code = data.get("code")
                    if code is not None and code != 200:
                        msg = extract_error_message(data)
                        raise_for_code(
                            f"Usage statistics error: {msg}",
                            code=code,
                            payload=data,
                        )

                    usage_data = data.get("data", data)
                    return UsageStatistics.from_dict(usage_data)

                raise ThordataNetworkError(
                    f"Unexpected usage statistics response: {type(data).__name__}",
                    original_error=None,
                )

        except asyncio.TimeoutError as e:
            raise ThordataTimeoutError(
                f"Usage statistics timed out: {e}", original_error=e
            ) from e
        except aiohttp.ClientError as e:
            raise ThordataNetworkError(
                f"Usage statistics failed: {e}", original_error=e
            ) from e

    async def get_residential_balance(self) -> Dict[str, Any]:
        """
        Get residential proxy balance.

        Uses public_token/public_key.
        """
        session = self._get_session()
        headers = self._build_gateway_headers()

        logger.info("Async getting residential proxy balance")

        try:
            async with session.post(
                f"{self._gateway_base_url}/getFlowBalance",
                headers=headers,
                data={},
                timeout=self._api_timeout,
            ) as response:
                response.raise_for_status()
                data = await response.json()

                code = data.get("code")
                if code != 200:
                    msg = extract_error_message(data)
                    raise_for_code(
                        f"Get balance failed: {msg}", code=code, payload=data
                    )

                return data.get("data", {})

        except asyncio.TimeoutError as e:
            raise ThordataTimeoutError(
                f"Get balance timed out: {e}", original_error=e
            ) from e
        except aiohttp.ClientError as e:
            raise ThordataNetworkError(
                f"Get balance failed: {e}", original_error=e
            ) from e

    async def get_residential_usage(
        self,
        start_time: Union[str, int],
        end_time: Union[str, int],
    ) -> Dict[str, Any]:
        """
        Get residential proxy usage records.

        Uses public_token/public_key.
        """
        session = self._get_session()
        headers = self._build_gateway_headers()
        payload = {"start_time": str(start_time), "end_time": str(end_time)}

        logger.info(f"Async getting residential usage: {start_time} to {end_time}")

        try:
            async with session.post(
                f"{self._gateway_base_url}/usageRecord",
                headers=headers,
                data=payload,
                timeout=self._api_timeout,
            ) as response:
                response.raise_for_status()
                data = await response.json()

                code = data.get("code")
                if code != 200:
                    msg = extract_error_message(data)
                    raise_for_code(f"Get usage failed: {msg}", code=code, payload=data)

                return data.get("data", {})

        except asyncio.TimeoutError as e:
            raise ThordataTimeoutError(
                f"Get usage timed out: {e}", original_error=e
            ) from e
        except aiohttp.ClientError as e:
            raise ThordataNetworkError(
                f"Get usage failed: {e}", original_error=e
            ) from e

    async def list_proxy_users(
        self, proxy_type: Union[ProxyType, int] = ProxyType.RESIDENTIAL
    ) -> ProxyUserList:
        """List all proxy users (sub-accounts)."""

        self._require_public_credentials()
        session = self._get_session()

        params = {
            "token": self.public_token,
            "key": self.public_key,
            "proxy_type": str(
                int(proxy_type) if isinstance(proxy_type, ProxyType) else proxy_type
            ),
        }

        logger.info(f"Async listing proxy users: type={params['proxy_type']}")

        try:
            async with session.get(
                f"{self._proxy_users_url}/user-list",
                params=params,
                timeout=self._api_timeout,
            ) as response:
                response.raise_for_status()
                data = await response.json()

                if isinstance(data, dict):
                    code = data.get("code")
                    if code is not None and code != 200:
                        msg = extract_error_message(data)
                        raise_for_code(
                            f"List proxy users error: {msg}", code=code, payload=data
                        )

                    user_data = data.get("data", data)
                    return ProxyUserList.from_dict(user_data)

                raise ThordataNetworkError(
                    f"Unexpected proxy users response: {type(data).__name__}",
                    original_error=None,
                )

        except asyncio.TimeoutError as e:
            raise ThordataTimeoutError(
                f"List users timed out: {e}", original_error=e
            ) from e
        except aiohttp.ClientError as e:
            raise ThordataNetworkError(
                f"List users failed: {e}", original_error=e
            ) from e

    async def create_proxy_user(
        self,
        username: str,
        password: str,
        proxy_type: Union[ProxyType, int] = ProxyType.RESIDENTIAL,
        traffic_limit: int = 0,
        status: bool = True,
    ) -> Dict[str, Any]:
        """Create a new proxy user (sub-account)."""
        self._require_public_credentials()
        session = self._get_session()

        headers = build_public_api_headers(
            self.public_token or "", self.public_key or ""
        )

        payload = {
            "proxy_type": str(
                int(proxy_type) if isinstance(proxy_type, ProxyType) else proxy_type
            ),
            "username": username,
            "password": password,
            "traffic_limit": str(traffic_limit),
            "status": "true" if status else "false",
        }

        logger.info(f"Async creating proxy user: {username}")

        try:
            async with session.post(
                f"{self._proxy_users_url}/create-user",
                data=payload,
                headers=headers,
                timeout=self._api_timeout,
            ) as response:
                response.raise_for_status()
                data = await response.json()

                code = data.get("code")
                if code != 200:
                    msg = extract_error_message(data)
                    raise_for_code(
                        f"Create proxy user failed: {msg}", code=code, payload=data
                    )

                return data.get("data", {})

        except asyncio.TimeoutError as e:
            raise ThordataTimeoutError(
                f"Create user timed out: {e}", original_error=e
            ) from e
        except aiohttp.ClientError as e:
            raise ThordataNetworkError(
                f"Create user failed: {e}", original_error=e
            ) from e

    async def add_whitelist_ip(
        self,
        ip: str,
        proxy_type: Union[ProxyType, int] = ProxyType.RESIDENTIAL,
        status: bool = True,
    ) -> Dict[str, Any]:
        """
        Add an IP to the whitelist for IP authentication.
        """
        self._require_public_credentials()
        session = self._get_session()

        headers = build_public_api_headers(
            self.public_token or "", self.public_key or ""
        )

        proxy_type_int = (
            int(proxy_type) if isinstance(proxy_type, ProxyType) else proxy_type
        )

        payload = {
            "proxy_type": str(proxy_type_int),
            "ip": ip,
            "status": "true" if status else "false",
        }

        logger.info(f"Async adding whitelist IP: {ip}")

        try:
            async with session.post(
                f"{self._whitelist_url}/add-ip",
                data=payload,
                headers=headers,
                timeout=self._api_timeout,
            ) as response:
                response.raise_for_status()
                data = await response.json()

                code = data.get("code")
                if code != 200:
                    msg = extract_error_message(data)
                    raise_for_code(
                        f"Add whitelist IP failed: {msg}", code=code, payload=data
                    )

                return data.get("data", {})

        except asyncio.TimeoutError as e:
            raise ThordataTimeoutError(
                f"Add whitelist timed out: {e}", original_error=e
            ) from e
        except aiohttp.ClientError as e:
            raise ThordataNetworkError(
                f"Add whitelist failed: {e}", original_error=e
            ) from e

    async def list_proxy_servers(
        self,
        proxy_type: int,
    ) -> List[ProxyServer]:
        """
        List ISP or Datacenter proxy servers.
        """

        self._require_public_credentials()
        session = self._get_session()

        params = {
            "token": self.public_token,
            "key": self.public_key,
            "proxy_type": str(proxy_type),
        }

        logger.info(f"Async listing proxy servers: type={proxy_type}")

        try:
            async with session.get(
                self._proxy_list_url,
                params=params,
                timeout=self._api_timeout,
            ) as response:
                response.raise_for_status()
                data = await response.json()

                if isinstance(data, dict):
                    code = data.get("code")
                    if code is not None and code != 200:
                        msg = extract_error_message(data)
                        raise_for_code(
                            f"List proxy servers error: {msg}", code=code, payload=data
                        )

                    server_list = data.get("data", data.get("list", []))
                elif isinstance(data, list):
                    server_list = data
                else:
                    raise ThordataNetworkError(
                        f"Unexpected proxy list response: {type(data).__name__}",
                        original_error=None,
                    )

                return [ProxyServer.from_dict(s) for s in server_list]

        except asyncio.TimeoutError as e:
            raise ThordataTimeoutError(
                f"List servers timed out: {e}", original_error=e
            ) from e
        except aiohttp.ClientError as e:
            raise ThordataNetworkError(
                f"List servers failed: {e}", original_error=e
            ) from e

    async def get_isp_regions(self) -> List[Dict[str, Any]]:
        """
        Get available ISP proxy regions.

        Uses public_token/public_key.
        """
        session = self._get_session()
        headers = self._build_gateway_headers()

        logger.info("Async getting ISP regions")

        try:
            async with session.post(
                f"{self._gateway_base_url}/getRegionIsp",
                headers=headers,
                data={},
                timeout=self._api_timeout,
            ) as response:
                response.raise_for_status()
                data = await response.json()

                code = data.get("code")
                if code != 200:
                    msg = extract_error_message(data)
                    raise_for_code(
                        f"Get ISP regions failed: {msg}", code=code, payload=data
                    )

                return data.get("data", [])

        except asyncio.TimeoutError as e:
            raise ThordataTimeoutError(
                f"Get ISP regions timed out: {e}", original_error=e
            ) from e
        except aiohttp.ClientError as e:
            raise ThordataNetworkError(
                f"Get ISP regions failed: {e}", original_error=e
            ) from e

    async def list_isp_proxies(self) -> List[Dict[str, Any]]:
        """
        List ISP proxies.

        Uses public_token/public_key.
        """
        session = self._get_session()
        headers = self._build_gateway_headers()

        logger.info("Async listing ISP proxies")

        try:
            async with session.post(
                f"{self._gateway_base_url}/queryListIsp",
                headers=headers,
                data={},
                timeout=self._api_timeout,
            ) as response:
                response.raise_for_status()
                data = await response.json()

                code = data.get("code")
                if code != 200:
                    msg = extract_error_message(data)
                    raise_for_code(
                        f"List ISP proxies failed: {msg}", code=code, payload=data
                    )

                return data.get("data", [])

        except asyncio.TimeoutError as e:
            raise ThordataTimeoutError(
                f"List ISP proxies timed out: {e}", original_error=e
            ) from e
        except aiohttp.ClientError as e:
            raise ThordataNetworkError(
                f"List ISP proxies failed: {e}", original_error=e
            ) from e

    async def get_wallet_balance(self) -> Dict[str, Any]:
        """
        Get wallet balance for ISP proxies.

        Uses public_token/public_key.
        """
        session = self._get_session()
        headers = self._build_gateway_headers()

        logger.info("Async getting wallet balance")

        try:
            async with session.post(
                f"{self._gateway_base_url}/getBalance",
                headers=headers,
                data={},
                timeout=self._api_timeout,
            ) as response:
                response.raise_for_status()
                data = await response.json()

                code = data.get("code")
                if code != 200:
                    msg = extract_error_message(data)
                    raise_for_code(
                        f"Get wallet balance failed: {msg}", code=code, payload=data
                    )

                return data.get("data", {})

        except asyncio.TimeoutError as e:
            raise ThordataTimeoutError(
                f"Get wallet balance timed out: {e}", original_error=e
            ) from e
        except aiohttp.ClientError as e:
            raise ThordataNetworkError(
                f"Get wallet balance failed: {e}", original_error=e
            ) from e

    async def get_proxy_expiration(
        self,
        ips: Union[str, List[str]],
        proxy_type: int,
    ) -> Dict[str, Any]:
        """
        Get expiration time for specific proxy IPs.
        """
        self._require_public_credentials()
        session = self._get_session()

        if isinstance(ips, list):
            ips = ",".join(ips)

        params = {
            "token": self.public_token,
            "key": self.public_key,
            "proxy_type": str(proxy_type),
            "ips": ips,
        }

        logger.info(f"Async getting proxy expiration: {ips}")

        try:
            async with session.get(
                self._proxy_expiration_url,
                params=params,
                timeout=self._api_timeout,
            ) as response:
                response.raise_for_status()
                data = await response.json()

                if isinstance(data, dict):
                    code = data.get("code")
                    if code is not None and code != 200:
                        msg = extract_error_message(data)
                        raise_for_code(
                            f"Get expiration error: {msg}", code=code, payload=data
                        )

                    return data.get("data", data)

                return data

        except asyncio.TimeoutError as e:
            raise ThordataTimeoutError(
                f"Get expiration timed out: {e}", original_error=e
            ) from e
        except aiohttp.ClientError as e:
            raise ThordataNetworkError(
                f"Get expiration failed: {e}", original_error=e
            ) from e

    # =========================================================================
    # Location API Methods
    # =========================================================================

    async def list_countries(
        self, proxy_type: Union[ProxyType, int] = ProxyType.RESIDENTIAL
    ) -> List[Dict[str, Any]]:
        """List supported countries."""
        return await self._get_locations(
            "countries",
            proxy_type=(
                int(proxy_type) if isinstance(proxy_type, ProxyType) else proxy_type
            ),
        )

    async def list_states(
        self,
        country_code: str,
        proxy_type: Union[ProxyType, int] = ProxyType.RESIDENTIAL,
    ) -> List[Dict[str, Any]]:
        """List supported states for a country."""
        return await self._get_locations(
            "states",
            proxy_type=(
                int(proxy_type) if isinstance(proxy_type, ProxyType) else proxy_type
            ),
            country_code=country_code,
        )

    async def list_cities(
        self,
        country_code: str,
        state_code: Optional[str] = None,
        proxy_type: Union[ProxyType, int] = ProxyType.RESIDENTIAL,
    ) -> List[Dict[str, Any]]:
        """List supported cities."""
        kwargs = {
            "proxy_type": (
                int(proxy_type) if isinstance(proxy_type, ProxyType) else proxy_type
            ),
            "country_code": country_code,
        }
        if state_code:
            kwargs["state_code"] = state_code

        return await self._get_locations("cities", **kwargs)

    async def list_asn(
        self,
        country_code: str,
        proxy_type: Union[ProxyType, int] = ProxyType.RESIDENTIAL,
    ) -> List[Dict[str, Any]]:
        """List supported ASNs."""
        return await self._get_locations(
            "asn",
            proxy_type=(
                int(proxy_type) if isinstance(proxy_type, ProxyType) else proxy_type
            ),
            country_code=country_code,
        )

    async def _get_locations(
        self, endpoint: str, **kwargs: Any
    ) -> List[Dict[str, Any]]:
        """Internal async locations API call."""
        self._require_public_credentials()

        params = {
            "token": self.public_token,
            "key": self.public_key,
        }

        for key, value in kwargs.items():
            params[key] = str(value)

        url = f"{self._locations_base_url}/{endpoint}"

        logger.debug(f"Async Locations API: {url}")

        # Create temporary session for this request (no proxy needed)
        async with aiohttp.ClientSession(trust_env=True) as temp_session:
            async with temp_session.get(url, params=params) as response:
                response.raise_for_status()
                data = await response.json()

                if isinstance(data, dict):
                    code = data.get("code")
                    if code is not None and code != 200:
                        msg = data.get("msg", "")
                        raise RuntimeError(
                            f"Locations API error ({endpoint}): code={code}, msg={msg}"
                        )
                    return data.get("data") or []

                if isinstance(data, list):
                    return data

                return []

    # =========================================================================
    # Helper Methods
    # =========================================================================

    def _require_public_credentials(self) -> None:
        """Ensure public API credentials are available."""
        if not self.public_token or not self.public_key:
            raise ThordataConfigError(
                "public_token and public_key are required for this operation. "
                "Please provide them when initializing AsyncThordataClient."
            )

    def _get_proxy_endpoint_overrides(
        self, product: ProxyProduct
    ) -> tuple[Optional[str], Optional[int], str]:
        prefix = product.value.upper()

        host = os.getenv(f"THORDATA_{prefix}_PROXY_HOST") or os.getenv(
            "THORDATA_PROXY_HOST"
        )
        port_raw = os.getenv(f"THORDATA_{prefix}_PROXY_PORT") or os.getenv(
            "THORDATA_PROXY_PORT"
        )
        protocol = (
            os.getenv(f"THORDATA_{prefix}_PROXY_PROTOCOL")
            or os.getenv("THORDATA_PROXY_PROTOCOL")
            or "http"
        )

        port: Optional[int] = None
        if port_raw:
            try:
                port = int(port_raw)
            except ValueError:
                port = None

        return host or None, port, protocol

    def _get_default_proxy_config_from_env(self) -> Optional[ProxyConfig]:
        u = os.getenv("THORDATA_RESIDENTIAL_USERNAME")
        p = os.getenv("THORDATA_RESIDENTIAL_PASSWORD")
        if u and p:
            host, port, protocol = self._get_proxy_endpoint_overrides(
                ProxyProduct.RESIDENTIAL
            )
            return ProxyConfig(
                username=u,
                password=p,
                product=ProxyProduct.RESIDENTIAL,
                host=host,
                port=port,
                protocol=protocol,
            )

        u = os.getenv("THORDATA_DATACENTER_USERNAME")
        p = os.getenv("THORDATA_DATACENTER_PASSWORD")
        if u and p:
            host, port, protocol = self._get_proxy_endpoint_overrides(
                ProxyProduct.DATACENTER
            )
            return ProxyConfig(
                username=u,
                password=p,
                product=ProxyProduct.DATACENTER,
                host=host,
                port=port,
                protocol=protocol,
            )

        u = os.getenv("THORDATA_MOBILE_USERNAME")
        p = os.getenv("THORDATA_MOBILE_PASSWORD")
        if u and p:
            host, port, protocol = self._get_proxy_endpoint_overrides(
                ProxyProduct.MOBILE
            )
            return ProxyConfig(
                username=u,
                password=p,
                product=ProxyProduct.MOBILE,
                host=host,
                port=port,
                protocol=protocol,
            )

        return None

    def _build_gateway_headers(self) -> Dict[str, str]:
        """
        Headers for gateway-style endpoints.

        Per our SDK rule: ONLY public_token/public_key exist.
        """
        self._require_public_credentials()
        return build_public_api_headers(self.public_token or "", self.public_key or "")


========================================================================
FILE: src\thordata\client.py
SIZE: 66506
TRUNCATED: no
========================================================================
"""
Synchronous client for the Thordata API.

This module provides the main ThordataClient class for interacting with
Thordata's proxy network, SERP API, Universal Scraping API, and Web Scraper API.

Example:
    >>> from thordata import ThordataClient
    >>>
    >>> client = ThordataClient(
    ...     scraper_token="your_token",
    ...     public_token="your_public_token",
    ...     public_key="your_public_key"
    ... )
    >>>
    >>> # Use the proxy network
    >>> response = client.get("https://httpbin.org/ip")
    >>> print(response.json())
    >>>
    >>> # Search with SERP API
    >>> results = client.serp_search("python tutorial", engine="google")
"""

from __future__ import annotations

import logging
import os
import ssl
from datetime import date
from typing import Any, Dict, List, Optional, Union
from urllib.parse import urlencode

import requests
import urllib3

from . import __version__ as _sdk_version
from ._utils import (
    build_auth_headers,
    build_builder_headers,
    build_public_api_headers,
    build_user_agent,
    decode_base64_image,
    extract_error_message,
    parse_json_response,
)
from .enums import Engine, ProxyType
from .exceptions import (
    ThordataConfigError,
    ThordataNetworkError,
    ThordataTimeoutError,
    raise_for_code,
)
from .models import (
    CommonSettings,
    ProxyConfig,
    ProxyProduct,
    ProxyServer,
    ProxyUserList,
    ScraperTaskConfig,
    SerpRequest,
    UniversalScrapeRequest,
    UsageStatistics,
    VideoTaskConfig,
    WhitelistProxyConfig,
)
from .retry import RetryConfig, with_retry

logger = logging.getLogger(__name__)


class ThordataClient:
    """
    The official synchronous Python client for Thordata.

    This client handles authentication and communication with:
    - Proxy Network (Residential/Datacenter/Mobile/ISP via HTTP/HTTPS)
    - SERP API (Real-time Search Engine Results)
    - Universal Scraping API (Web Unlocker - Single Page Rendering)
    - Web Scraper API (Async Task Management)

    Args:
        scraper_token: The API token from your Dashboard.
        public_token: The public API token (for task status, locations).
        public_key: The public API key.
        proxy_host: Custom proxy gateway host (optional).
        proxy_port: Custom proxy gateway port (optional).
        timeout: Default request timeout in seconds (default: 30).
        retry_config: Configuration for automatic retries (optional).

    Example:
        >>> client = ThordataClient(
        ...     scraper_token="your_scraper_token",
        ...     public_token="your_public_token",
        ...     public_key="your_public_key"
        ... )
    """

    # API Endpoints
    BASE_URL = "https://scraperapi.thordata.com"
    UNIVERSAL_URL = "https://universalapi.thordata.com"
    API_URL = "https://openapi.thordata.com/api/web-scraper-api"
    LOCATIONS_URL = "https://openapi.thordata.com/api/locations"

    def __init__(
        self,
        scraper_token: str,
        public_token: Optional[str] = None,
        public_key: Optional[str] = None,
        proxy_host: str = "pr.thordata.net",
        proxy_port: int = 9999,
        timeout: int = 30,
        api_timeout: int = 60,
        retry_config: Optional[RetryConfig] = None,
        auth_mode: str = "bearer",
        scraperapi_base_url: Optional[str] = None,
        universalapi_base_url: Optional[str] = None,
        web_scraper_api_base_url: Optional[str] = None,
        locations_base_url: Optional[str] = None,
    ) -> None:
        """Initialize the Thordata Client."""
        if not scraper_token:
            raise ThordataConfigError("scraper_token is required")

        # Core credentials
        self.scraper_token = scraper_token
        self.public_token = public_token
        self.public_key = public_key

        # Proxy configuration
        self._proxy_host = proxy_host
        self._proxy_port = proxy_port

        # Timeout configuration
        self._default_timeout = timeout
        self._api_timeout = api_timeout

        # Retry configuration
        self._retry_config = retry_config or RetryConfig()

        # Authentication mode (for scraping APIs)
        self._auth_mode = auth_mode.lower()
        if self._auth_mode not in ("bearer", "header_token"):
            raise ThordataConfigError(
                f"Invalid auth_mode: {auth_mode}. Must be 'bearer' or 'header_token'."
            )

        # NOTE:
        # - _proxy_session: used for proxy network traffic to target sites
        # - _api_session: used for Thordata APIs (SERP/Universal/Tasks/Locations)
        #
        # We intentionally do NOT set session-level proxies for _api_session,
        # so developers can rely on system proxy settings (e.g., Clash) via env vars.
        self._proxy_session = requests.Session()
        self._proxy_session.trust_env = False

        self._api_session = requests.Session()
        self._api_session.trust_env = True
        self._api_session.headers.update(
            {"User-Agent": build_user_agent(_sdk_version, "requests")}
        )

        # Base URLs (allow override via args or env vars for testing and custom routing)
        scraperapi_base = (
            scraperapi_base_url
            or os.getenv("THORDATA_SCRAPERAPI_BASE_URL")
            or self.BASE_URL
        ).rstrip("/")

        universalapi_base = (
            universalapi_base_url
            or os.getenv("THORDATA_UNIVERSALAPI_BASE_URL")
            or self.UNIVERSAL_URL
        ).rstrip("/")

        web_scraper_api_base = (
            web_scraper_api_base_url
            or os.getenv("THORDATA_WEB_SCRAPER_API_BASE_URL")
            or self.API_URL
        ).rstrip("/")

        locations_base = (
            locations_base_url
            or os.getenv("THORDATA_LOCATIONS_BASE_URL")
            or self.LOCATIONS_URL
        ).rstrip("/")

        # These URLs exist in your codebase; keep them for now (even if your org later migrates fully to openapi)
        gateway_base = os.getenv(
            "THORDATA_GATEWAY_BASE_URL", "https://api.thordata.com/api/gateway"
        )
        child_base = os.getenv(
            "THORDATA_CHILD_BASE_URL", "https://api.thordata.com/api/child"
        )
        self._gateway_base_url = gateway_base
        self._child_base_url = child_base

        self._serp_url = f"{scraperapi_base}/request"
        self._builder_url = f"{scraperapi_base}/builder"
        self._video_builder_url = f"{scraperapi_base}/video_builder"
        self._universal_url = f"{universalapi_base}/request"

        self._status_url = f"{web_scraper_api_base}/tasks-status"
        self._download_url = f"{web_scraper_api_base}/tasks-download"
        self._list_url = f"{web_scraper_api_base}/tasks-list"

        self._locations_base_url = locations_base

        # These 2 lines keep your existing behavior (derive account endpoints from locations_base)
        self._usage_stats_url = (
            f"{locations_base.replace('/locations', '')}/account/usage-statistics"
        )
        self._proxy_users_url = (
            f"{locations_base.replace('/locations', '')}/proxy-users"
        )

        whitelist_base = os.getenv(
            "THORDATA_WHITELIST_BASE_URL", "https://api.thordata.com/api"
        )
        self._whitelist_url = f"{whitelist_base}/whitelisted-ips"

        proxy_api_base = os.getenv(
            "THORDATA_PROXY_API_BASE_URL", "https://api.thordata.com/api"
        )
        self._proxy_list_url = f"{proxy_api_base}/proxy/proxy-list"
        self._proxy_expiration_url = f"{proxy_api_base}/proxy/expiration-time"

    # =========================================================================
    # Proxy Network Methods (Pure proxy network request functions)
    # =========================================================================
    def get(
        self,
        url: str,
        *,
        proxy_config: Optional[ProxyConfig] = None,
        timeout: Optional[int] = None,
        **kwargs: Any,
    ) -> requests.Response:
        """
        Send a GET request through the Thordata Proxy Network.

        Args:
            url: The target URL.
            proxy_config: Custom proxy configuration for geo-targeting/sessions.
            timeout: Request timeout in seconds.
            **kwargs: Additional arguments to pass to requests.get().

        Returns:
            The response object.

        Example:
            >>> # Basic request
            >>> response = client.get("https://httpbin.org/ip")
            >>>
            >>> # With geo-targeting
            >>> from thordata.models import ProxyConfig
            >>> config = ProxyConfig(
            ...     username="myuser",
            ...     password="mypass",
            ...     country="us",
            ...     city="seattle"
            ... )
            >>> response = client.get("https://httpbin.org/ip", proxy_config=config)
        """
        logger.debug(f"Proxy GET request: {url}")

        timeout = timeout or self._default_timeout

        if proxy_config is None:
            proxy_config = self._get_default_proxy_config_from_env()

        if proxy_config is None:
            raise ThordataConfigError(
                "Proxy credentials are missing. "
                "Pass proxy_config=ProxyConfig(username=..., password=..., product=...) "
                "or set THORDATA_RESIDENTIAL_USERNAME/THORDATA_RESIDENTIAL_PASSWORD (or DATACENTER/MOBILE)."
            )

        kwargs["proxies"] = proxy_config.to_proxies_dict()

        @with_retry(self._retry_config)
        def _do() -> requests.Response:
            return self._proxy_request_with_proxy_manager(
                "GET",
                url,
                proxy_config=proxy_config,
                timeout=timeout,
                headers=kwargs.pop("headers", None),
                params=kwargs.pop("params", None),
            )

        try:
            return _do()
        except requests.Timeout as e:
            raise ThordataTimeoutError(
                f"Request timed out: {e}", original_error=e
            ) from e
        except Exception as e:
            raise ThordataNetworkError(f"Request failed: {e}", original_error=e) from e

    def post(
        self,
        url: str,
        *,
        proxy_config: Optional[ProxyConfig] = None,
        timeout: Optional[int] = None,
        **kwargs: Any,
    ) -> requests.Response:
        """
        Send a POST request through the Thordata Proxy Network.

        Args:
            url: The target URL.
            proxy_config: Custom proxy configuration.
            timeout: Request timeout in seconds.
            **kwargs: Additional arguments to pass to requests.post().

        Returns:
            The response object.
        """
        logger.debug(f"Proxy POST request: {url}")

        timeout = timeout or self._default_timeout

        if proxy_config is None:
            proxy_config = self._get_default_proxy_config_from_env()

        if proxy_config is None:
            raise ThordataConfigError(
                "Proxy credentials are missing. "
                "Pass proxy_config=ProxyConfig(username=..., password=..., product=...) "
                "or set THORDATA_RESIDENTIAL_USERNAME/THORDATA_RESIDENTIAL_PASSWORD (or DATACENTER/MOBILE)."
            )

        kwargs["proxies"] = proxy_config.to_proxies_dict()

        @with_retry(self._retry_config)
        def _do() -> requests.Response:
            return self._proxy_request_with_proxy_manager(
                "POST",
                url,
                proxy_config=proxy_config,
                timeout=timeout,
                headers=kwargs.pop("headers", None),
                params=kwargs.pop("params", None),
                data=kwargs.pop("data", None),
            )

        try:
            return _do()
        except requests.Timeout as e:
            raise ThordataTimeoutError(
                f"Request timed out: {e}", original_error=e
            ) from e
        except Exception as e:
            raise ThordataNetworkError(f"Request failed: {e}", original_error=e) from e

    def build_proxy_url(
        self,
        username: str,  # Required
        password: str,  # Required
        *,
        country: Optional[str] = None,
        state: Optional[str] = None,
        city: Optional[str] = None,
        session_id: Optional[str] = None,
        session_duration: Optional[int] = None,
        product: Union[ProxyProduct, str] = ProxyProduct.RESIDENTIAL,
    ) -> str:
        """
        Build a proxy URL with custom targeting options.

        This is a convenience method for creating proxy URLs without
        manually constructing a ProxyConfig.

        Args:
            country: Target country code (e.g., 'us', 'gb').
            state: Target state (e.g., 'california').
            city: Target city (e.g., 'seattle').
            session_id: Session ID for sticky sessions.
            session_duration: Session duration in minutes (1-90).
            product: Proxy product type.

        Returns:
            The proxy URL string.

        Example:
            >>> url = client.build_proxy_url(country="us", city="seattle")
            >>> proxies = {"http": url, "https": url}
            >>> requests.get("https://example.com", proxies=proxies)
        """
        config = ProxyConfig(
            username=username,
            password=password,
            host=self._proxy_host,
            port=self._proxy_port,
            product=product,
            country=country,
            state=state,
            city=city,
            session_id=session_id,
            session_duration=session_duration,
        )
        return config.build_proxy_url()

    # =========================================================================
    # Internal API Request Retry Helper (For all API calls)
    # =========================================================================
    def _api_request_with_retry(
        self,
        method: str,
        url: str,
        *,
        data: Optional[Dict[str, Any]] = None,
        headers: Optional[Dict[str, str]] = None,
        params: Optional[Dict[str, Any]] = None,
    ) -> requests.Response:
        """Make an API request with automatic retry on transient failures."""

        @with_retry(self._retry_config)
        def _do_request() -> requests.Response:
            return self._api_session.request(
                method,
                url,
                data=data,
                headers=headers,
                params=params,
                timeout=self._api_timeout,
            )

        try:
            return _do_request()
        except requests.Timeout as e:
            raise ThordataTimeoutError(
                f"API request timed out: {e}", original_error=e
            ) from e
        except requests.RequestException as e:
            raise ThordataNetworkError(
                f"API request failed: {e}", original_error=e
            ) from e

    # =========================================================================
    # SERP API Methods (Search Engine Results Page functions)
    # =========================================================================
    def serp_search(
        self,
        query: str,
        *,
        engine: Union[Engine, str] = Engine.GOOGLE,
        num: int = 10,
        country: Optional[str] = None,
        language: Optional[str] = None,
        search_type: Optional[str] = None,
        device: Optional[str] = None,
        render_js: Optional[bool] = None,
        no_cache: Optional[bool] = None,
        output_format: str = "json",
        **kwargs: Any,
    ) -> Dict[str, Any]:
        """
        Execute a real-time SERP (Search Engine Results Page) search.

        Args:
            query: The search keywords.
            engine: Search engine (google, bing, yandex, duckduckgo, baidu).
            num: Number of results to retrieve (default: 10).
            country: Country code for localized results (e.g., 'us').
            language: Language code for interface (e.g., 'en').
            search_type: Type of search (images, news, shopping, videos, etc.).
            device: Device type ('desktop', 'mobile', 'tablet').
            render_js: Enable JavaScript rendering in SERP (render_js=True).
            no_cache: Disable internal caching (no_cache=True).
            output_format: 'json' to return parsed JSON (default),
                           'html' to return HTML wrapped in {'html': ...}.
            **kwargs: Additional engine-specific parameters.

        Returns:
            Dict[str, Any]: Parsed JSON results or a dict with 'html' key.

        Example:
            >>> # Basic search
            >>> results = client.serp_search("python tutorial")
            >>>
            >>> # With options
            >>> results = client.serp_search(
            ...     "laptop reviews",
            ...     engine="google",
            ...     num=20,
            ...     country="us",
            ...     search_type="shopping",
            ...     device="mobile",
            ...     render_js=True,
            ...     no_cache=True,
            ... )
        """
        # Normalize engine
        engine_str = engine.value if isinstance(engine, Engine) else engine.lower()

        # Build request using model
        request = SerpRequest(
            query=query,
            engine=engine_str,
            num=num,
            country=country,
            language=language,
            search_type=search_type,
            device=device,
            render_js=render_js,
            no_cache=no_cache,
            output_format=output_format,
            extra_params=kwargs,
        )

        payload = request.to_payload()
        headers = build_auth_headers(self.scraper_token, mode=self._auth_mode)

        logger.info(
            f"SERP Search: {engine_str} - {query[:50]}{'...' if len(query) > 50 else ''}"
        )

        try:
            response = self._api_request_with_retry(
                "POST",
                self._serp_url,
                data=payload,
                headers=headers,
            )
            response.raise_for_status()

            # JSON mode (default)
            if output_format.lower() == "json":
                data = response.json()

                if isinstance(data, dict):
                    code = data.get("code")
                    if code is not None and code != 200:
                        msg = extract_error_message(data)
                        raise_for_code(
                            f"SERP API Error: {msg}",
                            code=code,
                            payload=data,
                        )

                return parse_json_response(data)

            # HTML mode: wrap as dict to keep return type stable
            return {"html": response.text}

        except requests.Timeout as e:
            raise ThordataTimeoutError(
                f"SERP request timed out: {e}",
                original_error=e,
            ) from e
        except requests.RequestException as e:
            raise ThordataNetworkError(
                f"SERP request failed: {e}",
                original_error=e,
            ) from e

    def serp_search_advanced(self, request: SerpRequest) -> Dict[str, Any]:
        """
        Execute a SERP search using a SerpRequest object.

        This method provides full control over all search parameters.

        Args:
            request: A SerpRequest object with all parameters configured.

        Returns:
            Dict[str, Any]: Parsed JSON results or dict with 'html' key.

        Example:
            >>> from thordata.models import SerpRequest
            >>> request = SerpRequest(
            ...     query="python programming",
            ...     engine="google",
            ...     num=50,
            ...     country="us",
            ...     language="en",
            ...     search_type="news",
            ...     time_filter="week",
            ...     safe_search=True
            ... )
            >>> results = client.serp_search_advanced(request)
        """
        payload = request.to_payload()
        headers = build_auth_headers(self.scraper_token, mode=self._auth_mode)

        logger.info(
            f"SERP Advanced Search: {request.engine} - {request.query[:50]}{'...' if len(request.query) > 50 else ''}"
        )

        try:
            response = self._api_request_with_retry(
                "POST",
                self._serp_url,
                data=payload,
                headers=headers,
            )
            response.raise_for_status()

            if request.output_format.lower() == "json":
                data = response.json()

                if isinstance(data, dict):
                    code = data.get("code")
                    if code is not None and code != 200:
                        msg = extract_error_message(data)
                        raise_for_code(
                            f"SERP API Error: {msg}",
                            code=code,
                            payload=data,
                        )

                return parse_json_response(data)

            return {"html": response.text}

        except requests.Timeout as e:
            raise ThordataTimeoutError(
                f"SERP request timed out: {e}",
                original_error=e,
            ) from e
        except requests.RequestException as e:
            raise ThordataNetworkError(
                f"SERP request failed: {e}",
                original_error=e,
            ) from e

    # =========================================================================
    # Universal Scraping API Methods (Web Unlocker functions)
    # =========================================================================
    def universal_scrape(
        self,
        url: str,
        *,
        js_render: bool = False,
        output_format: str = "html",
        country: Optional[str] = None,
        block_resources: Optional[str] = None,
        wait: Optional[int] = None,
        wait_for: Optional[str] = None,
        **kwargs: Any,
    ) -> Union[str, bytes]:
        """
        Scrape a URL using the Universal Scraping API (Web Unlocker).

        Automatically bypasses Cloudflare, CAPTCHAs, and antibot systems.

        Args:
            url: Target URL.
            js_render: Enable JavaScript rendering (headless browser).
            output_format: "html" or "png" (screenshot).
            country: Geo-targeting country code.
            block_resources: Resources to block (e.g., 'script,image').
            wait: Wait time in milliseconds after page load.
            wait_for: CSS selector to wait for.
            **kwargs: Additional parameters.

        Returns:
            HTML string or PNG bytes depending on output_format.

        Example:
            >>> # Get HTML
            >>> html = client.universal_scrape("https://example.com", js_render=True)
            >>>
            >>> # Get screenshot
            >>> png = client.universal_scrape(
            ...     "https://example.com",
            ...     js_render=True,
            ...     output_format="png"
            ... )
            >>> with open("screenshot.png", "wb") as f:
            ...     f.write(png)
        """
        request = UniversalScrapeRequest(
            url=url,
            js_render=js_render,
            output_format=output_format,
            country=country,
            block_resources=block_resources,
            wait=wait,
            wait_for=wait_for,
            extra_params=kwargs,
        )

        return self.universal_scrape_advanced(request)

    def universal_scrape_advanced(
        self, request: UniversalScrapeRequest
    ) -> Union[str, bytes]:
        """
        Scrape using a UniversalScrapeRequest object for full control.

        Args:
            request: A UniversalScrapeRequest with all parameters.

        Returns:
            HTML string or PNG bytes.
        """
        payload = request.to_payload()
        headers = build_auth_headers(self.scraper_token, mode=self._auth_mode)

        logger.info(
            f"Universal Scrape: {request.url} (format: {request.output_format})"
        )

        try:
            response = self._api_request_with_retry(
                "POST",
                self._universal_url,
                data=payload,
                headers=headers,
            )
            response.raise_for_status()

            return self._process_universal_response(response, request.output_format)

        except requests.Timeout as e:
            raise ThordataTimeoutError(
                f"Universal scrape timed out: {e}", original_error=e
            ) from e
        except requests.RequestException as e:
            raise ThordataNetworkError(
                f"Universal scrape failed: {e}", original_error=e
            ) from e

    def _process_universal_response(
        self, response: requests.Response, output_format: str
    ) -> Union[str, bytes]:
        """Process the response from Universal API."""
        # Try to parse as JSON
        try:
            resp_json = response.json()
        except ValueError:
            # Raw content returned
            if output_format.lower() == "png":
                return response.content
            return response.text

        # Check for API-level errors
        if isinstance(resp_json, dict):
            code = resp_json.get("code")
            if code is not None and code != 200:
                msg = extract_error_message(resp_json)
                raise_for_code(
                    f"Universal API Error: {msg}", code=code, payload=resp_json
                )

        # Extract HTML
        if "html" in resp_json:
            return resp_json["html"]

        # Extract PNG
        if "png" in resp_json:
            return decode_base64_image(resp_json["png"])

        # Fallback
        return str(resp_json)

    # =========================================================================
    # Web Scraper API Methods (Only async task management functions)
    # =========================================================================
    def create_scraper_task(
        self,
        file_name: str,
        spider_id: str,
        spider_name: str,
        parameters: Dict[str, Any],
        universal_params: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        Create an asynchronous Web Scraper task.

        Note: Get spider_id and spider_name from the Thordata Dashboard.

        Args:
            file_name: Name for the output file.
            spider_id: Spider identifier from Dashboard.
            spider_name: Spider name (e.g., "youtube.com").
            parameters: Spider-specific parameters.
            universal_params: Global spider settings.

        Returns:
            The created task_id.

        Example:
            >>> task_id = client.create_scraper_task(
            ...     file_name="youtube_data",
            ...     spider_id="youtube_video-post_by-url",
            ...     spider_name="youtube.com",
            ...     parameters={"url": "https://youtube.com/@channel/videos"}
            ... )
        """
        config = ScraperTaskConfig(
            file_name=file_name,
            spider_id=spider_id,
            spider_name=spider_name,
            parameters=parameters,
            universal_params=universal_params,
        )

        return self.create_scraper_task_advanced(config)

    def create_scraper_task_advanced(self, config: ScraperTaskConfig) -> str:
        """
        Create a scraper task using a ScraperTaskConfig object.

        Args:
            config: Task configuration.

        Returns:
            The created task_id.
        """
        self._require_public_credentials()

        payload = config.to_payload()

        # Builder needs 3 headers: token, key, Authorization Bearer
        headers = build_builder_headers(
            self.scraper_token,
            self.public_token or "",
            self.public_key or "",
        )

        logger.info(f"Creating Scraper Task: {config.spider_name}")

        try:
            response = self._api_request_with_retry(
                "POST",
                self._builder_url,
                data=payload,
                headers=headers,
            )
            response.raise_for_status()

            data = response.json()
            code = data.get("code")

            if code != 200:
                msg = extract_error_message(data)
                raise_for_code(f"Task creation failed: {msg}", code=code, payload=data)

            return data["data"]["task_id"]

        except requests.RequestException as e:
            raise ThordataNetworkError(
                f"Task creation failed: {e}", original_error=e
            ) from e

    def create_video_task(
        self,
        file_name: str,
        spider_id: str,
        spider_name: str,
        parameters: Dict[str, Any],
        common_settings: "CommonSettings",
    ) -> str:
        """
        Create a YouTube video/audio download task.

        Uses the /video_builder endpoint.

        Args:
            file_name: Output file name. Supports {{TasksID}}, {{VideoID}}.
            spider_id: Spider identifier (e.g., "youtube_video_by-url").
            spider_name: Spider name (typically "youtube.com").
            parameters: Spider parameters (e.g., {"url": "..."}).
            common_settings: Video/audio settings.

        Returns:
            The created task_id.

        Example:
            >>> from thordata import CommonSettings
            >>> task_id = client.create_video_task(
            ...     file_name="{{VideoID}}",
            ...     spider_id="youtube_video_by-url",
            ...     spider_name="youtube.com",
            ...     parameters={"url": "https://youtube.com/watch?v=xxx"},
            ...     common_settings=CommonSettings(
            ...         resolution="1080p",
            ...         is_subtitles="true"
            ...     )
            ... )
        """

        config = VideoTaskConfig(
            file_name=file_name,
            spider_id=spider_id,
            spider_name=spider_name,
            parameters=parameters,
            common_settings=common_settings,
        )

        return self.create_video_task_advanced(config)

    def create_video_task_advanced(self, config: VideoTaskConfig) -> str:
        """
        Create a video task using VideoTaskConfig object.

        Args:
            config: Video task configuration.

        Returns:
            The created task_id.
        """

        self._require_public_credentials()

        payload = config.to_payload()
        headers = build_builder_headers(
            self.scraper_token,
            self.public_token or "",
            self.public_key or "",
        )

        logger.info(f"Creating Video Task: {config.spider_name} - {config.spider_id}")

        response = self._api_request_with_retry(
            "POST",
            self._video_builder_url,
            data=payload,
            headers=headers,
        )
        response.raise_for_status()

        data = response.json()
        code = data.get("code")

        if code != 200:
            msg = extract_error_message(data)
            raise_for_code(
                f"Video task creation failed: {msg}", code=code, payload=data
            )

        return data["data"]["task_id"]

    def get_task_status(self, task_id: str) -> str:
        """
        Check the status of an asynchronous scraping task.

        Returns:
            Status string (e.g., "running", "ready", "failed").

        Raises:
            ThordataConfigError: If public credentials are missing.
            ThordataAPIError: If API returns a non-200 code in JSON payload.
            ThordataNetworkError: If network/HTTP request fails.
        """
        self._require_public_credentials()

        headers = build_public_api_headers(
            self.public_token or "", self.public_key or ""
        )
        payload = {"tasks_ids": task_id}

        try:
            response = self._api_request_with_retry(
                "POST",
                self._status_url,
                data=payload,
                headers=headers,
            )
            response.raise_for_status()
            data = response.json()

            if isinstance(data, dict):
                code = data.get("code")
                if code is not None and code != 200:
                    msg = extract_error_message(data)
                    raise_for_code(
                        f"Task status API Error: {msg}",
                        code=code,
                        payload=data,
                    )

                items = data.get("data") or []
                for item in items:
                    if str(item.get("task_id")) == str(task_id):
                        return item.get("status", "unknown")

                return "unknown"

            # Unexpected payload type
            raise ThordataNetworkError(
                f"Unexpected task status response type: {type(data).__name__}",
                original_error=None,
            )

        except requests.Timeout as e:
            raise ThordataTimeoutError(
                f"Status check timed out: {e}", original_error=e
            ) from e
        except requests.RequestException as e:
            raise ThordataNetworkError(
                f"Status check failed: {e}", original_error=e
            ) from e

    def safe_get_task_status(self, task_id: str) -> str:
        """
        Backward-compatible status check.

        Returns:
            Status string, or "error" on any exception.
        """
        try:
            return self.get_task_status(task_id)
        except Exception:
            return "error"

    def get_task_result(self, task_id: str, file_type: str = "json") -> str:
        """
        Get the download URL for a completed task.
        """
        self._require_public_credentials()

        headers = build_public_api_headers(
            self.public_token or "", self.public_key or ""
        )
        payload = {"tasks_id": task_id, "type": file_type}

        logger.info(f"Getting result URL for Task: {task_id}")

        try:
            response = self._api_request_with_retry(
                "POST",
                self._download_url,
                data=payload,
                headers=headers,
            )
            response.raise_for_status()

            data = response.json()
            code = data.get("code")

            if code == 200 and data.get("data"):
                return data["data"]["download"]

            msg = extract_error_message(data)
            raise_for_code(f"Get result failed: {msg}", code=code, payload=data)
            # This line won't be reached, but satisfies mypy
            raise RuntimeError("Unexpected state")

        except requests.RequestException as e:
            raise ThordataNetworkError(
                f"Get result failed: {e}", original_error=e
            ) from e

    def list_tasks(
        self,
        page: int = 1,
        size: int = 20,
    ) -> Dict[str, Any]:
        """
        List all Web Scraper tasks.

        Args:
            page: Page number (starts from 1).
            size: Number of tasks per page.

        Returns:
            Dict containing 'count' and 'list' of tasks.

        Example:
            >>> result = client.list_tasks(page=1, size=10)
            >>> print(f"Total tasks: {result['count']}")
            >>> for task in result['list']:
            ...     print(f"Task {task['task_id']}: {task['status']}")
        """
        self._require_public_credentials()

        headers = build_public_api_headers(
            self.public_token or "", self.public_key or ""
        )
        payload: Dict[str, Any] = {}
        if page:
            payload["page"] = str(page)
        if size:
            payload["size"] = str(size)

        logger.info(f"Listing tasks: page={page}, size={size}")

        response = self._api_request_with_retry(
            "POST",
            self._list_url,
            data=payload,
            headers=headers,
        )
        response.raise_for_status()

        data = response.json()
        code = data.get("code")

        if code != 200:
            msg = extract_error_message(data)
            raise_for_code(f"List tasks failed: {msg}", code=code, payload=data)

        return data.get("data", {"count": 0, "list": []})

    def wait_for_task(
        self,
        task_id: str,
        *,
        poll_interval: float = 5.0,
        max_wait: float = 600.0,
    ) -> str:
        """
        Wait for a task to complete.

        Args:
            task_id: The task ID to wait for.
            poll_interval: Seconds between status checks.
            max_wait: Maximum seconds to wait.

        Returns:
            Final task status.

        Raises:
            TimeoutError: If max_wait is exceeded.

        Example:
            >>> task_id = client.create_scraper_task(...)
            >>> status = client.wait_for_task(task_id, max_wait=300)
            >>> if status in ("ready", "success"):
            ...     url = client.get_task_result(task_id)
        """
        import time

        start = time.monotonic()

        while (time.monotonic() - start) < max_wait:
            status = self.get_task_status(task_id)

            logger.debug(f"Task {task_id} status: {status}")

            terminal_statuses = {
                "ready",
                "success",
                "finished",
                "failed",
                "error",
                "cancelled",
            }

            if status.lower() in terminal_statuses:
                return status

            time.sleep(poll_interval)

        raise TimeoutError(f"Task {task_id} did not complete within {max_wait} seconds")

    # =========================================================================
    # Proxy Account Management Methods (Proxy balance, user, whitelist functions)
    # =========================================================================
    def get_usage_statistics(
        self,
        from_date: Union[str, date],
        to_date: Union[str, date],
    ) -> UsageStatistics:
        """
        Get account usage statistics for a date range.

        Args:
            from_date: Start date (YYYY-MM-DD string or date object).
            to_date: End date (YYYY-MM-DD string or date object).

        Returns:
            UsageStatistics object with traffic data.

        Raises:
            ValueError: If date range exceeds 180 days.

        Example:
            >>> from datetime import date, timedelta
            >>> today = date.today()
            >>> week_ago = today - timedelta(days=7)
            >>> stats = client.get_usage_statistics(week_ago, today)
            >>> print(f"Used: {stats.range_usage_gb():.2f} GB")
            >>> print(f"Balance: {stats.balance_gb():.2f} GB")
        """

        self._require_public_credentials()

        # Convert dates to strings
        if isinstance(from_date, date):
            from_date = from_date.strftime("%Y-%m-%d")
        if isinstance(to_date, date):
            to_date = to_date.strftime("%Y-%m-%d")

        params = {
            "token": self.public_token,
            "key": self.public_key,
            "from_date": from_date,
            "to_date": to_date,
        }

        logger.info(f"Getting usage statistics: {from_date} to {to_date}")

        response = self._api_request_with_retry(
            "GET",
            self._usage_stats_url,
            params=params,
        )
        response.raise_for_status()

        data = response.json()

        if isinstance(data, dict):
            code = data.get("code")
            if code is not None and code != 200:
                msg = extract_error_message(data)
                raise_for_code(
                    f"Usage statistics error: {msg}",
                    code=code,
                    payload=data,
                )

            # Extract data field
            usage_data = data.get("data", data)
            return UsageStatistics.from_dict(usage_data)

        raise ThordataNetworkError(
            f"Unexpected usage statistics response: {type(data).__name__}",
            original_error=None,
        )

    def get_residential_balance(self) -> Dict[str, Any]:
        """
        Get residential proxy balance.

        Uses public_token/public_key (Dashboard -> My account -> API).
        """
        headers = self._build_gateway_headers()

        logger.info("Getting residential proxy balance")

        response = self._api_request_with_retry(
            "POST",
            f"{self._gateway_base_url}/getFlowBalance",
            headers=headers,
            data={},
        )
        response.raise_for_status()

        data = response.json()
        code = data.get("code")

        if code != 200:
            msg = extract_error_message(data)
            raise_for_code(f"Get balance failed: {msg}", code=code, payload=data)

        return data.get("data", {})

    def get_residential_usage(
        self,
        start_time: Union[str, int],
        end_time: Union[str, int],
    ) -> Dict[str, Any]:
        """
        Get residential proxy usage records.

        Uses public_token/public_key (Dashboard -> My account -> API).
        """
        headers = self._build_gateway_headers()
        payload = {"start_time": str(start_time), "end_time": str(end_time)}

        logger.info(f"Getting residential usage: {start_time} to {end_time}")

        response = self._api_request_with_retry(
            "POST",
            f"{self._gateway_base_url}/usageRecord",
            headers=headers,
            data=payload,
        )
        response.raise_for_status()

        data = response.json()
        code = data.get("code")

        if code != 200:
            msg = extract_error_message(data)
            raise_for_code(f"Get usage failed: {msg}", code=code, payload=data)

        return data.get("data", {})

    def list_proxy_users(
        self, proxy_type: Union[ProxyType, int] = ProxyType.RESIDENTIAL
    ) -> ProxyUserList:
        """
        List all proxy users (sub-accounts).

        Args:
            proxy_type: Proxy type (1=Residential, 2=Unlimited).

        Returns:
            ProxyUserList with user details.

        Example:
            >>> users = client.list_proxy_users(proxy_type=ProxyType.RESIDENTIAL)
            >>> print(f"Total users: {users.user_count}")
            >>> for user in users.users:
            ...     print(f"{user.username}: {user.usage_gb():.2f} GB used")
        """

        self._require_public_credentials()

        params = {
            "token": self.public_token,
            "key": self.public_key,
            "proxy_type": str(
                int(proxy_type) if isinstance(proxy_type, ProxyType) else proxy_type
            ),
        }

        logger.info(f"Listing proxy users: type={params['proxy_type']}")

        response = self._api_request_with_retry(
            "GET",
            f"{self._proxy_users_url}/user-list",
            params=params,
        )
        response.raise_for_status()

        data = response.json()

        if isinstance(data, dict):
            code = data.get("code")
            if code is not None and code != 200:
                msg = extract_error_message(data)
                raise_for_code(
                    f"List proxy users error: {msg}", code=code, payload=data
                )

            user_data = data.get("data", data)
            return ProxyUserList.from_dict(user_data)

        raise ThordataNetworkError(
            f"Unexpected proxy users response: {type(data).__name__}",
            original_error=None,
        )

    def create_proxy_user(
        self,
        username: str,
        password: str,
        proxy_type: Union[ProxyType, int] = ProxyType.RESIDENTIAL,
        traffic_limit: int = 0,
        status: bool = True,
    ) -> Dict[str, Any]:
        """
        Create a new proxy user (sub-account).

        Args:
            username: Username for the new user.
            password: Password for the new user.
            proxy_type: Proxy type (1=Residential, 2=Unlimited).
            traffic_limit: Traffic limit in MB (0 = unlimited, min 100).
            status: Enable/disable user (True/False).

        Returns:
            API response data.

        Example:
            >>> result = client.create_proxy_user(
            ...     username="subuser1",
            ...     password="securepass",
            ...     traffic_limit=5120,  # 5GB
            ...     status=True
            ... )
        """
        self._require_public_credentials()

        headers = build_public_api_headers(
            self.public_token or "", self.public_key or ""
        )

        payload = {
            "proxy_type": str(
                int(proxy_type) if isinstance(proxy_type, ProxyType) else proxy_type
            ),
            "username": username,
            "password": password,
            "traffic_limit": str(traffic_limit),
            "status": "true" if status else "false",
        }

        logger.info(f"Creating proxy user: {username}")

        response = self._api_request_with_retry(
            "POST",
            f"{self._proxy_users_url}/create-user",
            data=payload,
            headers=headers,
        )
        response.raise_for_status()

        data = response.json()
        code = data.get("code")

        if code != 200:
            msg = extract_error_message(data)
            raise_for_code(f"Create proxy user failed: {msg}", code=code, payload=data)

        return data.get("data", {})

    def add_whitelist_ip(
        self,
        ip: str,
        proxy_type: Union[ProxyType, int] = ProxyType.RESIDENTIAL,
        status: bool = True,
    ) -> Dict[str, Any]:
        """
        Add an IP to the whitelist for IP authentication.

        Args:
            ip: IP address to whitelist.
            proxy_type: Proxy type (1=Residential, 2=Unlimited, 9=Mobile).
            status: Enable/disable the IP (True/False).

        Returns:
            API response data.

        Example:
            >>> result = client.add_whitelist_ip(
            ...     ip="123.45.67.89",
            ...     proxy_type=ProxyType.RESIDENTIAL,
            ...     status=True
            ... )
        """
        self._require_public_credentials()

        headers = build_public_api_headers(
            self.public_token or "", self.public_key or ""
        )

        # Convert ProxyType to int
        proxy_type_int = (
            int(proxy_type) if isinstance(proxy_type, ProxyType) else proxy_type
        )

        payload = {
            "proxy_type": str(proxy_type_int),
            "ip": ip,
            "status": "true" if status else "false",
        }

        logger.info(f"Adding whitelist IP: {ip}")

        response = self._api_request_with_retry(
            "POST",
            f"{self._whitelist_url}/add-ip",
            data=payload,
            headers=headers,
        )
        response.raise_for_status()

        data = response.json()
        code = data.get("code")

        if code != 200:
            msg = extract_error_message(data)
            raise_for_code(f"Add whitelist IP failed: {msg}", code=code, payload=data)

        return data.get("data", {})

    def list_proxy_servers(
        self,
        proxy_type: int,
    ) -> List[ProxyServer]:
        """
        List ISP or Datacenter proxy servers.

        Args:
            proxy_type: Proxy type (1=ISP, 2=Datacenter).

        Returns:
            List of ProxyServer objects.

        Example:
            >>> servers = client.list_proxy_servers(proxy_type=1)  # ISP proxies
            >>> for server in servers:
            ...     print(f"{server.ip}:{server.port} - expires: {server.expiration_time}")
        """

        self._require_public_credentials()

        params = {
            "token": self.public_token,
            "key": self.public_key,
            "proxy_type": str(proxy_type),
        }

        logger.info(f"Listing proxy servers: type={proxy_type}")

        response = self._api_request_with_retry(
            "GET",
            self._proxy_list_url,
            params=params,
        )
        response.raise_for_status()

        data = response.json()

        if isinstance(data, dict):
            code = data.get("code")
            if code is not None and code != 200:
                msg = extract_error_message(data)
                raise_for_code(
                    f"List proxy servers error: {msg}", code=code, payload=data
                )

            # Extract list from data field
            server_list = data.get("data", data.get("list", []))
        elif isinstance(data, list):
            server_list = data
        else:
            raise ThordataNetworkError(
                f"Unexpected proxy list response: {type(data).__name__}",
                original_error=None,
            )

        return [ProxyServer.from_dict(s) for s in server_list]

    def get_isp_regions(self) -> List[Dict[str, Any]]:
        """
        Get available ISP proxy regions.

        Uses public_token/public_key (Dashboard -> My account -> API).
        """
        headers = self._build_gateway_headers()

        logger.info("Getting ISP regions")

        response = self._api_request_with_retry(
            "POST",
            f"{self._gateway_base_url}/getRegionIsp",
            headers=headers,
            data={},
        )
        response.raise_for_status()

        data = response.json()
        code = data.get("code")

        if code != 200:
            msg = extract_error_message(data)
            raise_for_code(f"Get ISP regions failed: {msg}", code=code, payload=data)

        return data.get("data", [])

    def list_isp_proxies(self) -> List[Dict[str, Any]]:
        """
        List ISP proxies.

        Uses public_token/public_key (Dashboard -> My account -> API).
        """
        headers = self._build_gateway_headers()

        logger.info("Listing ISP proxies")

        response = self._api_request_with_retry(
            "POST",
            f"{self._gateway_base_url}/queryListIsp",
            headers=headers,
            data={},
        )
        response.raise_for_status()

        data = response.json()
        code = data.get("code")

        if code != 200:
            msg = extract_error_message(data)
            raise_for_code(f"List ISP proxies failed: {msg}", code=code, payload=data)

        return data.get("data", [])

    def get_wallet_balance(self) -> Dict[str, Any]:
        """
        Get wallet balance for ISP proxies.

        Uses public_token/public_key (Dashboard -> My account -> API).
        """
        headers = self._build_gateway_headers()

        logger.info("Getting wallet balance")

        response = self._api_request_with_retry(
            "POST",
            f"{self._gateway_base_url}/getBalance",
            headers=headers,
            data={},
        )
        response.raise_for_status()

        data = response.json()
        code = data.get("code")

        if code != 200:
            msg = extract_error_message(data)
            raise_for_code(f"Get wallet balance failed: {msg}", code=code, payload=data)

        return data.get("data", {})

    def get_proxy_expiration(
        self,
        ips: Union[str, List[str]],
        proxy_type: int,
    ) -> Dict[str, Any]:
        """
        Get expiration time for specific proxy IPs.

        Args:
            ips: Single IP or list of IPs to check.
            proxy_type: Proxy type (1=ISP, 2=Datacenter).

        Returns:
            Dict with expiration information.

        Example:
            >>> result = client.get_proxy_expiration("123.45.67.89", proxy_type=1)
            >>> print(result)
        """
        self._require_public_credentials()

        # Convert list to comma-separated string
        if isinstance(ips, list):
            ips = ",".join(ips)

        params = {
            "token": self.public_token,
            "key": self.public_key,
            "proxy_type": str(proxy_type),
            "ips": ips,
        }

        logger.info(f"Getting proxy expiration: {ips}")

        response = self._api_request_with_retry(
            "GET",
            self._proxy_expiration_url,
            params=params,
        )
        response.raise_for_status()

        data = response.json()

        if isinstance(data, dict):
            code = data.get("code")
            if code is not None and code != 200:
                msg = extract_error_message(data)
                raise_for_code(f"Get expiration error: {msg}", code=code, payload=data)

            return data.get("data", data)

        return data

    # =========================================================================
    # Location API Methods (Country/State/City/ASN functions)
    # =========================================================================
    def list_countries(
        self, proxy_type: Union[ProxyType, int] = ProxyType.RESIDENTIAL
    ) -> List[Dict[str, Any]]:
        """
        List supported countries for proxies.

        Args:
            proxy_type: 1 for residential, 2 for unlimited.

        Returns:
            List of country records with 'country_code' and 'country_name'.
        """
        return self._get_locations(
            "countries",
            proxy_type=(
                int(proxy_type) if isinstance(proxy_type, ProxyType) else proxy_type
            ),
        )

    def list_states(
        self,
        country_code: str,
        proxy_type: Union[ProxyType, int] = ProxyType.RESIDENTIAL,
    ) -> List[Dict[str, Any]]:
        """
        List supported states for a country.

        Args:
            country_code: Country code (e.g., 'US').
            proxy_type: Proxy type.

        Returns:
            List of state records.
        """
        return self._get_locations(
            "states",
            proxy_type=(
                int(proxy_type) if isinstance(proxy_type, ProxyType) else proxy_type
            ),
            country_code=country_code,
        )

    def list_cities(
        self,
        country_code: str,
        state_code: Optional[str] = None,
        proxy_type: Union[ProxyType, int] = ProxyType.RESIDENTIAL,
    ) -> List[Dict[str, Any]]:
        """
        List supported cities for a country/state.

        Args:
            country_code: Country code.
            state_code: Optional state code.
            proxy_type: Proxy type.

        Returns:
            List of city records.
        """
        kwargs = {
            "proxy_type": (
                int(proxy_type) if isinstance(proxy_type, ProxyType) else proxy_type
            ),
            "country_code": country_code,
        }
        if state_code:
            kwargs["state_code"] = state_code

        return self._get_locations("cities", **kwargs)

    def list_asn(
        self,
        country_code: str,
        proxy_type: Union[ProxyType, int] = ProxyType.RESIDENTIAL,
    ) -> List[Dict[str, Any]]:
        """
        List supported ASNs for a country.

        Args:
            country_code: Country code.
            proxy_type: Proxy type.

        Returns:
            List of ASN records.
        """
        return self._get_locations(
            "asn",
            proxy_type=(
                int(proxy_type) if isinstance(proxy_type, ProxyType) else proxy_type
            ),
            country_code=country_code,
        )

    def _get_locations(self, endpoint: str, **kwargs: Any) -> List[Dict[str, Any]]:
        """Internal method to call locations API."""
        self._require_public_credentials()

        params = {
            "token": self.public_token,
            "key": self.public_key,
        }

        for key, value in kwargs.items():
            params[key] = str(value)

        url = f"{self._locations_base_url}/{endpoint}"

        logger.debug(f"Locations API request: {url}")

        # Use requests.get directly (no proxy needed for this API)
        response = self._api_request_with_retry(
            "GET",
            url,
            params=params,
        )
        response.raise_for_status()

        data = response.json()

        if isinstance(data, dict):
            code = data.get("code")
            if code is not None and code != 200:
                msg = data.get("msg", "")
                raise RuntimeError(
                    f"Locations API error ({endpoint}): code={code}, msg={msg}"
                )
            return data.get("data") or []

        if isinstance(data, list):
            return data

        return []

    # =========================================================================
    # Helper Methods (Internal utility functions)
    # =========================================================================
    def _require_public_credentials(self) -> None:
        """Ensure public API credentials are available."""
        if not self.public_token or not self.public_key:
            raise ThordataConfigError(
                "public_token and public_key are required for this operation. "
                "Please provide them when initializing ThordataClient."
            )

    def _get_proxy_endpoint_overrides(
        self, product: ProxyProduct
    ) -> tuple[Optional[str], Optional[int], str]:
        """
        Read proxy endpoint overrides from env.

        Priority:
        1) THORDATA_<PRODUCT>_PROXY_HOST/PORT/PROTOCOL
        2) THORDATA_PROXY_HOST/PORT/PROTOCOL
        3) defaults (host/port None => ProxyConfig will use its product defaults)
        """
        prefix = product.value.upper()  # RESIDENTIAL / DATACENTER / MOBILE / ISP

        host = os.getenv(f"THORDATA_{prefix}_PROXY_HOST") or os.getenv(
            "THORDATA_PROXY_HOST"
        )
        port_raw = os.getenv(f"THORDATA_{prefix}_PROXY_PORT") or os.getenv(
            "THORDATA_PROXY_PORT"
        )
        protocol = (
            os.getenv(f"THORDATA_{prefix}_PROXY_PROTOCOL")
            or os.getenv("THORDATA_PROXY_PROTOCOL")
            or "http"
        )

        port: Optional[int] = None
        if port_raw:
            try:
                port = int(port_raw)
            except ValueError:
                port = None

        return host or None, port, protocol

    def _get_default_proxy_config_from_env(self) -> Optional[ProxyConfig]:
        """
        Try to build a default ProxyConfig from env vars.

        Priority order:
        1) Residential
        2) Datacenter
        3) Mobile
        """
        # Residential
        u = os.getenv("THORDATA_RESIDENTIAL_USERNAME")
        p = os.getenv("THORDATA_RESIDENTIAL_PASSWORD")
        if u and p:
            host, port, protocol = self._get_proxy_endpoint_overrides(
                ProxyProduct.RESIDENTIAL
            )
            return ProxyConfig(
                username=u,
                password=p,
                product=ProxyProduct.RESIDENTIAL,
                host=host,
                port=port,
                protocol=protocol,
            )

        # Datacenter
        u = os.getenv("THORDATA_DATACENTER_USERNAME")
        p = os.getenv("THORDATA_DATACENTER_PASSWORD")
        if u and p:
            host, port, protocol = self._get_proxy_endpoint_overrides(
                ProxyProduct.DATACENTER
            )
            return ProxyConfig(
                username=u,
                password=p,
                product=ProxyProduct.DATACENTER,
                host=host,
                port=port,
                protocol=protocol,
            )

        # Mobile
        u = os.getenv("THORDATA_MOBILE_USERNAME")
        p = os.getenv("THORDATA_MOBILE_PASSWORD")
        if u and p:
            host, port, protocol = self._get_proxy_endpoint_overrides(
                ProxyProduct.MOBILE
            )
            return ProxyConfig(
                username=u,
                password=p,
                product=ProxyProduct.MOBILE,
                host=host,
                port=port,
                protocol=protocol,
            )

        return None

    def _build_gateway_headers(self) -> Dict[str, str]:
        """
        Build headers for legacy gateway-style endpoints.

        IMPORTANT:
        - SDK does NOT expose "sign/apiKey" as a separate credential model.
        - Values ALWAYS come from public_token/public_key.
        - Some backend endpoints may still expect header field names "sign" and "apiKey".
        """
        self._require_public_credentials()
        return {
            "sign": self.public_token or "",
            "apiKey": self.public_key or "",
            "Content-Type": "application/x-www-form-urlencoded",
        }

    def _proxy_request_with_proxy_manager(
        self,
        method: str,
        url: str,
        *,
        proxy_config: ProxyConfig,
        timeout: int,
        headers: Optional[Dict[str, str]] = None,
        params: Optional[Dict[str, Any]] = None,
        data: Any = None,
    ) -> requests.Response:
        """
        Proxy Network request implemented via urllib3.ProxyManager.

        This is required to reliably support HTTPS proxy endpoints like:
        https://<endpoint>.pr.thordata.net:9999
        """
        # Build final URL (include query params)
        req = requests.Request(method=method.upper(), url=url, params=params)
        prepped = self._proxy_session.prepare_request(req)
        final_url = prepped.url or url

        proxy_url = proxy_config.build_proxy_endpoint()
        proxy_headers = urllib3.make_headers(
            proxy_basic_auth=proxy_config.build_proxy_basic_auth()
        )

        pm = urllib3.ProxyManager(
            proxy_url,
            proxy_headers=proxy_headers,
            proxy_ssl_context=(
                ssl.create_default_context()
                if proxy_url.startswith("https://")
                else None
            ),
        )

        # Encode form data if dict
        body = None
        req_headers = dict(headers or {})
        if data is not None:
            if isinstance(data, dict):
                # form-urlencoded
                body = urlencode({k: str(v) for k, v in data.items()})
                req_headers.setdefault(
                    "Content-Type", "application/x-www-form-urlencoded"
                )
            else:
                body = data

        http_resp = pm.request(
            method.upper(),
            final_url,
            body=body,
            headers=req_headers or None,
            timeout=urllib3.Timeout(connect=timeout, read=timeout),
            retries=False,
            preload_content=True,
        )

        # Convert urllib3 response -> requests.Response (keep your API stable)
        r = requests.Response()
        r.status_code = int(getattr(http_resp, "status", 0) or 0)
        r._content = http_resp.data or b""
        r.url = final_url
        r.headers = requests.structures.CaseInsensitiveDict(
            dict(http_resp.headers or {})
        )
        return r

    def _request_with_retry(
        self, method: str, url: str, **kwargs: Any
    ) -> requests.Response:
        """Make a request with automatic retry."""
        kwargs.setdefault("timeout", self._default_timeout)

        @with_retry(self._retry_config)
        def _do_request() -> requests.Response:
            return self._proxy_session.request(method, url, **kwargs)

        try:
            return _do_request()
        except requests.Timeout as e:
            raise ThordataTimeoutError(
                f"Request timed out: {e}", original_error=e
            ) from e
        except requests.RequestException as e:
            raise ThordataNetworkError(f"Request failed: {e}", original_error=e) from e

    def close(self) -> None:
        """Close the underlying session."""
        self._proxy_session.close()
        self._api_session.close()

    def __enter__(self) -> ThordataClient:
        return self

    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        self.close()


========================================================================
FILE: src\thordata\demo.py
SIZE: 3946
TRUNCATED: no
========================================================================
"""
Unified demo entrypoint for the Thordata Python SDK.

This module runs the example scripts from the repository's `examples/` directory
using `runpy`, so it does not require `examples/` to be an importable package.

Usage:
    python -m thordata.demo serp
    python -m thordata.demo universal
    python -m thordata.demo scraper
    python -m thordata.demo concurrency

Notes:
- This entrypoint is primarily intended for repository usage (dev/demo).
- When installed from PyPI, the `examples/` directory is typically not included.
"""

from __future__ import annotations

import os
import runpy
import sys
from pathlib import Path
from typing import Callable, Dict


def _configure_stdio() -> None:
    # Avoid UnicodeEncodeError on Windows consoles with legacy encodings.
    if hasattr(sys.stdout, "reconfigure"):
        sys.stdout.reconfigure(encoding="utf-8", errors="replace")
    if hasattr(sys.stderr, "reconfigure"):
        sys.stderr.reconfigure(encoding="utf-8", errors="replace")


def _load_env() -> None:
    # Optional .env support for local development
    try:
        from dotenv import load_dotenv
    except ImportError:
        return
    load_dotenv()


def _repo_root() -> Path:
    """
    Resolve repository root based on src layout:
    <repo>/src/thordata/demo.py -> parents[2] == <repo>
    """
    return Path(__file__).resolve().parents[2]


def _examples_dir() -> Path:
    return _repo_root() / "examples"


def _demo_map() -> Dict[str, Path]:
    ex = _examples_dir()
    return {
        "serp": ex / "demo_serp_api.py",
        "universal": ex / "demo_universal.py",
        "scraper": ex / "demo_web_scraper_api.py",
        "concurrency": ex / "async_high_concurrency.py",
    }


def _usage() -> str:
    names = ", ".join(sorted(_demo_map().keys()))
    return f"Usage: python -m thordata.demo [{names}]"


def _run_demo(path: Path) -> int:
    if not path.exists():
        print(f"Error: demo script not found: {path}")
        return 2

    # Ensure examples dir is on sys.path (helpful if demo imports local helpers).
    examples_dir = str(path.parent.resolve())
    if examples_dir not in sys.path:
        sys.path.insert(0, examples_dir)

    try:
        # Load without triggering `if __name__ == "__main__": ...`
        ns = runpy.run_path(str(path), run_name="__thordata_demo__")

        main_func = ns.get("main")
        if callable(main_func):
            return int(main_func())  # type: ignore[arg-type]

        # Fallback: run as __main__ for scripts without main()
        runpy.run_path(str(path), run_name="__main__")
        return 0

    except KeyboardInterrupt:
        raise
    except SystemExit as e:
        # In case fallback run as __main__ triggered SystemExit
        code = e.code
        if code is None:
            return 0
        if isinstance(code, int):
            return code
        return 1
    except Exception as e:
        import traceback

        print()
        print("-" * 60)
        print("[thordata.demo] The demo script raised an exception.")
        print(f"[thordata.demo] Script: {path.name}")
        print(f"[thordata.demo] Error:  {type(e).__name__}: {e}")
        print()
        print("Note: This is a failure within the demo script itself,")
        print("      not an issue with the thordata.demo entrypoint.")
        print("-" * 60)
        traceback.print_exc()
        return 1


def main() -> int:
    _configure_stdio()
    _load_env()

    if len(sys.argv) < 2:
        print(_usage())
        return 2

    name = sys.argv[1].strip().lower()
    mapping = _demo_map()

    path = mapping.get(name)
    if path is None:
        print(f"Unknown demo: {name}")
        print(_usage())
        return 2

    return _run_demo(path)


if __name__ == "__main__":
    raise SystemExit(main())


========================================================================
FILE: src\thordata\enums.py
SIZE: 8886
TRUNCATED: no
========================================================================
"""
Enumerations for the Thordata Python SDK.

This module provides type-safe enumerations for all Thordata API parameters,
making it easier to discover available options via IDE autocomplete.
"""

from enum import Enum, IntEnum

# =============================================================================
# Continent Enum
# =============================================================================


class Continent(str, Enum):
    """
    Continent codes for geo-targeting.
    """

    AFRICA = "af"
    ANTARCTICA = "an"
    ASIA = "as"
    EUROPE = "eu"
    NORTH_AMERICA = "na"
    OCEANIA = "oc"
    SOUTH_AMERICA = "sa"


# =============================================================================
# Proxy Host Enum
# =============================================================================


class ProxyHost(str, Enum):
    """
    Available proxy gateway hosts.

    Note: Dashboard provides user-specific hosts like {shard}.{region}.thordata.net
    """

    DEFAULT = "pr.thordata.net"
    NORTH_AMERICA = "t.na.thordata.net"
    EUROPE = "t.eu.thordata.net"


class ProxyPort(IntEnum):
    """
    Available proxy gateway ports.
    """

    RESIDENTIAL = 9999
    MOBILE = 5555
    DATACENTER = 7777
    ISP = 6666


# =============================================================================
# Search Engine Enums
# =============================================================================


class Engine(str, Enum):
    """
    Supported search engines for SERP API.

    Engine naming convention:
    - Base search: {engine} for basic web search (google, bing, yandex, duckduckgo)
    - Verticals: {engine}_{vertical} (e.g., google_news, bing_images)
    - Sub-verticals: {engine}_{vertical}_{sub} (e.g., google_scholar_cite)
    """

    # ===================
    # Google
    # ===================
    GOOGLE = "google"
    GOOGLE_SEARCH = "google_search"
    GOOGLE_AI_MODE = "google_ai_mode"
    GOOGLE_WEB = "google_web"
    GOOGLE_SHOPPING = "google_shopping"
    GOOGLE_LOCAL = "google_local"
    GOOGLE_VIDEOS = "google_videos"
    GOOGLE_NEWS = "google_news"
    GOOGLE_FLIGHTS = "google_flights"
    GOOGLE_IMAGES = "google_images"
    GOOGLE_LENS = "google_lens"
    GOOGLE_TRENDS = "google_trends"
    GOOGLE_HOTELS = "google_hotels"
    GOOGLE_PLAY = "google_play"
    GOOGLE_JOBS = "google_jobs"
    GOOGLE_SCHOLAR = "google_scholar"
    GOOGLE_SCHOLAR_CITE = "google_scholar_cite"
    GOOGLE_SCHOLAR_AUTHOR = "google_scholar_author"
    GOOGLE_MAPS = "google_maps"
    GOOGLE_FINANCE = "google_finance"
    GOOGLE_FINANCE_MARKETS = "google_finance_markets"
    GOOGLE_PATENTS = "google_patents"
    GOOGLE_PATENTS_DETAILS = "google_patents_details"

    # ===================
    # Bing
    # ===================
    BING = "bing"
    BING_SEARCH = "bing_search"
    BING_IMAGES = "bing_images"
    BING_VIDEOS = "bing_videos"
    BING_NEWS = "bing_news"
    BING_MAPS = "bing_maps"
    BING_SHOPPING = "bing_shopping"

    # ===================
    # Yandex
    # ===================
    YANDEX = "yandex"
    YANDEX_SEARCH = "yandex_search"

    # ===================
    # DuckDuckGo
    # ===================
    DUCKDUCKGO = "duckduckgo"
    DUCKDUCKGO_SEARCH = "duckduckgo_search"


class GoogleSearchType(str, Enum):
    """
    Search types specific to Google.

    These map to the second part of Google engine names.
    For example, GOOGLE + NEWS = google_news
    """

    SEARCH = "search"
    AI_MODE = "ai_mode"
    WEB = "web"
    SHOPPING = "shopping"
    LOCAL = "local"
    VIDEOS = "videos"
    NEWS = "news"
    FLIGHTS = "flights"
    IMAGES = "images"
    LENS = "lens"
    TRENDS = "trends"
    HOTELS = "hotels"
    PLAY = "play"
    JOBS = "jobs"
    SCHOLAR = "scholar"
    MAPS = "maps"
    FINANCE = "finance"
    PATENTS = "patents"


class BingSearchType(str, Enum):
    """
    Search types specific to Bing.
    """

    SEARCH = "search"
    IMAGES = "images"
    VIDEOS = "videos"
    NEWS = "news"
    MAPS = "maps"
    SHOPPING = "shopping"


class GoogleTbm(str, Enum):
    """
    Google tbm (to be matched) parameter values.

    Only available when using specific Google engines that support tbm.
    """

    NEWS = "nws"
    SHOPPING = "shop"
    IMAGES = "isch"
    VIDEOS = "vid"


class Device(str, Enum):
    """
    Device types for SERP API.
    """

    DESKTOP = "desktop"
    MOBILE = "mobile"
    TABLET = "tablet"


class TimeRange(str, Enum):
    """
    Time range filters for search results.
    """

    HOUR = "hour"
    DAY = "day"
    WEEK = "week"
    MONTH = "month"
    YEAR = "year"


# =============================================================================
# Proxy Enums
# =============================================================================


class ProxyType(IntEnum):
    """
    Types of proxy networks available.
    """

    RESIDENTIAL = 1
    UNLIMITED = 2
    DATACENTER = 3
    ISP = 4
    MOBILE = 5


class SessionType(str, Enum):
    """
    Proxy session types for connection persistence.
    """

    ROTATING = "rotating"
    STICKY = "sticky"


# =============================================================================
# Output Format Enums
# =============================================================================


class OutputFormat(str, Enum):
    """
    Output formats for Universal Scraping API.

    Currently supported: html, png
    """

    HTML = "html"
    PNG = "png"


class DataFormat(str, Enum):
    """
    Data formats for task result download.
    """

    JSON = "json"
    CSV = "csv"
    XLSX = "xlsx"


# =============================================================================
# Task Status Enums
# =============================================================================


class TaskStatus(str, Enum):
    """
    Possible statuses for async scraping tasks.
    """

    PENDING = "pending"
    RUNNING = "running"
    READY = "ready"
    SUCCESS = "success"
    FINISHED = "finished"
    FAILED = "failed"
    ERROR = "error"
    CANCELLED = "cancelled"
    UNKNOWN = "unknown"

    @classmethod
    def is_terminal(cls, status: "TaskStatus") -> bool:
        """Check if a status is terminal (no more updates expected)."""
        return status in {
            cls.READY,
            cls.SUCCESS,
            cls.FINISHED,
            cls.FAILED,
            cls.ERROR,
            cls.CANCELLED,
        }

    @classmethod
    def is_success(cls, status: "TaskStatus") -> bool:
        """Check if a status indicates success."""
        return status in {cls.READY, cls.SUCCESS, cls.FINISHED}

    @classmethod
    def is_failure(cls, status: "TaskStatus") -> bool:
        """Check if a status indicates failure."""
        return status in {cls.FAILED, cls.ERROR}


# =============================================================================
# Country Enum (Common Countries)
# =============================================================================


class Country(str, Enum):
    """
    Common country codes for geo-targeting.
    """

    # North America
    US = "us"
    CA = "ca"
    MX = "mx"

    # Europe
    GB = "gb"
    DE = "de"
    FR = "fr"
    ES = "es"
    IT = "it"
    NL = "nl"
    PL = "pl"
    RU = "ru"
    UA = "ua"
    SE = "se"
    NO = "no"
    DK = "dk"
    FI = "fi"
    CH = "ch"
    AT = "at"
    BE = "be"
    PT = "pt"
    IE = "ie"
    CZ = "cz"
    GR = "gr"

    # Asia Pacific
    CN = "cn"
    JP = "jp"
    KR = "kr"
    IN = "in"
    AU = "au"
    NZ = "nz"
    SG = "sg"
    HK = "hk"
    TW = "tw"
    TH = "th"
    VN = "vn"
    ID = "id"
    MY = "my"
    PH = "ph"
    PK = "pk"
    BD = "bd"

    # South America
    BR = "br"
    AR = "ar"
    CL = "cl"
    CO = "co"
    PE = "pe"
    VE = "ve"

    # Middle East & Africa
    AE = "ae"
    SA = "sa"
    IL = "il"
    TR = "tr"
    ZA = "za"
    EG = "eg"
    NG = "ng"
    KE = "ke"
    MA = "ma"


# =============================================================================
# Helper Functions
# =============================================================================


def normalize_enum_value(value: object, enum_class: type) -> str:
    """
    Safely convert an enum or string to its string value.
    """
    if isinstance(value, enum_class):
        return str(getattr(value, "value", value)).lower()
    if isinstance(value, str):
        return value.lower()
    raise TypeError(
        f"Expected {enum_class.__name__} or str, got {type(value).__name__}"
    )


========================================================================
FILE: src\thordata\exceptions.py
SIZE: 10373
TRUNCATED: no
========================================================================
"""
Custom exception types for the Thordata Python SDK.

Exception Hierarchy:
    ThordataError (base)
    â”œâ”€â”€ ThordataConfigError      - Configuration/initialization issues
    â”œâ”€â”€ ThordataNetworkError     - Network connectivity issues (retryable)
    â”‚   â””â”€â”€ ThordataTimeoutError - Request timeout (retryable)
    â””â”€â”€ ThordataAPIError         - API returned an error
        â”œâ”€â”€ ThordataAuthError        - 401/403 authentication issues
        â”œâ”€â”€ ThordataRateLimitError   - 429/402 rate limit/quota issues
        â”œâ”€â”€ ThordataServerError      - 5xx server errors (retryable)
        â””â”€â”€ ThordataValidationError  - 400 bad request / validation errors
"""

from __future__ import annotations

from typing import Any, Optional, Set

# =============================================================================
# Base Exception
# =============================================================================


class ThordataError(Exception):
    """Base error for all Thordata SDK issues."""

    def __init__(self, message: str) -> None:
        super().__init__(message)
        self.message = message


# =============================================================================
# Configuration Errors
# =============================================================================


class ThordataConfigError(ThordataError):
    """
    Raised when the SDK is misconfigured.

    Examples:
        - Missing required tokens
        - Invalid parameter combinations
    """

    pass


# =============================================================================
# Network Errors (Usually Retryable)
# =============================================================================


class ThordataNetworkError(ThordataError):
    """
    Raised when a network-level error occurs.

    This is typically retryable (DNS failures, connection refused, etc.)
    """

    def __init__(
        self,
        message: str,
        *,
        original_error: Optional[Exception] = None,
    ) -> None:
        super().__init__(message)
        self.original_error = original_error


class ThordataTimeoutError(ThordataNetworkError):
    """
    Raised when a request times out.

    This is typically retryable.
    """

    pass


# =============================================================================
# API Errors
# =============================================================================


class ThordataAPIError(ThordataError):
    """
    Generic API error raised when the backend returns a non-success code
    or an unexpected response payload.

    Attributes:
        message:      Human-readable error message.
        status_code:  HTTP status code from the response (e.g., 401, 500).
        code:         Application-level code from the Thordata API JSON response.
        payload:      Raw payload (dict/str) returned by the API.
        request_id:   Optional request ID for debugging with support.
    """

    # HTTP status codes that indicate this error type
    HTTP_STATUS_CODES: Set[int] = set()

    def __init__(
        self,
        message: str,
        *,
        status_code: Optional[int] = None,
        code: Optional[int] = None,
        payload: Any = None,
        request_id: Optional[str] = None,
    ) -> None:
        super().__init__(message)
        self.status_code = status_code
        self.code = code
        self.payload = payload
        self.request_id = request_id

    def __repr__(self) -> str:
        return (
            f"{self.__class__.__name__}("
            f"message={self.message!r}, "
            f"status_code={self.status_code}, "
            f"code={self.code}, "
            f"request_id={self.request_id!r})"
        )

    @property
    def is_retryable(self) -> bool:
        """Whether this error is typically safe to retry."""
        return False


class ThordataAuthError(ThordataAPIError):
    """
    Authentication or authorization failures.

    HTTP Status: 401, 403
    Common causes:
        - Invalid or expired token
        - Insufficient permissions
        - IP not whitelisted
    """

    HTTP_STATUS_CODES = {401, 403}

    @property
    def is_retryable(self) -> bool:
        return False  # Auth errors shouldn't be retried


class ThordataRateLimitError(ThordataAPIError):
    """
    Rate limiting or quota/balance issues.

    HTTP Status: 429, 402
    Common causes:
        - Too many requests per second
        - Insufficient account balance
        - Quota exceeded

    Attributes:
        retry_after: Suggested seconds to wait before retrying (if provided).
    """

    HTTP_STATUS_CODES = {429, 402}

    def __init__(
        self,
        message: str,
        *,
        retry_after: Optional[int] = None,
        **kwargs: Any,
    ) -> None:
        super().__init__(message, **kwargs)
        self.retry_after = retry_after

    @property
    def is_retryable(self) -> bool:
        # Rate limits are retryable after waiting
        return True


class ThordataServerError(ThordataAPIError):
    """
    Server-side errors (5xx).

    HTTP Status: 500, 502, 503, 504
    These are typically transient and safe to retry.
    """

    HTTP_STATUS_CODES = {500, 502, 503, 504}

    @property
    def is_retryable(self) -> bool:
        return True


class ThordataValidationError(ThordataAPIError):
    """
    Request validation errors.

    HTTP Status: 400, 422
    Common causes:
        - Invalid parameters
        - Missing required fields
        - Malformed request body
    """

    HTTP_STATUS_CODES = {400, 422}

    @property
    def is_retryable(self) -> bool:
        return False  # Bad requests shouldn't be retried


class ThordataNotCollectedError(ThordataAPIError):
    """
    The request was accepted but no valid data could be collected/parsed.

    API Code: 300
    Billing: Not billed (per Thordata billing rules).
    This error is often transient and typically safe to retry.
    """

    API_CODES = {300}
    HTTP_STATUS_CODES: Set[int] = set()

    @property
    def is_retryable(self) -> bool:
        return True


# =============================================================================
# Exception Factory
# =============================================================================


def raise_for_code(
    message: str,
    *,
    status_code: Optional[int] = None,
    code: Optional[int] = None,
    payload: Any = None,
    request_id: Optional[str] = None,
) -> None:
    """
    Factory function to raise the appropriate exception based on status/code.

    This centralizes the error-mapping logic that was previously duplicated
    across multiple methods.

    Args:
        message: Human-readable error message.
        status_code: HTTP status code (if available).
        code: Application-level code from API response.
        payload: Raw API response payload.
        request_id: Optional request ID for debugging.

    Raises:
        ThordataAuthError: For 401/403 codes.
        ThordataRateLimitError: For 429/402 codes.
        ThordataServerError: For 5xx codes.
        ThordataValidationError: For 400/422 codes.
        ThordataAPIError: For all other error codes.
    """
    # Determine the effective error code.
    # Prefer payload `code` when present and not success (200),
    # otherwise fall back to HTTP status when it indicates an error.
    effective_code: Optional[int] = None

    if code is not None and code != 200:
        effective_code = code
    elif status_code is not None and status_code != 200:
        effective_code = status_code
    else:
        effective_code = code if code is not None else status_code

    kwargs = {
        "status_code": status_code,
        "code": code,
        "payload": payload,
        "request_id": request_id,
    }

    # Not collected (API payload code 300, often retryable, not billed)
    # Check this FIRST since 300 is in API_CODES, not HTTP_STATUS_CODES
    if effective_code in ThordataNotCollectedError.API_CODES:
        raise ThordataNotCollectedError(message, **kwargs)

    # Auth errors
    if effective_code in ThordataAuthError.HTTP_STATUS_CODES:
        raise ThordataAuthError(message, **kwargs)

    # Rate limit errors
    if effective_code in ThordataRateLimitError.HTTP_STATUS_CODES:
        # Try to extract retry_after from payload
        retry_after = None
        if isinstance(payload, dict):
            retry_after = payload.get("retry_after")
        raise ThordataRateLimitError(message, retry_after=retry_after, **kwargs)

    # Server errors
    if effective_code is not None and 500 <= effective_code < 600:
        raise ThordataServerError(message, **kwargs)

    # Validation errors
    if effective_code in ThordataValidationError.HTTP_STATUS_CODES:
        raise ThordataValidationError(message, **kwargs)

    # Generic API error
    raise ThordataAPIError(message, **kwargs)


# =============================================================================
# Retry Helper
# =============================================================================


def is_retryable_exception(exc: Exception) -> bool:
    """
    Check if an exception is safe to retry.

    Args:
        exc: The exception to check.

    Returns:
        True if the exception is typically safe to retry.
    """
    # Network errors are retryable
    if isinstance(exc, ThordataNetworkError):
        return True

    # Check API errors with is_retryable property
    if isinstance(exc, ThordataAPIError):
        return exc.is_retryable

    # requests/aiohttp specific exceptions
    # (imported dynamically to avoid hard dependency)
    try:
        import requests

        if isinstance(exc, (requests.Timeout, requests.ConnectionError)):
            return True
    except ImportError:
        pass

    try:
        import aiohttp

        if isinstance(exc, (aiohttp.ClientError, aiohttp.ServerTimeoutError)):
            return True
    except ImportError:
        pass

    return False


========================================================================
FILE: src\thordata\models.py
SIZE: 38874
TRUNCATED: no
========================================================================
"""
Data models for the Thordata Python SDK.

This module provides type-safe dataclasses for configuring proxy requests,
SERP API calls, and Universal Scraping requests. Using these models enables
IDE autocomplete and reduces parameter errors.

Example:
    >>> from thordata.models import ProxyConfig, SerpRequest
    >>>
    >>> # Build a proxy URL with geo-targeting
    >>> proxy = ProxyConfig(
    ...     username="myuser",
    ...     password="mypass",
    ...     country="us",
    ...     city="seattle"
    ... )
    >>> print(proxy.build_proxy_url())

    >>> # Configure a SERP request
    >>> serp = SerpRequest(query="python tutorial", engine="google", num=20)
    >>> print(serp.to_payload())
"""

from __future__ import annotations

import json
import re
import ssl
import uuid
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional, Union

import urllib3

# =============================================================================
# Proxy Product Types
# =============================================================================


class ProxyProduct(str, Enum):
    """
    Thordata proxy product types with their default ports.

    Each product type has a specific port on the proxy gateway.
    """

    RESIDENTIAL = "residential"
    MOBILE = "mobile"
    DATACENTER = "datacenter"
    ISP = "isp"

    @property
    def default_port(self) -> int:
        """Get the default port for this proxy product."""
        ports = {
            "residential": 9999,
            "mobile": 5555,
            "datacenter": 7777,
            "isp": 6666,
        }
        return ports[self.value]


# =============================================================================
# Proxy Configuration Models
# =============================================================================


@dataclass
class ProxyConfig:
    """
    Configuration for building a Thordata proxy URL.

    This class handles the complex username format required by Thordata proxies,
    where geo-targeting and session parameters are embedded in the username.

    Args:
        username: Your Thordata account username (the part after 'td-customer-').
        password: Your Thordata account password.
        product: Proxy product type (residential, mobile, datacenter, isp).
        host: Proxy gateway host. If None, uses default based on product.
        port: Proxy gateway port. If None, uses default based on product.
        protocol: Proxy protocol - 'http' or 'https'.

        # Geo-targeting (all optional)
        continent: Target continent code (af/an/as/eu/na/oc/sa).
        country: Target country code in ISO 3166-1 alpha-2 format.
        state: Target state name in lowercase.
        city: Target city name in lowercase.
        asn: Target ASN code (e.g., 'AS12322'). Must be used with country.

        # Session control (optional)
        session_id: Session identifier for sticky sessions.
        session_duration: Session duration in minutes (1-90).

    Example:
        >>> config = ProxyConfig(
        ...     username="GnrqUwwu3obt",
        ...     password="PkCSzvt30iww",
        ...     product=ProxyProduct.RESIDENTIAL,
        ...     country="us",
        ...     state="california",
        ...     session_id="mysession123",
        ...     session_duration=10
        ... )
        >>> print(config.build_proxy_url())
        http://td-customer-GnrqUwwu3obt-country-us-state-california-sessid-mysession123-sesstime-10:PkCSzvt30iww@....pr.thordata.net:9999
    """

    username: str
    password: str
    product: Union[ProxyProduct, str] = ProxyProduct.RESIDENTIAL
    host: Optional[str] = None
    port: Optional[int] = None
    protocol: str = "http"

    # Geo-targeting
    continent: Optional[str] = None
    country: Optional[str] = None
    state: Optional[str] = None
    city: Optional[str] = None
    asn: Optional[str] = None

    # Session control
    session_id: Optional[str] = None
    session_duration: Optional[int] = None  # minutes, 1-90

    # Valid continent codes
    VALID_CONTINENTS = {"af", "an", "as", "eu", "na", "oc", "sa"}

    def __post_init__(self) -> None:
        """Validate configuration after initialization."""
        # Normalize product to enum
        if isinstance(self.product, str):
            self.product = ProxyProduct(self.product.lower())

        # Set default host and port based on product
        if self.host is None:
            # Set host based on product type
            host_map = {
                # User&Pass auth entry (docs examples use t.pr.thordata.net for authenticated proxy)
                ProxyProduct.RESIDENTIAL: "t.pr.thordata.net",
                ProxyProduct.DATACENTER: "dc.pr.thordata.net",
                ProxyProduct.MOBILE: "m.pr.thordata.net",
                ProxyProduct.ISP: "isp.pr.thordata.net",
            }
            self.host = host_map.get(self.product, "pr.thordata.net")

        if self.port is None:
            self.port = self.product.default_port

        self._validate()

    def _validate(self) -> None:
        """Validate the proxy configuration."""
        # Validate protocol
        if self.protocol not in ("http", "https"):
            raise ValueError(
                f"Invalid protocol: {self.protocol}. Must be 'http' or 'https'."
            )

        # Validate session duration
        if self.session_duration is not None:
            if not 1 <= self.session_duration <= 90:
                raise ValueError(
                    f"session_duration must be between 1 and 90 minutes, "
                    f"got {self.session_duration}"
                )
            if not self.session_id:
                raise ValueError("session_duration requires session_id to be set")

        # Validate ASN requires country
        if self.asn and not self.country:
            raise ValueError("ASN targeting requires country to be specified")

        # Validate continent code
        if self.continent and self.continent.lower() not in self.VALID_CONTINENTS:
            raise ValueError(
                f"Invalid continent code: {self.continent}. "
                f"Must be one of: {', '.join(sorted(self.VALID_CONTINENTS))}"
            )

        # Validate country code format (2 letters)
        if self.country and not re.match(r"^[a-zA-Z]{2}$", self.country):
            raise ValueError(
                f"Invalid country code: {self.country}. "
                "Must be a 2-letter ISO 3166-1 alpha-2 code."
            )

    def build_username(self) -> str:
        """
        Build the complete username string with embedded parameters.

        Returns:
            The formatted username string for proxy authentication.
        """
        parts = [f"td-customer-{self.username}"]

        # Add geo-targeting parameters (order matters)
        if self.continent:
            parts.append(f"continent-{self.continent.lower()}")

        if self.country:
            parts.append(f"country-{self.country.lower()}")

        if self.state:
            parts.append(f"state-{self.state.lower()}")

        if self.city:
            parts.append(f"city-{self.city.lower()}")

        if self.asn:
            # Ensure ASN has correct format
            asn_value = self.asn.upper()
            if not asn_value.startswith("AS"):
                asn_value = f"AS{asn_value}"
            parts.append(f"asn-{asn_value}")

        # Add session parameters
        if self.session_id:
            parts.append(f"sessid-{self.session_id}")

        if self.session_duration:
            parts.append(f"sesstime-{self.session_duration}")

        return "-".join(parts)

    def build_proxy_url(self) -> str:
        """
        Build the complete proxy URL.

        Returns:
            The formatted proxy URL for use with requests/aiohttp.
        """
        username = self.build_username()
        return f"{self.protocol}://{username}:{self.password}@{self.host}:{self.port}"

    def build_proxy_endpoint(self) -> str:
        """Proxy endpoint without credentials, for HTTPS proxy managers."""
        return f"{self.protocol}://{self.host}:{self.port}"

    def build_proxy_basic_auth(self) -> str:
        """Basic auth string 'username:password' for Proxy-Authorization."""
        return f"{self.build_username()}:{self.password}"

    def to_proxies_dict(self) -> Dict[str, str]:
        """
        Build a proxies dict suitable for the requests library.

        Returns:
            Dict with 'http' and 'https' keys pointing to the proxy URL.
        """
        url = self.build_proxy_url()
        return {"http": url, "https": url}

    def to_aiohttp_config(self) -> tuple:
        """
        Get proxy configuration for aiohttp.

        Returns:
            Tuple of (proxy_url, proxy_auth) for aiohttp.
        """
        try:
            import aiohttp

            proxy_url = f"{self.protocol}://{self.host}:{self.port}"
            proxy_auth = aiohttp.BasicAuth(
                login=self.build_username(), password=self.password
            )
            return proxy_url, proxy_auth
        except ImportError as e:
            raise ImportError(
                "aiohttp is required for async proxy configuration"
            ) from e


@dataclass
class WhitelistProxyConfig:
    """
    Proxy config for IP-whitelist authentication mode (no username/password).

    In whitelist mode, you do NOT pass proxy auth.
    You only connect to the proxy entry node (host:port).

    Examples (from docs):
      - Global random: pr.thordata.net:9999
      - Country nodes: us-pr.thordata.net:10000, etc.
    """

    host: str = "pr.thordata.net"
    port: int = 9999
    protocol: str = "http"  # use http for proxy scheme; target URL can still be https

    def __post_init__(self) -> None:
        if self.protocol not in ("http", "https"):
            raise ValueError("protocol must be 'http' or 'https'")

    def build_proxy_url(self) -> str:
        return f"{self.protocol}://{self.host}:{self.port}"

    def to_proxies_dict(self) -> Dict[str, str]:
        url = self.build_proxy_url()
        return {"http": url, "https": url}

    def to_aiohttp_config(self) -> tuple:
        # aiohttp: proxy_auth should be None in whitelist mode
        return self.build_proxy_url(), None


@dataclass
class StaticISPProxy:
    """
    Configuration for static ISP proxy with direct IP connection.

    Static ISP proxies connect directly to a purchased IP address,
    not through the gateway.

    Args:
        host: The static IP address you purchased.
        username: Your ISP proxy username.
        password: Your ISP proxy password.
        port: Port number (default: 6666).
        protocol: Proxy protocol - 'http' or 'https'.

    Example:
        >>> proxy = StaticISPProxy(
        ...     host="xx.xxx.xxx.xxx",
        ...     username="myuser",
        ...     password="mypass"
        ... )
        >>> print(proxy.build_proxy_url())
        http://myuser:mypass@xx.xxx.xxx.xxx:6666
    """

    host: str
    username: str
    password: str
    port: int = 6666
    protocol: str = "http"

    def __post_init__(self) -> None:
        """Validate configuration."""
        if self.protocol not in ("http", "https"):
            raise ValueError(
                f"Invalid protocol: {self.protocol}. Must be 'http' or 'https'."
            )

    def build_proxy_url(self) -> str:
        """
        Build the complete proxy URL for direct connection.

        Returns:
            The formatted proxy URL.
        """
        return (
            f"{self.protocol}://{self.username}:{self.password}@{self.host}:{self.port}"
        )

    def to_proxies_dict(self) -> Dict[str, str]:
        """
        Build a proxies dict suitable for the requests library.

        Returns:
            Dict with 'http' and 'https' keys pointing to the proxy URL.
        """
        url = self.build_proxy_url()
        return {"http": url, "https": url}

    def to_aiohttp_config(self) -> tuple:
        """
        Get proxy configuration for aiohttp.

        Returns:
            Tuple of (proxy_url, proxy_auth) for aiohttp.
        """
        try:
            import aiohttp

            proxy_url = f"{self.protocol}://{self.host}:{self.port}"
            proxy_auth = aiohttp.BasicAuth(login=self.username, password=self.password)
            return proxy_url, proxy_auth
        except ImportError as e:
            raise ImportError(
                "aiohttp is required for async proxy configuration"
            ) from e

    @classmethod
    def from_env(cls) -> "StaticISPProxy":
        """
        Create StaticISPProxy from environment variables.

        Required env vars:
            - THORDATA_ISP_HOST
            - THORDATA_ISP_USERNAME
            - THORDATA_ISP_PASSWORD

        Returns:
            Configured StaticISPProxy instance.

        Raises:
            ValueError: If required environment variables are missing.
        """
        import os

        host = os.getenv("THORDATA_ISP_HOST")
        username = os.getenv("THORDATA_ISP_USERNAME")
        password = os.getenv("THORDATA_ISP_PASSWORD")

        if not all([host, username, password]):
            raise ValueError(
                "THORDATA_ISP_HOST, THORDATA_ISP_USERNAME, and "
                "THORDATA_ISP_PASSWORD are required"
            )

        return cls(host=host, username=username, password=password)


@dataclass
class StickySession(ProxyConfig):
    """
    Convenience class for creating sticky session proxy configurations.

    A sticky session keeps the same IP address for a specified duration,
    useful for multi-step operations that require IP consistency.

    Args:
        duration_minutes: How long to keep the same IP (1-90 minutes).
        auto_session_id: If True, automatically generates a unique session ID.

    Example:
        >>> session = StickySession(
        ...     username="myuser",
        ...     password="mypass",
        ...     country="us",
        ...     duration_minutes=15
        ... )
        >>> # Each call to build_proxy_url() uses the same session
        >>> url = session.build_proxy_url()
    """

    duration_minutes: int = 10
    auto_session_id: bool = True

    def __post_init__(self) -> None:
        # Auto-generate session ID if requested and not provided
        if self.auto_session_id and not self.session_id:
            self.session_id = uuid.uuid4().hex[:12]

        # Set session_duration from duration_minutes
        self.session_duration = self.duration_minutes

        # Call parent post_init
        super().__post_init__()


# =============================================================================
# SERP API Models
# =============================================================================


@dataclass
class SerpRequest:
    """
    Configuration for a SERP API request.

    Supports Google, Bing, Yandex, DuckDuckGo, and Baidu search engines.

    Args:
        query: The search query string (required).
        engine: Search engine to use (default: 'google').
        num: Number of results per page (default: 10).
        start: Result offset for pagination (default: 0).

        # Localization
        country: Country code for results (gl parameter for Google).
        language: Language code for interface (hl parameter for Google).
        google_domain: Google domain to use (e.g., 'google.co.uk').

        # Geo-targeting
        location: Location name for geo-targeting.
        uule: Encoded location parameter (use with location).

        # Search type
        search_type: Type of search (images, news, shopping, videos, etc.).

        # Filters
        safe_search: Enable safe search filtering.
        time_filter: Time range filter (hour, day, week, month, year).
        no_autocorrect: Disable automatic spelling correction (nfpr).
        filter_duplicates: Enable/disable duplicate filtering.

        # Device & Rendering
        device: Device type ('desktop', 'mobile', 'tablet').
        render_js: Enable JavaScript rendering in SERP (render_js=True/False).
        no_cache: Disable internal caching (no_cache=True/False).

        # Output
        output_format: 'json' (default) or 'html'.

        # Advanced
        ludocid: Google Place ID.
        kgmid: Google Knowledge Graph ID.

        # Extra
        extra_params: Additional parameters to pass through (ibp, lsig, si, uds, ...).
    """

    query: str
    engine: str = "google"
    num: int = 10
    start: int = 0

    # Localization
    country: Optional[str] = None  # 'gl' for Google
    language: Optional[str] = None  # 'hl' for Google
    google_domain: Optional[str] = None
    countries_filter: Optional[str] = None  # 'cr' parameter
    languages_filter: Optional[str] = None  # 'lr' parameter

    # Geo-targeting
    location: Optional[str] = None
    uule: Optional[str] = None  # Encoded location

    # Search type
    search_type: Optional[str] = None  # tbm parameter (isch, shop, nws, vid, ...)

    # Filters
    safe_search: Optional[bool] = None
    time_filter: Optional[str] = None  # tbs parameter (time part)
    no_autocorrect: bool = False  # nfpr parameter
    filter_duplicates: Optional[bool] = None  # filter parameter

    # Device & Rendering
    device: Optional[str] = None  # 'desktop', 'mobile', 'tablet'
    render_js: Optional[bool] = None  # render_js parameter
    no_cache: Optional[bool] = None  # no_cache parameter

    # Output format
    output_format: str = "json"  # 'json' or 'html'

    # Advanced Google parameters
    ludocid: Optional[str] = None  # Google Place ID
    kgmid: Optional[str] = None  # Knowledge Graph ID

    # Pass-through
    extra_params: Dict[str, Any] = field(default_factory=dict)

    # Search type mappings for tbm parameter
    SEARCH_TYPE_MAP = {
        "images": "isch",
        "shopping": "shop",
        "news": "nws",
        "videos": "vid",
        # Direct values also work
        "isch": "isch",
        "shop": "shop",
        "nws": "nws",
        "vid": "vid",
    }

    # Time filter mappings for tbs parameter
    TIME_FILTER_MAP = {
        "hour": "qdr:h",
        "day": "qdr:d",
        "week": "qdr:w",
        "month": "qdr:m",
        "year": "qdr:y",
    }

    # Engine URL defaults
    ENGINE_URLS = {
        "google": "google.com",
        "bing": "bing.com",
        "yandex": "yandex.com",
        "duckduckgo": "duckduckgo.com",
        "baidu": "baidu.com",
    }

    def to_payload(self) -> Dict[str, Any]:
        """
        Convert to API request payload.

        Returns:
            Dictionary ready to be sent to the SERP API.
        """
        engine = self.engine.lower()

        payload: Dict[str, Any] = {
            "engine": engine,
            "num": str(self.num),
        }

        fmt = self.output_format.lower()
        if fmt == "json":
            payload["json"] = "1"
        elif fmt == "html":
            # omit "json" to get raw HTML (per docs: no json -> HTML)
            pass
        else:
            # keep backward compatibility: if user passes "2"/"both"/etc.
            if fmt in ("2", "both", "json+html", "json_html"):
                payload["json"] = "2"

        # Handle query parameter (Yandex uses 'text', others use 'q')
        if engine == "yandex":
            payload["text"] = self.query
        else:
            payload["q"] = self.query

        # Domain overrides (preferred by docs)
        if self.google_domain:
            payload["google_domain"] = self.google_domain

        # Pagination
        if self.start > 0:
            payload["start"] = str(self.start)

        # Localization
        if self.country:
            payload["gl"] = self.country.lower()

        if self.language:
            payload["hl"] = self.language.lower()

        if self.countries_filter:
            payload["cr"] = self.countries_filter

        if self.languages_filter:
            payload["lr"] = self.languages_filter

        # Geo-targeting
        if self.location:
            payload["location"] = self.location

        if self.uule:
            payload["uule"] = self.uule

        # Search type (tbm)
        if self.search_type:
            search_type_lower = self.search_type.lower()
            tbm_value = self.SEARCH_TYPE_MAP.get(search_type_lower, search_type_lower)
            payload["tbm"] = tbm_value

        # Filters
        if self.safe_search is not None:
            payload["safe"] = "active" if self.safe_search else "off"

        if self.time_filter:
            time_lower = self.time_filter.lower()
            tbs_value = self.TIME_FILTER_MAP.get(time_lower, time_lower)
            payload["tbs"] = tbs_value

        if self.no_autocorrect:
            payload["nfpr"] = "1"

        if self.filter_duplicates is not None:
            payload["filter"] = "1" if self.filter_duplicates else "0"

        # Device
        if self.device:
            payload["device"] = self.device.lower()

        # Rendering & cache control
        if self.render_js is not None:
            payload["render_js"] = "True" if self.render_js else "False"

        if self.no_cache is not None:
            payload["no_cache"] = "True" if self.no_cache else "False"

        # Advanced Google parameters
        if self.ludocid:
            payload["ludocid"] = self.ludocid

        if self.kgmid:
            payload["kgmid"] = self.kgmid

        # Extra parameters (ibp, lsig, si, uds, etc.)
        payload.update(self.extra_params)

        return payload


# =============================================================================
# Universal Scraper (Web Unlocker) Models
# =============================================================================


@dataclass
class UniversalScrapeRequest:
    """
    Configuration for a Universal Scraping API (Web Unlocker) request.

    This API bypasses anti-bot protections like Cloudflare, CAPTCHAs, etc.

    Args:
        url: Target URL to scrape (required).
        js_render: Enable JavaScript rendering with headless browser.
        output_format: Output format - 'html' or 'png' (screenshot).
        country: Country code for geo-targeting the request.
        block_resources: Block specific resources (e.g., 'script', 'image').
        clean_content: Remove JS/CSS from returned content (e.g., 'js,css').
        wait: Wait time in milliseconds after page load (max 100000).
        wait_for: CSS selector to wait for before returning.
        headers: Custom request headers as list of {name, value} dicts.
        cookies: Custom cookies as list of {name, value} dicts.
        extra_params: Additional parameters to pass through.

    Example:
        >>> req = UniversalScrapeRequest(
        ...     url="https://example.com",
        ...     js_render=True,
        ...     output_format="html",
        ...     country="us",
        ...     wait=5000,
        ...     wait_for=".content"
        ... )
        >>> payload = req.to_payload()
    """

    url: str
    js_render: bool = False
    output_format: str = "html"  # 'html' or 'png'
    country: Optional[str] = None
    block_resources: Optional[str] = None  # e.g., 'script', 'image', 'script,image'
    clean_content: Optional[str] = None  # e.g., 'js', 'css', 'js,css'
    wait: Optional[int] = None  # Milliseconds, max 100000
    wait_for: Optional[str] = None  # CSS selector
    headers: Optional[List[Dict[str, str]]] = None  # [{"name": "...", "value": "..."}]
    cookies: Optional[List[Dict[str, str]]] = None  # [{"name": "...", "value": "..."}]
    extra_params: Dict[str, Any] = field(default_factory=dict)  # è¿™ä¸ªå¿…é¡»ç”¨ field()

    def __post_init__(self) -> None:
        """Validate configuration."""
        valid_formats = {"html", "png"}
        if self.output_format.lower() not in valid_formats:
            raise ValueError(
                f"Invalid output_format: {self.output_format}. "
                f"Must be one of: {', '.join(valid_formats)}"
            )

        if self.wait is not None and (self.wait < 0 or self.wait > 100000):
            raise ValueError(
                f"wait must be between 0 and 100000 milliseconds, got {self.wait}"
            )

    def to_payload(self) -> Dict[str, Any]:
        """
        Convert to API request payload.

        Returns:
            Dictionary ready to be sent to the Universal API.
        """
        payload: Dict[str, Any] = {
            "url": self.url,
            "js_render": "True" if self.js_render else "False",
            "type": self.output_format.lower(),
        }

        if self.country:
            payload["country"] = self.country.lower()

        if self.block_resources:
            payload["block_resources"] = self.block_resources

        if self.clean_content:
            payload["clean_content"] = self.clean_content

        if self.wait is not None:
            payload["wait"] = str(self.wait)

        if self.wait_for:
            payload["wait_for"] = self.wait_for

        if self.headers:
            payload["headers"] = json.dumps(self.headers)

        if self.cookies:
            payload["cookies"] = json.dumps(self.cookies)

        payload.update(self.extra_params)

        return payload


# =============================================================================
# Web Scraper Task Models
# =============================================================================


@dataclass
class ScraperTaskConfig:
    """
    Configuration for creating a Web Scraper API task.

    Note: You must get spider_id and spider_name from the Thordata Dashboard.

    Args:
        file_name: Name for the output file.
        spider_id: Spider identifier from Dashboard.
        spider_name: Spider name (usually the target domain).
        parameters: Spider-specific parameters.
        universal_params: Global spider settings.
        include_errors: Include error details in output.

    Example:
        >>> config = ScraperTaskConfig(
        ...     file_name="youtube_data",
        ...     spider_id="youtube_video-post_by-url",
        ...     spider_name="youtube.com",
        ...     parameters={
        ...         "url": "https://youtube.com/@channel/videos",
        ...         "num_of_posts": "50"
        ...     }
        ... )
        >>> payload = config.to_payload()
    """

    file_name: str
    spider_id: str
    spider_name: str
    parameters: Dict[str, Any]
    universal_params: Optional[Dict[str, Any]] = None
    include_errors: bool = True

    def to_payload(self) -> Dict[str, Any]:
        """
        Convert to API request payload.

        Returns:
            Dictionary ready to be sent to the Web Scraper API.
        """
        payload: Dict[str, Any] = {
            "file_name": self.file_name,
            "spider_id": self.spider_id,
            "spider_name": self.spider_name,
            "spider_parameters": json.dumps([self.parameters]),
            "spider_errors": "true" if self.include_errors else "false",
        }

        if self.universal_params:
            payload["spider_universal"] = json.dumps(self.universal_params)

        return payload


@dataclass
class CommonSettings:
    """
    Common settings for YouTube video/audio downloads.

    Used by /video_builder endpoint as `common_settings` parameter.
    Also known as `spider_universal` in some documentation.

    Args:
        resolution: Video resolution (360p/480p/720p/1080p/1440p/2160p).
        audio_format: Audio format (opus/mp3).
        bitrate: Audio bitrate (48/64/128/160/256/320 or with Kbps suffix).
        is_subtitles: Whether to download subtitles ("true"/"false").
        subtitles_language: Subtitle language code (e.g., "en", "zh-Hans").

    Example for video:
        >>> settings = CommonSettings(
        ...     resolution="1080p",
        ...     is_subtitles="true",
        ...     subtitles_language="en"
        ... )

    Example for audio:
        >>> settings = CommonSettings(
        ...     audio_format="mp3",
        ...     bitrate="320",
        ...     is_subtitles="true",
        ...     subtitles_language="en"
        ... )
    """

    # Video settings
    resolution: Optional[str] = None

    # Audio settings
    audio_format: Optional[str] = None
    bitrate: Optional[str] = None

    # Subtitle settings (used by both video and audio)
    is_subtitles: Optional[str] = None
    subtitles_language: Optional[str] = None

    # Valid values for validation
    VALID_RESOLUTIONS = {"360p", "480p", "720p", "1080p", "1440p", "2160p"}
    VALID_AUDIO_FORMATS = {"opus", "mp3"}

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary, excluding None values."""
        result = {}
        if self.resolution is not None:
            result["resolution"] = self.resolution
        if self.audio_format is not None:
            result["audio_format"] = self.audio_format
        if self.bitrate is not None:
            result["bitrate"] = self.bitrate
        if self.is_subtitles is not None:
            result["is_subtitles"] = self.is_subtitles
        if self.subtitles_language is not None:
            result["subtitles_language"] = self.subtitles_language
        return result

    def to_json(self) -> str:
        """Convert to JSON string for form submission."""
        return json.dumps(self.to_dict())


@dataclass
class VideoTaskConfig:
    """
    Configuration for creating a YouTube video/audio download task.

    Uses the /video_builder endpoint.

    Args:
        file_name: Name for the output file. Supports {{TasksID}}, {{VideoID}}.
        spider_id: Spider identifier (e.g., "youtube_video_by-url", "youtube_audio_by-url").
        spider_name: Spider name (typically "youtube.com").
        parameters: Spider-specific parameters (e.g., video URL).
        common_settings: Video/audio settings (resolution, format, subtitles).
        include_errors: Include error details in output.

    Example:
        >>> config = VideoTaskConfig(
        ...     file_name="{{VideoID}}",
        ...     spider_id="youtube_video_by-url",
        ...     spider_name="youtube.com",
        ...     parameters={"url": "https://www.youtube.com/watch?v=xxx"},
        ...     common_settings=CommonSettings(
        ...         resolution="1080p",
        ...         is_subtitles="true",
        ...         subtitles_language="en"
        ...     )
        ... )
    """

    file_name: str
    spider_id: str
    spider_name: str
    parameters: Dict[str, Any]
    common_settings: CommonSettings
    include_errors: bool = True

    def to_payload(self) -> Dict[str, Any]:
        """
        Convert to API request payload.

        Returns:
            Dictionary ready to be sent to the video_builder API.
        """
        payload: Dict[str, Any] = {
            "file_name": self.file_name,
            "spider_id": self.spider_id,
            "spider_name": self.spider_name,
            "spider_parameters": json.dumps([self.parameters]),
            "spider_errors": "true" if self.include_errors else "false",
            "common_settings": self.common_settings.to_json(),
        }
        return payload


# =============================================================================
# Response Models
# =============================================================================


@dataclass
class TaskStatusResponse:
    """
    Response from task status check.

    Attributes:
        task_id: The task identifier.
        status: Current task status.
        progress: Optional progress percentage.
        message: Optional status message.
    """

    task_id: str
    status: str
    progress: Optional[int] = None
    message: Optional[str] = None

    def is_complete(self) -> bool:
        """Check if the task has completed (success or failure)."""
        terminal_statuses = {
            "ready",
            "success",
            "finished",
            "failed",
            "error",
            "cancelled",
        }
        return self.status.lower() in terminal_statuses

    def is_success(self) -> bool:
        """Check if the task completed successfully."""
        success_statuses = {"ready", "success", "finished"}
        return self.status.lower() in success_statuses

    def is_failed(self) -> bool:
        """Check if the task failed."""
        failure_statuses = {"failed", "error"}
        return self.status.lower() in failure_statuses


@dataclass
class UsageStatistics:
    """
    Response model for account usage statistics.

    Attributes:
        total_usage_traffic: Total traffic used (KB).
        traffic_balance: Remaining traffic balance (KB).
        query_days: Number of days in the query range.
        range_usage_traffic: Traffic used in the specified date range (KB).
        data: Daily usage breakdown.
    """

    total_usage_traffic: float
    traffic_balance: float
    query_days: int
    range_usage_traffic: float
    data: List[Dict[str, Any]]

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "UsageStatistics":
        """Create from API response dict."""
        return cls(
            total_usage_traffic=float(data.get("total_usage_traffic", 0)),
            traffic_balance=float(data.get("traffic_balance", 0)),
            query_days=int(data.get("query_days", 0)),
            range_usage_traffic=float(data.get("range_usage_traffic", 0)),
            data=data.get("data", []),
        )

    def total_usage_gb(self) -> float:
        """Get total usage in GB."""
        return self.total_usage_traffic / (1024 * 1024)

    def balance_gb(self) -> float:
        """Get balance in GB."""
        return self.traffic_balance / (1024 * 1024)

    def range_usage_gb(self) -> float:
        """Get range usage in GB."""
        return self.range_usage_traffic / (1024 * 1024)


@dataclass
class ProxyUser:
    """
    Proxy user (sub-account) information.

    Attributes:
        username: User's username.
        password: User's password.
        status: User status (True=enabled, False=disabled).
        traffic_limit: Traffic limit in MB (0 = unlimited).
        usage_traffic: Traffic used in KB.
    """

    username: str
    password: str
    status: bool
    traffic_limit: int
    usage_traffic: float

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ProxyUser":
        """Create from API response dict."""
        return cls(
            username=data.get("username", ""),
            password=data.get("password", ""),
            status=data.get("status") in (True, "true", 1),
            traffic_limit=int(data.get("traffic_limit", 0)),
            usage_traffic=float(data.get("usage_traffic", 0)),
        )

    def usage_gb(self) -> float:
        """Get usage in GB."""
        return self.usage_traffic / (1024 * 1024)

    def limit_gb(self) -> float:
        """Get limit in GB (0 means unlimited)."""
        if self.traffic_limit == 0:
            return 0
        return self.traffic_limit / 1024


@dataclass
class ProxyUserList:
    """
    Response model for proxy user list.

    Attributes:
        limit: Total traffic limit (KB).
        remaining_limit: Remaining traffic limit (KB).
        user_count: Number of users.
        users: List of proxy users.
    """

    limit: float
    remaining_limit: float
    user_count: int
    users: List[ProxyUser]

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ProxyUserList":
        """Create from API response dict."""
        user_list = data.get("list", [])
        users = [ProxyUser.from_dict(u) for u in user_list]

        return cls(
            limit=float(data.get("limit", 0)),
            remaining_limit=float(data.get("remaining_limit", 0)),
            user_count=int(data.get("user_count", len(users))),
            users=users,
        )


@dataclass
class ProxyServer:
    """
    ISP or Datacenter proxy server information.

    Attributes:
        ip: Proxy server IP address.
        port: Proxy server port.
        username: Authentication username.
        password: Authentication password.
        expiration_time: Expiration timestamp (Unix timestamp or datetime string).
        region: Server region (optional).
    """

    ip: str
    port: int
    username: str
    password: str
    expiration_time: Optional[Union[int, str]] = None
    region: Optional[str] = None

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ProxyServer":
        """Create from API response dict."""
        return cls(
            ip=data.get("ip", ""),
            port=int(data.get("port", 0)),
            username=data.get("username", data.get("user", "")),
            password=data.get("password", data.get("pwd", "")),
            expiration_time=data.get("expiration_time", data.get("expireTime")),
            region=data.get("region"),
        )

    def to_proxy_url(self, protocol: str = "http") -> str:
        """
        Build proxy URL for this server.

        Args:
            protocol: Proxy protocol (http/https/socks5).

        Returns:
            Complete proxy URL.
        """
        return f"{protocol}://{self.username}:{self.password}@{self.ip}:{self.port}"

    def is_expired(self) -> bool:
        """Check if proxy has expired (if expiration_time is available)."""
        if self.expiration_time is None:
            return False

        import time

        if isinstance(self.expiration_time, int):
            return time.time() > self.expiration_time

        # String timestamp handling would need datetime parsing
        return False


========================================================================
FILE: src\thordata\retry.py
SIZE: 11889
TRUNCATED: no
========================================================================
"""
Retry mechanism for the Thordata Python SDK.

This module provides configurable retry logic for handling transient failures
in API requests, with support for exponential backoff and jitter.

Example:
    >>> from thordata.retry import RetryConfig, with_retry
    >>>
    >>> config = RetryConfig(max_retries=3, backoff_factor=1.0)
    >>>
    >>> @with_retry(config)
    >>> def make_request():
    ...     return requests.get("https://api.example.com")
"""

from __future__ import annotations

import inspect
import logging
import random
import time
from dataclasses import dataclass, field
from functools import wraps
from typing import Any, Callable, Optional, Set, Tuple

from .exceptions import (
    ThordataNetworkError,
    ThordataRateLimitError,
    ThordataServerError,
    is_retryable_exception,
)

logger = logging.getLogger(__name__)


@dataclass
class RetryConfig:
    """
    Configuration for retry behavior.

    Attributes:
        max_retries: Maximum number of retry attempts (default: 3).
        backoff_factor: Multiplier for exponential backoff (default: 1.0).
            Wait time = backoff_factor * (2 ** attempt_number)
        max_backoff: Maximum wait time in seconds (default: 60).
        jitter: Add random jitter to prevent thundering herd (default: True).
        jitter_factor: Maximum jitter as fraction of wait time (default: 0.1).
        retry_on_status_codes: HTTP status codes to retry on.
        retry_on_exceptions: Exception types to retry on.

    Example:
        >>> config = RetryConfig(
        ...     max_retries=5,
        ...     backoff_factor=2.0,
        ...     max_backoff=120
        ... )
    """

    max_retries: int = 3
    backoff_factor: float = 1.0
    max_backoff: float = 60.0
    jitter: bool = True
    jitter_factor: float = 0.1

    # Status codes to retry on (5xx server errors + 429 rate limit)
    retry_on_status_codes: Set[int] = field(
        default_factory=lambda: {429, 500, 502, 503, 504}
    )
    retry_on_api_codes: Set[int] = field(
        default_factory=lambda: {300}  # API response body code
    )

    # Exception types to always retry on
    retry_on_exceptions: Tuple[type, ...] = field(
        default_factory=lambda: (
            ThordataNetworkError,
            ThordataServerError,
        )
    )

    def calculate_delay(self, attempt: int) -> float:
        """
        Calculate the delay before the next retry attempt.

        Args:
            attempt: Current attempt number (0-indexed).

        Returns:
            Delay in seconds.
        """
        # Exponential backoff
        delay = self.backoff_factor * (2**attempt)

        # Apply maximum cap
        delay = min(delay, self.max_backoff)

        # Add jitter if enabled
        if self.jitter:
            jitter_range = delay * self.jitter_factor
            delay += random.uniform(-jitter_range, jitter_range)
            delay = max(0.1, delay)  # Ensure positive delay

        return delay

    def should_retry(
        self, exception: Exception, attempt: int, status_code: Optional[int] = None
    ) -> bool:
        """
        Determine if a request should be retried.

        Args:
            exception: The exception that was raised.
            attempt: Current attempt number.
            status_code: HTTP status code if available.

        Returns:
            True if the request should be retried.
        """
        # Check if we've exceeded max retries
        if attempt >= self.max_retries:
            return False

        # Check status code
        if status_code and status_code in self.retry_on_status_codes:
            return True

        # Check exception type
        if isinstance(exception, self.retry_on_exceptions):
            return True

        # Check rate limit with retry_after
        if isinstance(exception, ThordataRateLimitError):
            return True

        # Use generic retryable check
        return is_retryable_exception(exception)


def with_retry(
    config: Optional[RetryConfig] = None,
    on_retry: Optional[Callable[[int, Exception, float], None]] = None,
) -> Callable:
    """
    Decorator to add retry logic to a function.

    Args:
        config: Retry configuration. Uses defaults if not provided.
        on_retry: Optional callback called before each retry.
            Receives (attempt, exception, delay).

    Returns:
        Decorated function with retry logic.

    Example:
        >>> @with_retry(RetryConfig(max_retries=3))
        ... def fetch_data():
        ...     return requests.get("https://api.example.com")

        >>> @with_retry()
        ... async def async_fetch():
        ...     async with aiohttp.ClientSession() as session:
        ...         return await session.get("https://api.example.com")
    """
    if config is None:
        config = RetryConfig()

    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def sync_wrapper(*args: Any, **kwargs: Any) -> Any:
            last_exception: Optional[Exception] = None

            for attempt in range(config.max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e

                    status_code = _extract_status_code(e)

                    if not config.should_retry(e, attempt, status_code):
                        raise

                    delay = config.calculate_delay(attempt)

                    if isinstance(e, ThordataRateLimitError) and e.retry_after:
                        delay = max(delay, e.retry_after)

                    logger.warning(
                        f"Retry attempt {attempt + 1}/{config.max_retries} "
                        f"after {delay:.2f}s due to: {e}"
                    )

                    if on_retry:
                        on_retry(attempt, e, delay)

                    time.sleep(delay)

            if last_exception:
                raise last_exception
            raise RuntimeError("Unexpected retry loop exit")

        @wraps(func)
        async def async_wrapper(*args: Any, **kwargs: Any) -> Any:

            last_exception: Optional[Exception] = None

            for attempt in range(config.max_retries + 1):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    last_exception = e

                    status_code = _extract_status_code(e)

                    if not config.should_retry(e, attempt, status_code):
                        raise

                    delay = config.calculate_delay(attempt)

                    if isinstance(e, ThordataRateLimitError) and e.retry_after:
                        delay = max(delay, e.retry_after)

                    logger.warning(
                        f"Async retry attempt {attempt + 1}/{config.max_retries} "
                        f"after {delay:.2f}s due to: {e}"
                    )

                    if on_retry:
                        on_retry(attempt, e, delay)

                    await asyncio.sleep(delay)

            if last_exception:
                raise last_exception
            raise RuntimeError("Unexpected retry loop exit")

        # Check if the function is async
        import asyncio

        if inspect.iscoroutinefunction(func):
            return async_wrapper
        return sync_wrapper

    return decorator


def _extract_status_code(exception: Exception) -> Optional[int]:
    """
    Extract HTTP status code from various exception types.

    Args:
        exception: The exception to extract from.

    Returns:
        HTTP status code if found, None otherwise.
    """
    # Unwrap nested/original errors (e.g., ThordataNetworkError(original_error=...))
    if hasattr(exception, "original_error") and exception.original_error:
        nested = exception.original_error
        if isinstance(nested, Exception):
            nested_code = _extract_status_code(nested)
            if nested_code is not None:
                return nested_code

    # Check Thordata exceptions
    if hasattr(exception, "status_code"):
        return exception.status_code
    if hasattr(exception, "code"):
        return exception.code

    # Check requests exceptions
    if hasattr(exception, "response"):
        response = exception.response
        if response is not None and hasattr(response, "status_code"):
            return response.status_code

    # Check aiohttp exceptions
    if hasattr(exception, "status"):
        return exception.status

    return None


class RetryableRequest:
    """
    Context manager for retryable requests with detailed control.

    This provides more control than the decorator approach, allowing
    you to check retry status during execution.

    Example:
        >>> config = RetryConfig(max_retries=3)
        >>> with RetryableRequest(config) as retry:
        ...     while True:
        ...         try:
        ...             response = requests.get("https://api.example.com")
        ...             response.raise_for_status()
        ...             break
        ...         except Exception as e:
        ...             if not retry.should_continue(e):
        ...                 raise
        ...             retry.wait()
    """

    def __init__(self, config: Optional[RetryConfig] = None) -> None:
        self.config = config or RetryConfig()
        self.attempt = 0
        self.last_exception: Optional[Exception] = None

    def __enter__(self) -> RetryableRequest:
        return self

    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        pass

    def should_continue(
        self, exception: Exception, status_code: Optional[int] = None
    ) -> bool:
        """
        Check if we should continue retrying.

        Args:
            exception: The exception that occurred.
            status_code: HTTP status code if available.

        Returns:
            True if we should retry, False otherwise.
        """
        self.last_exception = exception

        if status_code is None:
            status_code = _extract_status_code(exception)

        should_retry = self.config.should_retry(exception, self.attempt, status_code)

        if should_retry:
            self.attempt += 1

        return should_retry

    def wait(self) -> float:
        """
        Wait before the next retry attempt.

        Returns:
            The actual delay used.
        """
        delay = self.config.calculate_delay(self.attempt - 1)

        # Handle rate limit retry_after
        if (
            isinstance(self.last_exception, ThordataRateLimitError)
            and self.last_exception.retry_after
        ):
            delay = max(delay, self.last_exception.retry_after)

        logger.debug(f"Waiting {delay:.2f}s before retry {self.attempt}")
        time.sleep(delay)

        return delay

    async def async_wait(self) -> float:
        """
        Async version of wait().

        Returns:
            The actual delay used.
        """
        import asyncio

        delay = self.config.calculate_delay(self.attempt - 1)

        if (
            isinstance(self.last_exception, ThordataRateLimitError)
            and self.last_exception.retry_after
        ):
            delay = max(delay, self.last_exception.retry_after)

        logger.debug(f"Async waiting {delay:.2f}s before retry {self.attempt}")
        await asyncio.sleep(delay)

        return delay


========================================================================
FILE: src\thordata\_utils.py
SIZE: 4888
TRUNCATED: no
========================================================================
"""
Internal utility functions for the Thordata Python SDK.

These are not part of the public API and may change without notice.
"""

from __future__ import annotations

import base64
import json
import logging
from typing import Any, Dict

logger = logging.getLogger(__name__)


def parse_json_response(data: Any) -> Any:
    """
    Parse a response that might be double-encoded JSON.

    Some API endpoints return JSON as a string inside JSON.

    Args:
        data: The response data to parse.

    Returns:
        Parsed data.
    """
    if isinstance(data, str):
        try:
            return json.loads(data)
        except json.JSONDecodeError:
            return data
    return data


def decode_base64_image(png_str: str) -> bytes:
    """
    Decode a base64-encoded PNG image.

    Handles Data URI scheme (data:image/png;base64,...) and fixes padding.

    Args:
        png_str: Base64-encoded string, possibly with Data URI prefix.

    Returns:
        Decoded PNG bytes.

    Raises:
        ValueError: If the string is empty or cannot be decoded.
    """
    if not png_str:
        raise ValueError("Empty PNG data received")

    # Remove Data URI scheme if present
    if "," in png_str:
        png_str = png_str.split(",", 1)[1]

    # Clean up whitespace
    png_str = png_str.replace("\n", "").replace("\r", "").replace(" ", "")

    # Fix Base64 padding
    missing_padding = len(png_str) % 4
    if missing_padding:
        png_str += "=" * (4 - missing_padding)

    try:
        return base64.b64decode(png_str)
    except Exception as e:
        raise ValueError(f"Failed to decode base64 image: {e}") from e


def build_auth_headers(token: str, mode: str = "bearer") -> Dict[str, str]:
    """
    Build authorization headers for API requests.

    Supports two modes:
    - bearer: Authorization: Bearer <token> (Thordata Docs examples)
    - header_token: token: <token> (Interface documentation)

    Args:
        token: The scraper token.
        mode: Authentication mode ("bearer" or "header_token").

    Returns:
        Headers dict with Authorization/token and Content-Type.
    """
    headers = {
        "Content-Type": "application/x-www-form-urlencoded",
    }

    if mode == "bearer":
        headers["Authorization"] = f"Bearer {token}"
    elif mode == "header_token":
        headers["token"] = token
    else:
        # Fallback to bearer for compatibility
        headers["Authorization"] = f"Bearer {token}"

    return headers


def build_builder_headers(
    scraper_token: str,
    public_token: str,
    public_key: str,
) -> Dict[str, str]:
    """
    Build headers for Web Scraper builder API.

    Builder requires THREE auth headers per official docs:
    - token: public token
    - key: public key
    - Authorization: Bearer scraper_token

    Args:
        scraper_token: The scraper API token.
        public_token: The public API token.
        public_key: The public API key.

    Returns:
        Headers dict with all required auth headers.
    """
    return {
        "token": public_token,
        "key": public_key,
        "Authorization": f"Bearer {scraper_token}",
        "Content-Type": "application/x-www-form-urlencoded",
    }


def build_public_api_headers(public_token: str, public_key: str) -> Dict[str, str]:
    """
    Build headers for public API requests (task status, locations, etc.)

    Args:
        public_token: The public API token.
        public_key: The public API key.

    Returns:
        Headers dict with token, key, and Content-Type.
    """
    return {
        "token": public_token,
        "key": public_key,
        "Content-Type": "application/x-www-form-urlencoded",
    }


def extract_error_message(payload: Any) -> str:
    """
    Extract a human-readable error message from an API response.

    Args:
        payload: The API response payload.

    Returns:
        Error message string.
    """
    if isinstance(payload, dict):
        # Try common error message fields
        for key in ("msg", "message", "error", "detail", "description"):
            if key in payload:
                return str(payload[key])

        # Fall back to full payload
        return str(payload)

    return str(payload)


def build_user_agent(sdk_version: str, http_client: str) -> str:
    """
    Build a default User-Agent for the SDK.

    Args:
        sdk_version: SDK version string.
        http_client: "requests" or "aiohttp" (or any identifier).

    Returns:
        A User-Agent string.
    """
    import platform

    py = platform.python_version()
    system = platform.system()
    return f"thordata-python-sdk/{sdk_version} (python {py}; {system}; {http_client})"


========================================================================
FILE: src\thordata\__init__.py
SIZE: 3346
TRUNCATED: no
========================================================================
"""
Thordata Python SDK

Official Python client for Thordata's Proxy Network, SERP API,
Universal Scraping API (Web Unlocker), and Web Scraper API.

Basic Usage:
    >>> from thordata import ThordataClient
    >>>
    >>> client = ThordataClient(
    ...     scraper_token="your_token",
    ...     public_token="your_public_token",
    ...     public_key="your_public_key"
    ... )
    >>>
    >>> # Proxy request
    >>> response = client.get("https://httpbin.org/ip")
    >>>
    >>> # SERP search
    >>> results = client.serp_search("python tutorial", engine="google")
    >>>
    >>> # Universal scrape
    >>> html = client.universal_scrape("https://example.com", js_render=True)

Async Usage:
    >>> from thordata import AsyncThordataClient
    >>> import asyncio
    >>>
    >>> async def main():
    ...     async with AsyncThordataClient(
    ...         scraper_token="your_token"
    ...     ) as client:
    ...         response = await client.get("https://httpbin.org/ip")
    >>>
    >>> asyncio.run(main())
"""

__version__ = "0.8.0"
__author__ = "Thordata Developer Team"
__email__ = "support@thordata.com"

# Main clients
from .async_client import AsyncThordataClient
from .client import ThordataClient

# Enums
from .enums import (
    BingSearchType,
    Continent,
    Country,
    DataFormat,
    Device,
    Engine,
    GoogleSearchType,
    GoogleTbm,
    OutputFormat,
    ProxyHost,
    ProxyPort,
    ProxyType,
    SessionType,
    TaskStatus,
    TimeRange,
)

# Exceptions
from .exceptions import (
    ThordataAPIError,
    ThordataAuthError,
    ThordataConfigError,
    ThordataError,
    ThordataNetworkError,
    ThordataNotCollectedError,
    ThordataRateLimitError,
    ThordataServerError,
    ThordataTimeoutError,
    ThordataValidationError,
)

# Models
from .models import (
    CommonSettings,
    ProxyConfig,
    ProxyProduct,
    ProxyServer,
    ProxyUser,
    ProxyUserList,
    ScraperTaskConfig,
    SerpRequest,
    StaticISPProxy,
    StickySession,
    TaskStatusResponse,
    UniversalScrapeRequest,
    UsageStatistics,
    VideoTaskConfig,
)

# Retry utilities
from .retry import RetryConfig

# Public API
__all__ = [
    # Version
    "__version__",
    # Clients
    "ThordataClient",
    "AsyncThordataClient",
    # Enums
    "Engine",
    "GoogleSearchType",
    "BingSearchType",
    "ProxyType",
    "SessionType",
    "Continent",
    "Country",
    "OutputFormat",
    "DataFormat",
    "TaskStatus",
    "Device",
    "TimeRange",
    "ProxyHost",
    "ProxyPort",
    "GoogleTbm",
    # Models
    "ProxyConfig",
    "ProxyProduct",
    "ProxyServer",
    "ProxyUser",
    "ProxyUserList",
    "UsageStatistics",
    "StaticISPProxy",
    "StickySession",
    "SerpRequest",
    "UniversalScrapeRequest",
    "ScraperTaskConfig",
    "CommonSettings",
    "VideoTaskConfig",
    "TaskStatusResponse",
    # Exceptions
    "ThordataError",
    "ThordataConfigError",
    "ThordataNetworkError",
    "ThordataTimeoutError",
    "ThordataAPIError",
    "ThordataAuthError",
    "ThordataRateLimitError",
    "ThordataServerError",
    "ThordataValidationError",
    "ThordataNotCollectedError",
    # Retry
    "RetryConfig",
]


========================================================================
FILE: tests\conftest.py
SIZE: 1365
TRUNCATED: no
========================================================================
"""
Pytest configuration and fixtures for Thordata SDK tests.
"""

from unittest.mock import MagicMock, patch

import pytest

from thordata import ThordataClient


@pytest.fixture
def mock_credentials():
    """Provide test credentials."""
    return {
        "scraper_token": "test_scraper_token",
        "public_token": "test_public_token",
        "public_key": "test_public_key",
    }


@pytest.fixture
def mock_response():
    """Create a mock requests.Response object."""
    response = MagicMock()
    response.status_code = 200
    response.raise_for_status = MagicMock()
    response.json.return_value = {"code": 200, "data": {}}
    response.text = "<html></html>"
    response.content = b"<html></html>"
    return response


@pytest.fixture
def mock_session(mock_response):
    """Create a mock requests.Session object."""
    session = MagicMock()
    session.get.return_value = mock_response
    session.post.return_value = mock_response
    session.request.return_value = mock_response
    return session


@pytest.fixture
def client(mock_credentials):
    """Create a ThordataClient for testing."""
    return ThordataClient(
        scraper_token=mock_credentials["scraper_token"],
        public_token=mock_credentials["public_token"],
        public_key=mock_credentials["public_key"],
    )


========================================================================
FILE: tests\test_async_client.py
SIZE: 2546
TRUNCATED: no
========================================================================
"""
Tests for AsyncThordataClient.
"""

import aiohttp
import pytest

# check aioresponses
try:
    from aioresponses import aioresponses

    HAS_AIORESPONSES = True
except ImportError:
    HAS_AIORESPONSES = False

from thordata import AsyncThordataClient
from thordata.exceptions import ThordataConfigError
from thordata.models import ProxyConfig, ProxyProduct

# Mark all tests in this module as async
pytestmark = pytest.mark.asyncio

# Mock Credentials
TEST_SCRAPER = "async_scraper_token"
TEST_PUB_TOKEN = "async_public_token"
TEST_PUB_KEY = "async_key"


def _https_proxy_config_dummy() -> ProxyConfig:
    # Dummy values are fine because AsyncThordataClient will block before any network call
    return ProxyConfig(
        username="dummy",
        password="dummy",
        product=ProxyProduct.RESIDENTIAL,
        protocol="https",
        host="vpn_dummy.pr.thordata.net",
        port=9999,
    )


@pytest.fixture
async def async_client():
    """Fixture for AsyncThordataClient with context management."""
    client = AsyncThordataClient(
        scraper_token=TEST_SCRAPER,
        public_token=TEST_PUB_TOKEN,
        public_key=TEST_PUB_KEY,
    )
    async with client:
        yield client


async def test_async_client_initialization(async_client):
    """Test async client properties."""
    assert async_client.scraper_token == TEST_SCRAPER
    assert async_client.public_token == TEST_PUB_TOKEN
    assert async_client.public_key == TEST_PUB_KEY

    # The fixture likely enters async context, so session should exist
    assert async_client._session is not None
    assert not async_client._session.closed


async def test_async_proxy_network_https_not_supported():
    async with AsyncThordataClient(scraper_token="test_token") as client:
        with pytest.raises(ThordataConfigError) as exc:
            await client.get(
                "https://httpbin.org/ip",
                proxy_config=_https_proxy_config_dummy(),
            )

        assert "Proxy Network requires an HTTPS proxy endpoint" in str(exc.value)


async def test_async_http_error_handling():
    async with AsyncThordataClient(scraper_token="test_token") as client:
        with pytest.raises(ThordataConfigError) as exc:
            await client.get(
                "https://httpbin.org/status/404",
                proxy_config=_https_proxy_config_dummy(),
            )

        assert "Proxy Network requires an HTTPS proxy endpoint" in str(exc.value)


========================================================================
FILE: tests\test_async_client_errors.py
SIZE: 3144
TRUNCATED: no
========================================================================
"""
Tests for AsyncThordataClient error handling.
"""

from typing import Any, Dict
from unittest.mock import AsyncMock, MagicMock, PropertyMock, patch

import pytest

from thordata import (
    AsyncThordataClient,
    ThordataAuthError,
    ThordataRateLimitError,
)


class DummyAsyncResponse:
    """
    Minimal async fake response object for aiohttp.
    """

    def __init__(self, json_data: Dict[str, Any], status: int = 200) -> None:
        self._json_data = json_data
        self.status = status

    async def __aenter__(self) -> "DummyAsyncResponse":
        return self

    async def __aexit__(self, exc_type, exc, tb) -> None:
        return None

    def raise_for_status(self) -> None:
        pass

    async def json(self) -> Dict[str, Any]:
        return self._json_data

    async def read(self) -> bytes:
        return b""

    async def text(self) -> str:
        import json

        return json.dumps(self._json_data)


@pytest.mark.asyncio
async def test_async_universal_scrape_rate_limit_error() -> None:
    """
    When Universal API returns JSON with code=402, the async client should raise
    ThordataRateLimitError.
    """
    client = AsyncThordataClient(
        scraper_token="SCRAPER_TOKEN",
        public_token="PUBLIC_TOKEN",
        public_key="PUBLIC_KEY",
    )

    # Create a mock session with closed=False
    mock_session = MagicMock()
    mock_session.closed = False  # Explicitly set closed to False
    mock_response = DummyAsyncResponse({"code": 402, "msg": "Insufficient balance"})
    mock_session.post.return_value = mock_response

    # Manually set the session
    client._session = mock_session

    with pytest.raises(ThordataRateLimitError) as exc_info:
        await client.universal_scrape("https://example.com")

    err = exc_info.value
    assert err.code == 402
    assert isinstance(err.payload, dict)
    assert err.payload.get("msg") == "Insufficient balance"


@pytest.mark.asyncio
async def test_async_create_scraper_task_auth_error() -> None:
    """
    When Web Scraper API returns JSON with code=401, the async client should raise
    ThordataAuthError.
    """
    client = AsyncThordataClient(
        scraper_token="SCRAPER_TOKEN",
        public_token="PUBLIC_TOKEN",
        public_key="PUBLIC_KEY",
    )

    # Create a mock session with closed=False
    mock_session = MagicMock()
    mock_session.closed = False  # Explicitly set closed to False
    mock_response = DummyAsyncResponse({"code": 401, "msg": "Unauthorized"})
    mock_session.post.return_value = mock_response

    # Manually set the session
    client._session = mock_session

    with pytest.raises(ThordataAuthError) as exc_info:
        await client.create_scraper_task(
            file_name="test.json",
            spider_id="dummy-spider",
            spider_name="example.com",
            parameters={"foo": "bar"},
        )

    err = exc_info.value
    assert err.code == 401
    assert isinstance(err.payload, dict)
    assert err.payload.get("msg") == "Unauthorized"


========================================================================
FILE: tests\test_client.py
SIZE: 3789
TRUNCATED: no
========================================================================
"""
Tests for thordata.client module.
"""

from unittest.mock import MagicMock, patch

import pytest

from thordata import ThordataClient
from thordata.exceptions import ThordataConfigError


class TestClientInitialization:
    """Tests for ThordataClient initialization."""

    def test_basic_init(self):
        """Test basic client initialization."""
        client = ThordataClient(scraper_token="test_token")
        assert client.scraper_token == "test_token"
        assert client.public_token is None
        assert client.public_key is None

    def test_full_init(self):
        """Test client initialization with all parameters."""
        client = ThordataClient(
            scraper_token="scraper",
            public_token="public",
            public_key="key",
            timeout=60,
        )
        assert client.scraper_token == "scraper"
        assert client.public_token == "public"
        assert client.public_key == "key"

    def test_missing_scraper_token(self):
        """Test that missing scraper_token raises error."""
        with pytest.raises(ThordataConfigError, match="scraper_token is required"):
            ThordataClient(scraper_token="")

    def test_context_manager(self):
        """Test client as context manager."""
        with ThordataClient(scraper_token="test") as client:
            assert client is not None


class TestClientMethods:
    """Tests for ThordataClient methods."""

    @pytest.fixture
    def client(self):
        """Create a client for testing."""
        return ThordataClient(
            scraper_token="test_token",
            public_token="pub_token",
            public_key="pub_key",
        )

    def test_build_proxy_url(self, client):
        """Test build_proxy_url method."""
        url = client.build_proxy_url(
            username="testuser",
            password="testpass",
            country="us",
            city="seattle",
        )
        assert "td-customer-testuser" in url
        assert "country-us" in url
        assert "city-seattle" in url
        assert "testpass" in url

    @patch.object(ThordataClient, "_get_locations")
    def test_list_countries(self, mock_get_locations, client):
        """Test list_countries method."""
        mock_get_locations.return_value = [
            {"country_code": "us", "country_name": "United States"},
        ]

        result = client.list_countries()

        mock_get_locations.assert_called_once()
        assert len(result) == 1
        assert result[0]["country_code"] == "us"

    def test_require_public_credentials(self):
        """Test that methods requiring public credentials raise error."""
        client = ThordataClient(scraper_token="test")

        with pytest.raises(ThordataConfigError, match="public_token and public_key"):
            client.get_task_status("some_task_id")

    def test_list_tasks(self, client):
        """Test list_tasks method."""
        # Mock the _api_request_with_retry method directly
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.raise_for_status = MagicMock()
        mock_response.json.return_value = {
            "code": 200,
            "data": {
                "count": 5,
                "list": [
                    {"task_id": "task_1", "status": "ready"},
                    {"task_id": "task_2", "status": "running"},
                ],
            },
        }

        with patch.object(
            client, "_api_request_with_retry", return_value=mock_response
        ):
            result = client.list_tasks(page=1, size=10)

        assert result["count"] == 5
        assert len(result["list"]) == 2


========================================================================
FILE: tests\test_client_errors.py
SIZE: 2653
TRUNCATED: no
========================================================================
"""
Tests for ThordataClient error handling.
"""

from typing import Any, Dict
from unittest.mock import MagicMock, patch

import pytest
import requests

from thordata import (
    ThordataAuthError,
    ThordataClient,
    ThordataRateLimitError,
)


class DummyResponse:
    """
    Minimal fake Response object for testing.
    """

    def __init__(self, json_data: Dict[str, Any], status_code: int = 200) -> None:
        self._json_data = json_data
        self.status_code = status_code

    def raise_for_status(self) -> None:
        if 400 <= self.status_code:
            raise requests.HTTPError(response=self)

    def json(self) -> Dict[str, Any]:
        return self._json_data

    @property
    def text(self) -> str:
        import json

        return json.dumps(self._json_data)

    @property
    def content(self) -> bytes:
        return b""


def _make_client() -> ThordataClient:
    """Create a test client with dummy tokens."""
    return ThordataClient(
        scraper_token="SCRAPER_TOKEN",
        public_token="PUBLIC_TOKEN",
        public_key="PUBLIC_KEY",
    )


def test_universal_scrape_rate_limit_error() -> None:
    """
    When Universal API returns JSON with code=402, the client should raise
    ThordataRateLimitError instead of a generic Exception.
    """
    client = _make_client()

    mock_response = DummyResponse({"code": 402, "msg": "Insufficient balance"})

    with patch.object(client, "_api_request_with_retry", return_value=mock_response):
        with pytest.raises(ThordataRateLimitError) as exc_info:
            client.universal_scrape("https://example.com")

    err = exc_info.value
    assert err.code == 402
    assert isinstance(err.payload, dict)
    assert err.payload.get("msg") == "Insufficient balance"


def test_create_scraper_task_auth_error() -> None:
    """
    When Web Scraper API returns JSON with code=401, the client should raise
    ThordataAuthError.
    """
    client = _make_client()

    mock_response = DummyResponse({"code": 401, "msg": "Unauthorized"})

    with patch.object(client, "_api_request_with_retry", return_value=mock_response):
        with pytest.raises(ThordataAuthError) as exc_info:
            client.create_scraper_task(
                file_name="test.json",
                spider_id="dummy-spider",
                spider_name="example.com",
                parameters={"foo": "bar"},
            )

    err = exc_info.value
    assert err.code == 401
    assert isinstance(err.payload, dict)
    assert err.payload.get("msg") == "Unauthorized"


========================================================================
FILE: tests\test_enums.py
SIZE: 4016
TRUNCATED: no
========================================================================
"""
Tests for thordata.enums module.
"""

import pytest

from thordata.enums import (
    Continent,
    Country,
    Engine,
    GoogleSearchType,
    ProxyType,
    TaskStatus,
    normalize_enum_value,
)
from thordata.models import ProxyProduct  # ProxyProduct åœ¨ models.py ä¸­


class TestEngine:
    """Tests for Engine enum."""

    def test_engine_values(self):
        """Test that engine values are lowercase strings."""
        assert Engine.GOOGLE.value == "google"
        assert Engine.BING.value == "bing"
        assert Engine.YANDEX.value == "yandex"

    def test_engine_is_str(self):
        """Test that Engine inherits from str."""
        assert isinstance(Engine.GOOGLE, str)
        assert Engine.GOOGLE == "google"


class TestGoogleSearchType:
    """Tests for GoogleSearchType enum."""

    def test_search_types(self):
        """Test Google search type values."""
        assert GoogleSearchType.SEARCH.value == "search"
        assert GoogleSearchType.SHOPPING.value == "shopping"
        assert GoogleSearchType.NEWS.value == "news"
        assert GoogleSearchType.IMAGES.value == "images"


class TestProxyType:
    """Tests for ProxyType enum."""

    def test_proxy_type_values(self):
        """Test proxy type integer values."""
        assert ProxyType.RESIDENTIAL == 1
        assert ProxyType.UNLIMITED == 2
        assert ProxyType.DATACENTER == 3


class TestProxyProduct:
    """Tests for ProxyProduct enum."""

    def test_default_ports(self):
        """Test default ports for each product."""
        assert ProxyProduct.RESIDENTIAL.default_port == 9999
        assert ProxyProduct.MOBILE.default_port == 5555
        assert ProxyProduct.DATACENTER.default_port == 7777
        assert ProxyProduct.ISP.default_port == 6666


class TestContinent:
    """Tests for Continent enum."""

    def test_continent_codes(self):
        """Test continent codes."""
        assert Continent.ASIA.value == "as"
        assert Continent.EUROPE.value == "eu"
        assert Continent.NORTH_AMERICA.value == "na"


class TestCountry:
    """Tests for Country enum."""

    def test_country_codes(self):
        """Test country codes are lowercase."""
        assert Country.US.value == "us"
        assert Country.GB.value == "gb"
        assert Country.JP.value == "jp"


class TestTaskStatus:
    """Tests for TaskStatus enum."""

    def test_is_terminal(self):
        """Test is_terminal method."""
        assert TaskStatus.is_terminal(TaskStatus.READY) is True
        assert TaskStatus.is_terminal(TaskStatus.SUCCESS) is True
        assert TaskStatus.is_terminal(TaskStatus.FAILED) is True
        assert TaskStatus.is_terminal(TaskStatus.RUNNING) is False
        assert TaskStatus.is_terminal(TaskStatus.PENDING) is False

    def test_is_success(self):
        """Test is_success method."""
        assert TaskStatus.is_success(TaskStatus.READY) is True
        assert TaskStatus.is_success(TaskStatus.SUCCESS) is True
        assert TaskStatus.is_success(TaskStatus.FAILED) is False

    def test_is_failure(self):
        """Test is_failure method."""
        assert TaskStatus.is_failure(TaskStatus.FAILED) is True
        assert TaskStatus.is_failure(TaskStatus.ERROR) is True
        assert TaskStatus.is_failure(TaskStatus.SUCCESS) is False


class TestNormalizeEnumValue:
    """Tests for normalize_enum_value function."""

    def test_with_enum(self):
        """Test with enum value."""
        result = normalize_enum_value(Engine.GOOGLE, Engine)
        assert result == "google"

    def test_with_string(self):
        """Test with string value."""
        result = normalize_enum_value("GOOGLE", Engine)
        assert result == "google"

    def test_with_invalid_type(self):
        """Test with invalid type."""
        with pytest.raises(TypeError, match="Expected Engine or str"):
            normalize_enum_value(123, Engine)


========================================================================
FILE: tests\test_examples.py
SIZE: 6284
TRUNCATED: no
========================================================================
"""
Integration tests for example scripts.

These tests run example scripts against a local mock server to verify
they execute without errors. They don't require real API credentials.
"""

import base64
import json
import os
import subprocess
import sys
from urllib.parse import parse_qs

import pytest
from pytest_httpserver import HTTPServer
from werkzeug.wrappers import Request, Response

# 1x1 transparent PNG for testing
PNG_1X1_BASE64 = (
    "iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mP8/x8AAwMB/"
    "6Xn2mQAAAAASUVORK5CYII="
)


class TestExampleScripts:
    """Test that example scripts run without errors."""

    @pytest.fixture
    def base_env(self):
        """Base environment variables."""
        env = os.environ.copy()
        env.update(
            {
                "THORDATA_SCRAPER_TOKEN": "test_token",
                "THORDATA_PUBLIC_TOKEN": "test_public",
                "THORDATA_PUBLIC_KEY": "test_key",
                "PYTHONIOENCODING": "utf-8",
                "PYTHONUTF8": "1",
                "NO_PROXY": "127.0.0.1,localhost",
                "no_proxy": "127.0.0.1,localhost",
            }
        )
        return env

    def _run_script(
        self, script_path: str, env: dict, timeout: int = 60
    ) -> subprocess.CompletedProcess:
        """Run a Python script and return the result."""
        return subprocess.run(
            [sys.executable, script_path],
            env=env,
            capture_output=True,
            text=True,
            encoding="utf-8",
            errors="replace",
            timeout=timeout,
        )

    def test_demo_serp_api(self, base_env, httpserver: HTTPServer):
        """Test demo_serp_api.py runs without errors."""

        def serp_handler(request: Request) -> Response:
            body = request.get_data(as_text=True) or ""
            form = parse_qs(body)
            engine = (form.get("engine") or [""])[0]

            if "shopping" in engine:
                payload = {
                    "code": 200,
                    "shopping": [{"title": "Test Laptop", "price": "$999"}],
                }
            elif "news" in engine:
                payload = {
                    "code": 200,
                    "news_results": [{"title": "Test News", "source": "Test"}],
                }
            else:
                payload = {
                    "code": 200,
                    "organic": [
                        {"title": "Test Result", "link": "https://example.com"}
                    ],
                }

            return Response(
                json.dumps(payload), status=200, content_type="application/json"
            )

        httpserver.expect_request("/request", method="POST").respond_with_handler(
            serp_handler
        )

        base_url = httpserver.url_for("/").rstrip("/").replace("localhost", "127.0.0.1")
        env = base_env.copy()
        env["THORDATA_SCRAPERAPI_BASE_URL"] = base_url

        result = self._run_script("examples/demo_serp_api.py", env)
        assert result.returncode == 0, f"{result.stdout}\n{result.stderr}"

    def test_demo_universal(self, base_env, httpserver: HTTPServer):
        """Test demo_universal.py runs without errors."""

        def universal_handler(request: Request) -> Response:
            body = request.get_data(as_text=True) or ""
            form = parse_qs(body)

            # Check if PNG format requested
            req_type = (form.get("type") or ["html"])[0].lower()

            if req_type == "png":
                payload = {"code": 200, "png": PNG_1X1_BASE64}
            else:
                payload = {"code": 200, "html": "<html><body>Test</body></html>"}

            return Response(
                json.dumps(payload), status=200, content_type="application/json"
            )

        httpserver.expect_request("/request", method="POST").respond_with_handler(
            universal_handler
        )

        base_url = httpserver.url_for("/").rstrip("/").replace("localhost", "127.0.0.1")
        env = base_env.copy()
        env["THORDATA_UNIVERSALAPI_BASE_URL"] = base_url

        result = self._run_script("examples/demo_universal.py", env)
        assert result.returncode == 0, f"{result.stdout}\n{result.stderr}"

    def test_demo_web_scraper_api(self, base_env, httpserver: HTTPServer):
        """Test demo_web_scraper_api.py runs without errors."""
        # Builder endpoint
        httpserver.expect_request("/builder", method="POST").respond_with_json(
            {"code": 200, "data": {"task_id": "test_task_123"}}
        )

        # Status endpoint
        httpserver.expect_request("/tasks-status", method="POST").respond_with_json(
            {"code": 200, "data": [{"task_id": "test_task_123", "status": "ready"}]}
        )

        # Download endpoint
        httpserver.expect_request("/tasks-download", method="POST").respond_with_json(
            {"code": 200, "data": {"download": "https://example.com/result.json"}}
        )

        base_url = httpserver.url_for("/").rstrip("/").replace("localhost", "127.0.0.1")
        env = base_env.copy()
        env["THORDATA_SCRAPERAPI_BASE_URL"] = base_url
        env["THORDATA_WEB_SCRAPER_API_BASE_URL"] = base_url

        result = self._run_script("examples/demo_web_scraper_api.py", env)
        assert result.returncode == 0, f"{result.stdout}\n{result.stderr}"

    def test_async_high_concurrency(self, base_env, httpserver: HTTPServer):
        """Test async_high_concurrency.py runs without errors."""
        httpserver.expect_request("/request", method="POST").respond_with_json(
            {"code": 200, "organic": [{"title": "Test", "link": "https://example.com"}]}
        )

        base_url = httpserver.url_for("/").rstrip("/").replace("localhost", "127.0.0.1")
        env = base_env.copy()
        env["THORDATA_SCRAPERAPI_BASE_URL"] = base_url
        env["THORDATA_CONCURRENCY"] = "3"  # Keep small for tests

        result = self._run_script("examples/async_high_concurrency.py", env)
        assert result.returncode == 0, f"{result.stdout}\n{result.stderr}"


========================================================================
FILE: tests\test_exceptions.py
SIZE: 6134
TRUNCATED: no
========================================================================
"""
Tests for thordata.exceptions module.
"""

import pytest

from thordata.exceptions import (
    ThordataAPIError,
    ThordataAuthError,
    ThordataConfigError,
    ThordataError,
    ThordataNetworkError,
    ThordataRateLimitError,
    ThordataServerError,
    ThordataTimeoutError,
    ThordataValidationError,
    is_retryable_exception,
    raise_for_code,
)


class TestExceptionHierarchy:
    """Tests for exception class hierarchy."""

    def test_base_error(self):
        """Test base ThordataError."""
        err = ThordataError("test message")
        assert str(err) == "test message"
        assert err.message == "test message"

    def test_config_error(self):
        """Test ThordataConfigError."""
        err = ThordataConfigError("missing token")
        assert isinstance(err, ThordataError)

    def test_network_error(self):
        """Test ThordataNetworkError."""
        original = ConnectionError("connection refused")
        err = ThordataNetworkError("network failed", original_error=original)
        assert isinstance(err, ThordataError)
        assert err.original_error is original

    def test_timeout_error(self):
        """Test ThordataTimeoutError."""
        err = ThordataTimeoutError("request timed out")
        assert isinstance(err, ThordataNetworkError)

    def test_api_error(self):
        """Test ThordataAPIError."""
        err = ThordataAPIError(
            "api error",
            status_code=400,
            code=400,
            payload={"error": "bad request"},
        )
        assert isinstance(err, ThordataError)
        assert err.status_code == 400
        assert err.code == 400
        assert err.payload == {"error": "bad request"}

    def test_auth_error(self):
        """Test ThordataAuthError."""
        err = ThordataAuthError("unauthorized", status_code=401)
        assert isinstance(err, ThordataAPIError)
        assert err.is_retryable is False

    def test_rate_limit_error(self):
        """Test ThordataRateLimitError."""
        err = ThordataRateLimitError(
            "rate limited",
            status_code=429,
            retry_after=60,
        )
        assert isinstance(err, ThordataAPIError)
        assert err.retry_after == 60
        assert err.is_retryable is True

    def test_server_error(self):
        """Test ThordataServerError."""
        err = ThordataServerError("internal error", status_code=500)
        assert isinstance(err, ThordataAPIError)
        assert err.is_retryable is True

    def test_validation_error(self):
        """Test ThordataValidationError."""
        err = ThordataValidationError("invalid params", status_code=400)
        assert isinstance(err, ThordataAPIError)
        assert err.is_retryable is False


class TestRaiseForCode:
    """Tests for raise_for_code function."""

    def test_raises_auth_error_for_401(self):
        """Test that 401 raises ThordataAuthError."""
        with pytest.raises(ThordataAuthError):
            raise_for_code("auth failed", status_code=401)

    def test_raises_auth_error_for_403(self):
        """Test that 403 raises ThordataAuthError."""
        with pytest.raises(ThordataAuthError):
            raise_for_code("forbidden", status_code=403)

    def test_raises_rate_limit_error_for_429(self):
        """Test that 429 raises ThordataRateLimitError."""
        with pytest.raises(ThordataRateLimitError):
            raise_for_code("rate limited", status_code=429)

    def test_raises_rate_limit_error_for_402(self):
        """Test that 402 raises ThordataRateLimitError."""
        with pytest.raises(ThordataRateLimitError):
            raise_for_code("payment required", status_code=402)

    def test_raises_server_error_for_500(self):
        """Test that 500 raises ThordataServerError."""
        with pytest.raises(ThordataServerError):
            raise_for_code("server error", status_code=500)

    def test_raises_server_error_for_503(self):
        """Test that 503 raises ThordataServerError."""
        with pytest.raises(ThordataServerError):
            raise_for_code("service unavailable", status_code=503)

    def test_raises_validation_error_for_400(self):
        """Test that 400 raises ThordataValidationError."""
        with pytest.raises(ThordataValidationError):
            raise_for_code("bad request", status_code=400)

    def test_raises_generic_api_error(self):
        """Test that unknown codes raise ThordataAPIError."""
        with pytest.raises(ThordataAPIError):
            raise_for_code("unknown error", status_code=418)


class TestIsRetryableException:
    """Tests for is_retryable_exception function."""

    def test_network_error_is_retryable(self):
        """Test that network errors are retryable."""
        err = ThordataNetworkError("connection failed")
        assert is_retryable_exception(err) is True

    def test_timeout_error_is_retryable(self):
        """Test that timeout errors are retryable."""
        err = ThordataTimeoutError("timed out")
        assert is_retryable_exception(err) is True

    def test_server_error_is_retryable(self):
        """Test that server errors are retryable."""
        err = ThordataServerError("internal error", status_code=500)
        assert is_retryable_exception(err) is True

    def test_rate_limit_error_is_retryable(self):
        """Test that rate limit errors are retryable."""
        err = ThordataRateLimitError("rate limited", status_code=429)
        assert is_retryable_exception(err) is True

    def test_auth_error_is_not_retryable(self):
        """Test that auth errors are not retryable."""
        err = ThordataAuthError("unauthorized", status_code=401)
        assert is_retryable_exception(err) is False

    def test_validation_error_is_not_retryable(self):
        """Test that validation errors are not retryable."""
        err = ThordataValidationError("bad request", status_code=400)
        assert is_retryable_exception(err) is False


========================================================================
FILE: tests\test_models.py
SIZE: 10988
TRUNCATED: no
========================================================================
"""
Tests for thordata.models module.
"""

import pytest

from thordata.models import (
    ProxyConfig,
    ProxyProduct,
    ScraperTaskConfig,
    SerpRequest,
    StickySession,
    UniversalScrapeRequest,
)


class TestProxyConfig:
    """Tests for ProxyConfig dataclass."""

    def test_basic_creation(self):
        """Test basic ProxyConfig creation."""
        config = ProxyConfig(
            username="testuser",
            password="testpass",
        )
        assert config.username == "testuser"
        assert config.password == "testpass"
        assert config.product == ProxyProduct.RESIDENTIAL

    def test_build_username_basic(self):
        """Test username building without options."""
        config = ProxyConfig(username="testuser", password="testpass")
        assert config.build_username() == "td-customer-testuser"

    def test_build_username_with_country(self):
        """Test username building with country."""
        config = ProxyConfig(
            username="testuser",
            password="testpass",
            country="us",
        )
        assert config.build_username() == "td-customer-testuser-country-us"

    def test_build_username_with_full_geo(self):
        """Test username building with full geo-targeting."""
        config = ProxyConfig(
            username="testuser",
            password="testpass",
            country="us",
            state="california",
            city="los_angeles",
        )
        expected = "td-customer-testuser-country-us-state-california-city-los_angeles"
        assert config.build_username() == expected

    def test_build_username_with_session(self):
        """Test username building with sticky session."""
        config = ProxyConfig(
            username="testuser",
            password="testpass",
            country="gb",
            session_id="mysession123",
            session_duration=10,
        )
        expected = "td-customer-testuser-country-gb-sessid-mysession123-sesstime-10"
        assert config.build_username() == expected

    def test_build_username_with_asn(self):
        """Test username building with ASN."""
        config = ProxyConfig(
            username="testuser",
            password="testpass",
            country="fr",
            asn="AS12322",
        )
        expected = "td-customer-testuser-country-fr-asn-AS12322"
        assert config.build_username() == expected

    def test_build_proxy_url(self):
        """Test full proxy URL building."""
        config = ProxyConfig(
            username="testuser",
            password="testpass",
            country="us",
        )
        url = config.build_proxy_url()
        assert "td-customer-testuser-country-us" in url
        assert ":testpass@" in url
        assert "pr.thordata.net" in url

    def test_to_proxies_dict(self):
        """Test conversion to proxies dict."""
        config = ProxyConfig(username="testuser", password="testpass")
        proxies = config.to_proxies_dict()
        assert "http" in proxies
        assert "https" in proxies
        assert proxies["http"] == proxies["https"]

    def test_invalid_protocol(self):
        """Test validation of protocol."""
        with pytest.raises(ValueError, match="Invalid protocol"):
            ProxyConfig(
                username="testuser",
                password="testpass",
                protocol="ftp",
            )

    def test_invalid_session_duration(self):
        """Test validation of session duration."""
        with pytest.raises(ValueError, match="session_duration must be between"):
            ProxyConfig(
                username="testuser",
                password="testpass",
                session_id="test",
                session_duration=100,  # Max is 90
            )

    def test_session_duration_requires_session_id(self):
        """Test that session_duration requires session_id."""
        with pytest.raises(ValueError, match="session_duration requires session_id"):
            ProxyConfig(
                username="testuser",
                password="testpass",
                session_duration=10,
            )

    def test_asn_requires_country(self):
        """Test that ASN requires country."""
        with pytest.raises(ValueError, match="ASN targeting requires country"):
            ProxyConfig(
                username="testuser",
                password="testpass",
                asn="AS12322",
            )

    def test_invalid_continent(self):
        """Test validation of continent code."""
        with pytest.raises(ValueError, match="Invalid continent code"):
            ProxyConfig(
                username="testuser",
                password="testpass",
                continent="xx",
            )

    def test_invalid_country_code(self):
        """Test validation of country code format."""
        with pytest.raises(ValueError, match="Invalid country code"):
            ProxyConfig(
                username="testuser",
                password="testpass",
                country="usa",  # Should be 2 letters
            )

    def test_proxy_product_ports(self):
        """Test that different products have different default ports."""
        residential = ProxyConfig(
            username="user", password="pass", product=ProxyProduct.RESIDENTIAL
        )
        assert residential.port == 9999

        mobile = ProxyConfig(
            username="user", password="pass", product=ProxyProduct.MOBILE
        )
        assert mobile.port == 5555

        datacenter = ProxyConfig(
            username="user", password="pass", product=ProxyProduct.DATACENTER
        )
        assert datacenter.port == 7777


class TestStickySession:
    """Tests for StickySession dataclass."""

    def test_auto_session_id(self):
        """Test automatic session ID generation."""
        session = StickySession(
            username="testuser",
            password="testpass",
            duration_minutes=15,
        )
        assert session.session_id is not None
        assert len(session.session_id) == 12

    def test_custom_session_id(self):
        """Test using a custom session ID."""
        session = StickySession(
            username="testuser",
            password="testpass",
            session_id="mycustomid",
            duration_minutes=10,
            auto_session_id=False,
        )
        assert session.session_id == "mycustomid"


class TestSerpRequest:
    """Tests for SerpRequest dataclass."""

    def test_basic_payload(self):
        """Test basic SERP request payload."""
        request = SerpRequest(query="test query")
        payload = request.to_payload()

        assert payload["q"] == "test query"
        assert payload["engine"] == "google"
        assert payload["num"] == "10"
        assert payload["json"] == "1"

    def test_yandex_uses_text_param(self):
        """Test that Yandex uses 'text' instead of 'q'."""
        request = SerpRequest(query="test query", engine="yandex")
        payload = request.to_payload()

        assert "text" in payload
        assert payload["text"] == "test query"
        assert "q" not in payload

    def test_search_type_mapping(self):
        """Test search type parameter mapping."""
        request = SerpRequest(
            query="test",
            search_type="shopping",
        )
        payload = request.to_payload()
        assert payload["tbm"] == "shop"

    def test_time_filter_mapping(self):
        """Test time filter parameter mapping."""
        request = SerpRequest(
            query="test",
            time_filter="week",
        )
        payload = request.to_payload()
        assert payload["tbs"] == "qdr:w"

    def test_localization_params(self):
        """Test localization parameters."""
        request = SerpRequest(
            query="test",
            country="us",
            language="en",
        )
        payload = request.to_payload()
        assert payload["gl"] == "us"
        assert payload["hl"] == "en"

    def test_pagination(self):
        """Test pagination parameters."""
        request = SerpRequest(
            query="test",
            num=20,
            start=40,
        )
        payload = request.to_payload()
        assert payload["num"] == "20"
        assert payload["start"] == "40"


class TestUniversalScrapeRequest:
    """Tests for UniversalScrapeRequest dataclass."""

    def test_basic_payload(self):
        """Test basic Universal scrape request payload."""
        request = UniversalScrapeRequest(url="https://example.com")
        payload = request.to_payload()

        assert payload["url"] == "https://example.com"
        assert payload["js_render"] == "False"
        assert payload["type"] == "html"

    def test_js_render_enabled(self):
        """Test JS rendering option."""
        request = UniversalScrapeRequest(
            url="https://example.com",
            js_render=True,
        )
        payload = request.to_payload()
        assert payload["js_render"] == "True"

    def test_wait_params(self):
        """Test wait parameters."""
        request = UniversalScrapeRequest(
            url="https://example.com",
            wait=5000,
            wait_for=".content",
        )
        payload = request.to_payload()
        assert payload["wait"] == "5000"
        assert payload["wait_for"] == ".content"

    def test_invalid_output_format(self):
        """Test validation of output format."""
        with pytest.raises(ValueError, match="Invalid output_format"):
            UniversalScrapeRequest(
                url="https://example.com",
                output_format="pdf",  # Only html/png supported
            )

    def test_invalid_wait_value(self):
        """Test validation of wait value."""
        with pytest.raises(ValueError, match="wait must be between"):
            UniversalScrapeRequest(
                url="https://example.com",
                wait=200000,  # Max is 100000
            )


class TestScraperTaskConfig:
    """Tests for ScraperTaskConfig dataclass."""

    def test_basic_payload(self):
        """Test basic task config payload."""
        config = ScraperTaskConfig(
            file_name="test_output",
            spider_id="test_spider",
            spider_name="example.com",
            parameters={"url": "https://example.com"},
        )
        payload = config.to_payload()

        assert payload["file_name"] == "test_output"
        assert payload["spider_id"] == "test_spider"
        assert payload["spider_name"] == "example.com"
        assert "spider_parameters" in payload
        assert payload["spider_errors"] == "true"


========================================================================
FILE: tests\test_spec_parity.py
SIZE: 2340
TRUNCATED: no
========================================================================
import json
import os
from pathlib import Path

import pytest

from thordata.exceptions import raise_for_code
from thordata.models import ProxyProduct, SerpRequest


def _load_spec() -> dict:
    spec_path = os.getenv("THORDATA_SDK_SPEC_PATH")
    if spec_path:
        return json.loads(Path(spec_path).read_text(encoding="utf-8"))

    default = Path(__file__).resolve().parent.parent / "sdk-spec" / "v1.json"
    if not default.exists():
        pytest.skip(f"Spec file not found: {default}")
    return json.loads(default.read_text(encoding="utf-8"))


def test_spec_proxy_ports_match_python() -> None:
    spec = _load_spec()
    proxy = spec["proxy"]["products"]

    assert ProxyProduct.RESIDENTIAL.default_port == int(proxy["residential"]["port"])
    assert ProxyProduct.MOBILE.default_port == int(proxy["mobile"]["port"])
    assert ProxyProduct.DATACENTER.default_port == int(proxy["datacenter"]["port"])
    assert ProxyProduct.ISP.default_port == int(proxy["isp"]["port"])


def test_spec_serp_search_type_mapping_matches_python() -> None:
    spec = _load_spec()
    mapping = spec["serp"]["mappings"]["searchTypeToTbm"]

    assert SerpRequest.SEARCH_TYPE_MAP["news"] == mapping["news"]
    assert SerpRequest.SEARCH_TYPE_MAP["images"] == mapping["images"]
    assert SerpRequest.SEARCH_TYPE_MAP["shopping"] == mapping["shopping"]
    assert SerpRequest.SEARCH_TYPE_MAP["videos"] == mapping["videos"]


def test_spec_serp_time_filter_mapping_matches_python() -> None:
    spec = _load_spec()
    mapping = spec["serp"]["mappings"]["timeFilterToTbs"]

    assert SerpRequest.TIME_FILTER_MAP["week"] == mapping["week"]
    assert SerpRequest.TIME_FILTER_MAP["day"] == mapping["day"]
    assert SerpRequest.TIME_FILTER_MAP["month"] == mapping["month"]
    assert SerpRequest.TIME_FILTER_MAP["year"] == mapping["year"]


def test_raise_for_code_precedence_payload_code_over_http_status() -> None:
    # Ensure code=300 is treated as NotCollected even if status_code is 200.
    payload = {"code": 300, "msg": "Not collected"}
    with pytest.raises(Exception) as excinfo:
        raise_for_code("Error", status_code=200, code=300, payload=payload)
    # type name check to avoid importing specific class here
    assert "NotCollected" in excinfo.value.__class__.__name__


========================================================================
FILE: tests\test_task_status_and_wait.py
SIZE: 2272
TRUNCATED: no
========================================================================
import pytest
from pytest_httpserver import HTTPServer

from thordata import (
    AsyncThordataClient,
    ThordataAuthError,
    ThordataClient,
)


def test_wait_for_task_timeout_uses_monotonic(monkeypatch) -> None:
    client = ThordataClient(scraper_token="dummy", public_token="p", public_key="k")

    # Always "running" so it must time out quickly.
    monkeypatch.setattr(client, "get_task_status", lambda task_id: "running")

    with pytest.raises(TimeoutError):
        client.wait_for_task("t1", poll_interval=0.01, max_wait=0.05)


@pytest.mark.asyncio
async def test_async_wait_for_task_timeout_uses_monotonic(monkeypatch) -> None:
    async with AsyncThordataClient(
        scraper_token="dummy", public_token="p", public_key="k"
    ) as client:

        async def _always_running(task_id: str) -> str:
            return "running"

        monkeypatch.setattr(client, "get_task_status", _always_running)

        with pytest.raises(TimeoutError):
            await client.wait_for_task("t1", poll_interval=0.01, max_wait=0.05)


def test_get_task_status_raises_on_non_200_code(httpserver: HTTPServer) -> None:
    httpserver.expect_request("/tasks-status", method="POST").respond_with_json(
        {"code": 401, "msg": "Unauthorized"}
    )

    base_url = httpserver.url_for("/").rstrip("/").replace("localhost", "127.0.0.1")

    client = ThordataClient(
        scraper_token="dummy",
        public_token="p",
        public_key="k",
        web_scraper_api_base_url=base_url,
    )

    with pytest.raises(ThordataAuthError):
        client.get_task_status("t1")


@pytest.mark.asyncio
async def test_async_get_task_status_raises_on_non_200_code(
    httpserver: HTTPServer,
) -> None:
    httpserver.expect_request("/tasks-status", method="POST").respond_with_json(
        {"code": 401, "msg": "Unauthorized"}
    )

    base_url = httpserver.url_for("/").rstrip("/").replace("localhost", "127.0.0.1")

    async with AsyncThordataClient(
        scraper_token="dummy",
        public_token="p",
        public_key="k",
        web_scraper_api_base_url=base_url,
    ) as client:
        with pytest.raises(ThordataAuthError):
            await client.get_task_status("t1")


========================================================================
FILE: tests\test_user_agent.py
SIZE: 1532
TRUNCATED: no
========================================================================
import os

import pytest
from pytest_httpserver import HTTPServer
from werkzeug.wrappers import Request, Response

from thordata import AsyncThordataClient, ThordataClient


def test_sync_user_agent_is_sent(httpserver: HTTPServer) -> None:
    def handler(request: Request) -> Response:
        ua = request.headers.get("User-Agent", "")
        assert "thordata-python-sdk/" in ua
        return Response(
            '{"code":200,"organic":[]}', status=200, content_type="application/json"
        )

    httpserver.expect_request("/request", method="POST").respond_with_handler(handler)
    base_url = httpserver.url_for("/").rstrip("/").replace("localhost", "127.0.0.1")

    client = ThordataClient(scraper_token="dummy", scraperapi_base_url=base_url)
    client.serp_search("python", num=1)


@pytest.mark.asyncio
async def test_async_user_agent_is_sent(httpserver: HTTPServer) -> None:
    def handler(request: Request) -> Response:
        ua = request.headers.get("User-Agent", "")
        assert "thordata-python-sdk/" in ua
        return Response(
            '{"code":200,"organic":[]}', status=200, content_type="application/json"
        )

    httpserver.expect_request("/request", method="POST").respond_with_handler(handler)
    base_url = httpserver.url_for("/").rstrip("/").replace("localhost", "127.0.0.1")

    async with AsyncThordataClient(
        scraper_token="dummy", scraperapi_base_url=base_url
    ) as client:
        await client.serp_search("python", num=1)


========================================================================
FILE: tests\__init__.py
SIZE: 0
TRUNCATED: no
========================================================================
