# examples/cookbook/ai_video_dataset_builder.py
"""
COOKBOOK: AI Video Dataset Builder
----------------------------------
Scenario: An AI company needs metadata from thousands of YouTube videos 
to train a multi-modal model (LMM).

This script demonstrates how to use Thordata Web Scraper to fetch
video metadata efficiently.
"""

import os
import time
import json
from thordata_sdk import ThordataClient

# âš ï¸ é…ç½®ä½ çš„ Key
SCRAPER_TOKEN = os.getenv("THORDATA_SCRAPER_TOKEN", "YOUR_TOKEN_HERE")
PUBLIC_TOKEN = os.getenv("THORDATA_PUBLIC_TOKEN", "YOUR_TOKEN_HERE")
PUBLIC_KEY = os.getenv("THORDATA_PUBLIC_KEY", "YOUR_KEY_HERE")

def build_dataset():
    if SCRAPER_TOKEN == "YOUR_TOKEN_HERE":
        print("Please set your tokens first.")
        return

    client = ThordataClient(SCRAPER_TOKEN, PUBLIC_TOKEN, PUBLIC_KEY)
    
    # å‡è®¾æˆ‘ä»¬éœ€è¦æŠ“å–è¿™äº›é¢‘é“çš„è§†é¢‘ä½œä¸ºè®­ç»ƒæ•°æ®
    target_channels = [
        "https://www.youtube.com/@OpenAI",
        "https://www.youtube.com/@NVIDIA",
        "https://www.youtube.com/@DeepMind"
    ]
    
    print(f"ğŸ¤– Starting AI Dataset Job for {len(target_channels)} channels...")
    
    tasks = {}
    
    # 1. æ‰¹é‡åˆ›å»ºä»»åŠ¡
    for url in target_channels:
        channel_name = url.split("@")[-1]
        print(f"   -> Queueing scraper for: {channel_name}")
        
        task_id = client.create_scraper_task(
            file_name=f"dataset_{channel_name}",
            spider_id="youtube_video-post_by-url",
            spider_name="youtube.com",
            individual_params={
                "url": url,
                "order_by": "", # Default sort
                "num_of_posts": "" # Default batch
            }
        )
        tasks[channel_name] = task_id
    
    print(f"âœ… All tasks queued. IDs: {list(tasks.values())}")
    print("â³ Waiting for cloud processing (Simulated)...")
    
    # è¿™é‡Œä»…æ¼”ç¤ºé€»è¾‘ï¼Œå®é™…ä½¿ç”¨æ—¶å¯ä»¥ä½¿ç”¨ async_client å¹¶å‘è½®è¯¢
    print("   (In a real scenario, data would be downloaded and saved to 'datasets/' folder)")
    print("âœ… Dataset pipeline initialized successfully.")

if __name__ == "__main__":
    build_dataset()