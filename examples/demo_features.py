# D:\Thordata_Work\thordata-python-sdk\examples\demo_features.py

import os
import sys
import time
import json
from dotenv import load_dotenv # pip install python-dotenv

# è·¯å¾„å¤„ç†
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# ğŸŒŸ æ–°å¢ï¼šå¯¼å…¥ Engine æšä¸¾
from thordata_sdk.client import ThordataClient 
from thordata_sdk.enums import Engine

def main():
    load_dotenv() # è‡ªåŠ¨è¯»å– .env
    print("=== Thordata SDK Demo (v0.2.4+) ===")

    # ä»ç¯å¢ƒå˜é‡è·å–ï¼Œä¸å†ç¡¬ç¼–ç 
    SCRAPER_TOKEN = os.getenv("THORDATA_SCRAPER_TOKEN")
    PUBLIC_TOKEN = os.getenv("THORDATA_PUBLIC_TOKEN") 
    PUBLIC_KEY = os.getenv("THORDATA_PUBLIC_KEY")

    if not SCRAPER_TOKEN:
        print("âŒ Error: Missing Token. Please set THORDATA_SCRAPER_TOKEN in .env file.")
        return

    client = ThordataClient(SCRAPER_TOKEN, PUBLIC_TOKEN, PUBLIC_KEY)

    # ==========================================
    # 1. Test SERP API (Using Enum!)
    # ==========================================
    print("\n--- 1. SERP Search (Google) ---")
    try:
        query = "Thordata technology"
        print(f"Searching for: '{query}'...")
        
        # ğŸŒŸ æœ€ä½³å®è·µï¼šä½¿ç”¨ Engine.GOOGLE è€Œä¸æ˜¯å­—ç¬¦ä¸² "google"
        results = client.serp_search(query, engine=Engine.GOOGLE)
        
        metadata = results.get("search_metadata", {})
        print(f"âœ… Status: {metadata.get('status', 'Unknown')}")
        
        if "organic" in results:
            print(f"   Found {len(results['organic'])} organic results. Top 2:")
            for item in results["organic"][:2]:
                print(f"   - {item.get('title')}")
                print(f"     {item.get('link')}")
        else:
            print("   âš ï¸ No organic results found.")
            
    except Exception as e:
        print(f"âŒ SERP Failed: {e}")

    # ==========================================
    # 2. Test Web Scraper API
    # ==========================================
    print("\n--- 2. Web Scraper (YouTube) ---")
    try:
        print("Creating task...")
        task_id = client.create_scraper_task(
            file_name="demo_youtube_data",
            spider_id="youtube_video-post_by-url",
            spider_name="youtube.com", # è¿™é‡Œä¾ç„¶å¯ä»¥ç”¨å­—ç¬¦ä¸²ï¼Œæˆ–è€…å¦‚æœä½ å®šä¹‰äº† ScraperTarget æšä¸¾ä¹Ÿå¯ä»¥ç”¨
            individual_params={
                "url": "https://www.youtube.com/@stephcurry/videos",
                "order_by": "",
                "num_of_posts": ""
            }
        )
        print(f"âœ… Task Created! ID: {task_id}")

        print("Waiting for completion...")
        # ç®€å•è½®è¯¢é€»è¾‘
        for i in range(10): 
            status = client.get_task_status(task_id)
            print(f"   Check {i+1}: {status}")
            if status in ["Ready", "Success"]:
                break
            if status == "Failed":
                print("âŒ Task failed.")
                return
            time.sleep(3)

        if status in ["Ready", "Success"]:
            url = client.get_task_result(task_id)
            print(f"\nâœ… Download URL: {url}")
            
    except Exception as e:
        print(f"âŒ Scraper Failed: {e}")

if __name__ == "__main__":
    main()